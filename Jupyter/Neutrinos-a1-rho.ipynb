{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tree loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Task 2\n",
    "\n",
    "#This is a script for testing for neutrinos reconstruction\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"/eos/home-a/acraplet/.local/lib/python2.7/site-packages\")\n",
    "sys.path.append(\"/home/acraplet/Alie/Masters/ICHiggsTauTauMasters/\")\n",
    "import uproot \n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from lbn_modified3 import LBN, LBNLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#for some reason pylorentz is installed somewhere differently ?\n",
    "#sys.path.append(\"/eos/home-a/acraplet/.local/lib/python2.7/site-packages\")\n",
    "sys.path.append(\"/home/acraplet/Alie/Masters/ICHiggsTauTauMasters/\")\n",
    "from pylorentz import Momentum4\n",
    "from pylorentz import Vector4\n",
    "from pylorentz import Position4\n",
    "\n",
    "# loading the tree\n",
    "tree = uproot.open(\"/home/acraplet/Alie/Masters/MVAFILE_AllHiggs_tt.root\")[\"ntuple\"]\n",
    "#tree = uproot.open(\"/eos/user/d/dwinterb/SWAN_projects/Masters_CP/MVAFILE_GluGluHToTauTauUncorrelatedDecay_Filtered_tt_2018.root\")[\"ntuple\"]\n",
    "print(\"\\n Tree loaded\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1\n",
      "Check 1\n",
      "677734.3999999999 This is the length\n",
      "panda Data frame created \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_E_1</th>\n",
       "      <th>pi_px_1</th>\n",
       "      <th>pi_py_1</th>\n",
       "      <th>pi_pz_1</th>\n",
       "      <th>pi_E_2</th>\n",
       "      <th>pi_px_2</th>\n",
       "      <th>pi_py_2</th>\n",
       "      <th>pi_pz_2</th>\n",
       "      <th>pi0_E_1</th>\n",
       "      <th>pi0_px_1</th>\n",
       "      <th>...</th>\n",
       "      <th>met</th>\n",
       "      <th>metx</th>\n",
       "      <th>mety</th>\n",
       "      <th>tau_decay_mode_1</th>\n",
       "      <th>tau_decay_mode_2</th>\n",
       "      <th>mva_dm_1</th>\n",
       "      <th>mva_dm_2</th>\n",
       "      <th>rand</th>\n",
       "      <th>wt_cp_ps</th>\n",
       "      <th>wt_cp_sm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45.423448</td>\n",
       "      <td>-13.747046</td>\n",
       "      <td>-38.825621</td>\n",
       "      <td>-19.153590</td>\n",
       "      <td>35.805782</td>\n",
       "      <td>8.526798</td>\n",
       "      <td>34.653880</td>\n",
       "      <td>2.904638</td>\n",
       "      <td>10.039846</td>\n",
       "      <td>-3.551914</td>\n",
       "      <td>...</td>\n",
       "      <td>30.017723</td>\n",
       "      <td>22.520916</td>\n",
       "      <td>19.846209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861532</td>\n",
       "      <td>0.950417</td>\n",
       "      <td>1.228852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24.506373</td>\n",
       "      <td>-14.056253</td>\n",
       "      <td>9.809321</td>\n",
       "      <td>-17.514046</td>\n",
       "      <td>19.587723</td>\n",
       "      <td>8.539313</td>\n",
       "      <td>-3.149378</td>\n",
       "      <td>-17.344192</td>\n",
       "      <td>41.244844</td>\n",
       "      <td>-24.443820</td>\n",
       "      <td>...</td>\n",
       "      <td>29.751921</td>\n",
       "      <td>29.431940</td>\n",
       "      <td>-4.351746</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932849</td>\n",
       "      <td>1.936855</td>\n",
       "      <td>0.124674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.319610</td>\n",
       "      <td>8.794122</td>\n",
       "      <td>0.774085</td>\n",
       "      <td>12.519393</td>\n",
       "      <td>17.469988</td>\n",
       "      <td>-9.876231</td>\n",
       "      <td>-3.253855</td>\n",
       "      <td>14.037575</td>\n",
       "      <td>69.457213</td>\n",
       "      <td>38.899682</td>\n",
       "      <td>...</td>\n",
       "      <td>25.280821</td>\n",
       "      <td>23.547763</td>\n",
       "      <td>9.199062</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.132842</td>\n",
       "      <td>0.400455</td>\n",
       "      <td>1.461517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>94.211361</td>\n",
       "      <td>42.942276</td>\n",
       "      <td>-33.711103</td>\n",
       "      <td>-76.780750</td>\n",
       "      <td>3.656937</td>\n",
       "      <td>-2.129749</td>\n",
       "      <td>0.251363</td>\n",
       "      <td>-2.958835</td>\n",
       "      <td>25.585962</td>\n",
       "      <td>11.770386</td>\n",
       "      <td>...</td>\n",
       "      <td>9.850734</td>\n",
       "      <td>8.986096</td>\n",
       "      <td>4.035721</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514073</td>\n",
       "      <td>0.061072</td>\n",
       "      <td>0.059870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>25.899289</td>\n",
       "      <td>-22.245884</td>\n",
       "      <td>-12.141924</td>\n",
       "      <td>-5.333670</td>\n",
       "      <td>23.795390</td>\n",
       "      <td>21.402816</td>\n",
       "      <td>9.436143</td>\n",
       "      <td>-4.368043</td>\n",
       "      <td>23.216735</td>\n",
       "      <td>-20.268636</td>\n",
       "      <td>...</td>\n",
       "      <td>9.680394</td>\n",
       "      <td>9.445943</td>\n",
       "      <td>-2.117590</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504356</td>\n",
       "      <td>0.931771</td>\n",
       "      <td>0.654131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pi_E_1    pi_px_1    pi_py_1    pi_pz_1     pi_E_2    pi_px_2  \\\n",
       "entry                                                                     \n",
       "8      45.423448 -13.747046 -38.825621 -19.153590  35.805782   8.526798   \n",
       "25     24.506373 -14.056253   9.809321 -17.514046  19.587723   8.539313   \n",
       "27     15.319610   8.794122   0.774085  12.519393  17.469988  -9.876231   \n",
       "45     94.211361  42.942276 -33.711103 -76.780750   3.656937  -2.129749   \n",
       "50     25.899289 -22.245884 -12.141924  -5.333670  23.795390  21.402816   \n",
       "\n",
       "         pi_py_2    pi_pz_2    pi0_E_1   pi0_px_1  ...        met       metx  \\\n",
       "entry                                              ...                         \n",
       "8      34.653880   2.904638  10.039846  -3.551914  ...  30.017723  22.520916   \n",
       "25     -3.149378 -17.344192  41.244844 -24.443820  ...  29.751921  29.431940   \n",
       "27     -3.253855  14.037575  69.457213  38.899682  ...  25.280821  23.547763   \n",
       "45      0.251363  -2.958835  25.585962  11.770386  ...   9.850734   8.986096   \n",
       "50      9.436143  -4.368043  23.216735 -20.268636  ...   9.680394   9.445943   \n",
       "\n",
       "            mety  tau_decay_mode_1  tau_decay_mode_2  mva_dm_1  mva_dm_2  \\\n",
       "entry                                                                      \n",
       "8      19.846209                 1                 1         1         1   \n",
       "25     -4.351746                 1                 1         1         1   \n",
       "27      9.199062                 1                 1         1         1   \n",
       "45      4.035721                 1                 1         1         1   \n",
       "50     -2.117590                 1                 1         1         1   \n",
       "\n",
       "           rand  wt_cp_ps  wt_cp_sm  \n",
       "entry                                \n",
       "8      0.861532  0.950417  1.228852  \n",
       "25     0.932849  1.936855  0.124674  \n",
       "27     0.132842  0.400455  1.461517  \n",
       "45     0.514073  0.061072  0.059870  \n",
       "50     0.504356  0.931771  0.654131  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define what variables are to be read into the dataframe\n",
    "momenta_features = [ \"pi_E_1\", \"pi_px_1\", \"pi_py_1\", \"pi_pz_1\", #leading charged pi 4-momentum\n",
    "              \"pi_E_2\", \"pi_px_2\", \"pi_py_2\", \"pi_pz_2\", #subleading charged pi 4-momentum\n",
    "              \"pi0_E_1\",\"pi0_px_1\",\"pi0_py_1\",\"pi0_pz_1\", #leading neutral pi 4-momentum\n",
    "              \"pi0_E_2\",\"pi0_px_2\",\"pi0_py_2\",\"pi0_pz_2\", #subleading neutral pi 4-momentum\n",
    "              \"gen_nu_p_1\", \"gen_nu_phi_1\", \"gen_nu_eta_1\", #leading neutrino, gen level\n",
    "              \"gen_nu_p_2\", \"gen_nu_phi_2\", \"gen_nu_eta_2\", #subleading neutrino, gen level  \n",
    "              \"pi2_E_1\", \"pi2_px_1\", \"pi2_py_1\", \"pi2_pz_1\",\n",
    "              \"pi3_E_1\", \"pi3_px_1\", \"pi3_py_1\", \"pi3_pz_1\"\n",
    "                ] \n",
    "\n",
    "other_features = [ \"ip_x_1\", \"ip_y_1\", \"ip_z_1\",        #leading impact parameter\n",
    "                   \"ip_x_2\", \"ip_y_2\", \"ip_z_2\",        #subleading impact parameter\n",
    "                   #\"y_1_1\", \"y_1_2\",\n",
    "                   \"gen_phitt\", \"ip_sig_2\", \"ip_sig_1\"\n",
    "                 ]    # ratios of energies\n",
    "\n",
    "target = [ \"met\", \"metx\", \"mety\", #\"aco_angle_1\", \"aco_angle_6\", \"aco_angle_5\", \"aco_angle_7\"\n",
    "         ]  #acoplanarity angle\n",
    "    \n",
    "selectors = [ \"tau_decay_mode_1\",\"tau_decay_mode_2\",\n",
    "             \"mva_dm_1\",\"mva_dm_2\",\"rand\",\"wt_cp_ps\",\"wt_cp_sm\",\n",
    "            ]\n",
    "\n",
    "additional_info = [ \"sv_x_1\", \"sv_y_1\", \"sv_z_1\",\n",
    "                    \"sv_x_2\", \"sv_y_2\", \"sv_z_2\",\n",
    "                    ]\n",
    "\n",
    "sv_covariance_matrices = [\"svcov00_1\", \"svcov01_1\", \"svcov02_1\",\n",
    "                       \"svcov10_1\", \"svcov11_1\", \"svcov12_1\", \n",
    "                       \"svcov20_1\", \"svcov21_1\", \"svcov22_1\", \n",
    "                       \"svcov00_2\", \"svcov01_2\", \"svcov02_2\",\n",
    "                       \"svcov10_2\", \"svcov11_2\", \"svcov12_2\", \n",
    "                       \"svcov20_2\", \"svcov21_2\", \"svcov22_2\", \n",
    "    \n",
    "]\n",
    "\n",
    "ip_covariance_matrices = [\"ipcov00_1\", \"ipcov01_1\", \"ipcov02_1\",\n",
    "                       \"ipcov10_1\", \"ipcov11_1\", \"ipcov12_1\", \n",
    "                       \"ipcov20_1\", \"ipcov21_1\", \"ipcov22_1\", \n",
    "                       \"ipcov00_2\", \"ipcov01_2\", \"ipcov02_2\",\n",
    "                       \"ipcov10_2\", \"ipcov11_2\", \"ipcov12_2\", \n",
    "                       \"ipcov20_2\", \"ipcov21_2\", \"ipcov22_2\", \n",
    "    \n",
    "]\n",
    "\n",
    "met_covariance_matrices = [\"metcov00\", \n",
    "                           \"metcov01\", \n",
    "                           \"metcov10\", \n",
    "                           \"metcov11\" ]\n",
    "\n",
    "covs = sv_covariance_matrices + ip_covariance_matrices + met_covariance_matrices\n",
    "\n",
    "\n",
    "variables4=(momenta_features+other_features+target+selectors)#+additional_info + covs) #copying Kinglsey's way cause it is very clean\n",
    "print('Check 1')\n",
    "\n",
    "df4 = tree.pandas.df(variables4)\n",
    "# plt.figure()\n",
    "# plt.hist2d(df4[\"mva_dm_1\"],df4[\"mva_dm_2\"], label = 'mva_dm_2 against mva_dm_1')\n",
    "# plt.grid()\n",
    "# #plt.legend()\n",
    "# plt.savefig('The_mva_dm2d.png')\n",
    "\n",
    "print('Check 1')\n",
    "\n",
    "df4 = df4[\n",
    "      (df4[\"tau_decay_mode_1\"] == 1) \n",
    "    & (df4[\"tau_decay_mode_2\"] == 1) \n",
    "    & (df4[\"mva_dm_1\"] == 1) \n",
    "    & (df4[\"mva_dm_2\"] == 1)\n",
    "    & (df4[\"gen_nu_p_1\"] > -4000)\n",
    "    & (df4[\"gen_nu_p_2\"] > -4000)\n",
    "]\n",
    "\n",
    "print(0.7*len(df4),'This is the length') #up to here we are fine\n",
    "\n",
    "df_ps = df4[\n",
    "      (df4[\"rand\"]<df4[\"wt_cp_ps\"]/2)     #a data frame only including the pseudoscalars\n",
    "]\n",
    "\n",
    "df_sm = df4[\n",
    "      (df4[\"rand\"]<df4[\"wt_cp_sm\"]/2)     #data frame only including the scalars\n",
    "]\n",
    "\n",
    "print(\"panda Data frame created \\n\")\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will be mega slow so we do not want to do it each time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_product(vector3_1,vector3_2):\n",
    "    if len(vector3_1)!=3 or len(vector3_1)!=3:\n",
    "        print('These are not 3D arrays !')\n",
    "    x_perp_vector=vector3_1[1]*vector3_2[2]-vector3_1[2]*vector3_2[1]\n",
    "    y_perp_vector=vector3_1[2]*vector3_2[0]-vector3_1[0]*vector3_2[2]\n",
    "    z_perp_vector=vector3_1[0]*vector3_2[1]-vector3_1[1]*vector3_2[0]\n",
    "    return np.array([x_perp_vector,y_perp_vector,z_perp_vector])\n",
    "\n",
    "def dot_product(vector1,vector2):\n",
    "    if len(vector1)!=len(vector2):\n",
    "        raise Arrays_of_different_size\n",
    "    prod=0\n",
    "    for i in range(len(vector1)):\n",
    "        prod=prod+vector1[i]*vector2[i]\n",
    "    return prod\n",
    "\n",
    "\n",
    "def norm(vector):\n",
    "    if len(vector)!=3:\n",
    "        print('This is only for a 3d vector')\n",
    "    return np.sqrt(vector[0]**2+vector[1]**2+vector[2]**2)\n",
    "\n",
    "def remove9999 (Momenta4, leading):\n",
    "    if leading == 1:\n",
    "        nu_ref = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_1\"])), df4[\"gen_nu_eta_1\"], df4[\"gen_nu_phi_1\"], df4[\"gen_nu_p_1\"])\n",
    "    if leading == 2:\n",
    "        nu_ref = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_2\"])), df4[\"gen_nu_eta_2\"], df4[\"gen_nu_phi_2\"], df4[\"gen_nu_p_2\"])\n",
    "    \n",
    "    array = np.array(Momenta4).T\n",
    "    array = array[nu_ref.p_z != 9999]\n",
    "    array = array.T\n",
    "    return Momentum4(array[0], array[1], array[2], array[3])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the usefull 4 momenta\n",
    "\n",
    "#neutrinos refs, in E, px, py, pz form\n",
    "nu_1 = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_1\"])), df4[\"gen_nu_eta_1\"], df4[\"gen_nu_phi_1\"], df4[\"gen_nu_p_1\"])\n",
    "nu_2 = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_2\"])), df4[\"gen_nu_eta_2\"], df4[\"gen_nu_phi_2\"], df4[\"gen_nu_p_2\"])\n",
    "\n",
    "#Charged and neutral pion momenta\n",
    "pi_1_4Mom = Momentum4(df4[\"pi_E_1\"],df4[\"pi_px_1\"],df4[\"pi_py_1\"],df4[\"pi_pz_1\"])\n",
    "pi_2_4Mom = Momentum4(df4[\"pi_E_2\"],df4[\"pi_px_2\"],df4[\"pi_py_2\"],df4[\"pi_pz_2\"]) \n",
    "pi2_1_4Mom = Momentum4(df4[\"pi2_E_1\"],df4[\"pi2_px_1\"],df4[\"pi2_py_1\"],df4[\"pi2_pz_1\"])\n",
    "pi3_1_4Mom = Momentum4(df4[\"pi3_E_1\"],df4[\"pi3_px_1\"],df4[\"pi3_py_1\"],df4[\"pi3_pz_1\"])\n",
    "\n",
    "#Same for the pi0\n",
    "pi0_1_4Mom = Momentum4(df4[\"pi0_E_1\"],df4[\"pi0_px_1\"],df4[\"pi0_py_1\"],df4[\"pi0_pz_1\"])\n",
    "pi0_2_4Mom = Momentum4(df4[\"pi0_E_2\"],df4[\"pi0_px_2\"],df4[\"pi0_py_2\"],df4[\"pi0_pz_2\"])\n",
    "\n",
    "tau_1_vis = pi_1_4Mom + pi0_1_4Mom #  pi3_1_4Mom + pi2_1_4Mom #\n",
    "tau_2_vis = pi_2_4Mom + pi0_2_4Mom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = tau_1_vis.eta - nu_1.eta\n",
    "hist2 = tau_2_vis.eta - nu_2.eta\n",
    "\n",
    "hist1 = np.array(hist1)\n",
    "hist2 = np.array(hist2)\n",
    "\n",
    "# hist1 = tf.where(tf.math.is_inf(hist1), 999, hist1)\n",
    "\n",
    "plt.close()\n",
    "plt.figure()\n",
    "#plt.xlim(0,5)\n",
    "plt.hist(hist1, alpha = 0.5, bins = 1000, label='tau_vis_1, len: %i\\nmean: %.2f, std: %.2f'%(len(tau_1_vis.eta), hist1.mean(), hist1.std() ))\n",
    "plt.hist(hist2, alpha = 0.5, bins = 1000, label='eta_nu1, len: %i\\nmean: %.2f, std: %.2f'%(len(nu_2.eta), hist2.mean(), hist2.std()))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlim(-0.5,0.5)\n",
    "\n",
    "plt.title('Colinearity approximation - eta')\n",
    "plt.xlabel('Difference between eta visible and eta neutrinos (rad)')\n",
    "plt.savefig('Eta_colinearity')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acraplet/miniconda3/envs/ICMasters/lib/python3.8/site-packages/numpy/lib/histograms.py:851: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  indices = f_indices.astype(np.intp)\n",
      "/home/acraplet/miniconda3/envs/ICMasters/lib/python3.8/site-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/home/acraplet/miniconda3/envs/ICMasters/lib/python3.8/site-packages/matplotlib/transforms.py:1966: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x, y = float(x), float(y)\n"
     ]
    }
   ],
   "source": [
    "tau_2 = tau_2_vis + nu_2\n",
    "tau_1 = tau_1_vis + nu_1\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1.m, alpha = 0.5, bins=100)\n",
    "plt.hist(tau_2.m, alpha = 0.5,  bins=100)\n",
    "plt.xlabel('Mass_GEV')\n",
    "plt.xlim(0,4)\n",
    "#plt.savefig('Is_this_a1.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Here, start straining to regress neutrinos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the nans of the ip values:\n",
    "# \"ip_x_1\", \"ip_y_1\", \"ip_z_1\",        #leading impact parameter\n",
    "#                    \"ip_x_2\", \"ip_y_2\", \"ip_z_2\"\n",
    "def one_d(val):\n",
    "    return tf.constant(val, shape = df4[\"pi_px_1\"].shape, dtype = np.float32)\n",
    "\n",
    "def Mom4_to_tf(Mom4_1D):\n",
    "    return tf.convert_to_tensor(Mom4_1D, dtype = 'float32')\n",
    "\n",
    "ip_x_1 = tf.where(tf.math.is_nan(df4[\"ip_x_1\"]), 0, df4[\"ip_x_1\"])\n",
    "ip_y_1 = tf.where(tf.math.is_nan(df4[\"ip_y_1\"]), 0, df4[\"ip_y_1\"])\n",
    "ip_z_1 = tf.where(tf.math.is_nan(df4[\"ip_z_1\"]), 0, df4[\"ip_z_1\"])\n",
    "ip_x_2 = tf.where(tf.math.is_nan(df4[\"ip_x_2\"]), 0, df4[\"ip_x_2\"])\n",
    "ip_y_2 = tf.where(tf.math.is_nan(df4[\"ip_y_2\"]), 0, df4[\"ip_y_2\"])\n",
    "ip_z_2 = tf.where(tf.math.is_nan(df4[\"ip_z_2\"]), 0, df4[\"ip_z_2\"])\n",
    "\n",
    "\n",
    "sv_x_1 = tf.where(tf.math.is_nan(df4[\"sv_x_1\"]), 0, df4[\"sv_x_1\"])\n",
    "sv_y_1 = tf.where(tf.math.is_nan(df4[\"sv_y_1\"]), 0, df4[\"sv_y_1\"])\n",
    "sv_z_1 = tf.where(tf.math.is_nan(df4[\"sv_z_1\"]), 0, df4[\"sv_z_1\"])\n",
    "sv_x_2 = tf.where(tf.math.is_nan(df4[\"sv_x_2\"]), 0, df4[\"sv_x_2\"])\n",
    "sv_y_2 = tf.where(tf.math.is_nan(df4[\"sv_y_2\"]), 0, df4[\"sv_y_2\"])\n",
    "sv_z_2 = tf.where(tf.math.is_nan(df4[\"sv_z_2\"]), 0, df4[\"sv_z_2\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(571700,)\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lab_frame (InputLayer)       [(None, 571700, 22)]      0         \n",
      "_________________________________________________________________\n",
      "learning (Dense)             (None, 571700, 300)       6900      \n",
      "_________________________________________________________________\n",
      "learning2 (Dense)            (None, 571700, 300)       90300     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 571700, 300)       0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 571700, 14)        4214      \n",
      "=================================================================\n",
      "Total params: 101,414\n",
      "Trainable params: 101,414\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 571700, 22) for input Tensor(\"lab_frame_1:0\", shape=(None, 571700, 22), dtype=float32), but it was called on an input with incompatible shape (None, 22).\n",
      "WARNING:tensorflow:AutoGraph could not transform <function loss_D_p at 0x7f3628eab5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpe74zpv_b.py, line 21)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function loss_D_p at 0x7f3628eab5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpe74zpv_b.py, line 21)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "(None,) this is shape\n",
      "(None,) this is shape\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 571700, 22) for input Tensor(\"lab_frame_1:0\", shape=(None, 571700, 22), dtype=float32), but it was called on an input with incompatible shape (None, 22).\n",
      "(None,) this is shape\n",
      "(None,) this is shape\n",
      "800/801 [============================>.] - ETA: 0s - loss: 21.4953 - mae: 35.9676 - loss_D_p: 0.2000 - loss_phi: 9.3679 - loss_p: 10.3544 - loss_mass_tau: 0.6494 - loss_mass_Higgs: 0.9237WARNING:tensorflow:Model was constructed with shape (None, 571700, 22) for input Tensor(\"lab_frame_1:0\", shape=(None, 571700, 22), dtype=float32), but it was called on an input with incompatible shape (None, 22).\n",
      "(None,) this is shape\n",
      "(None,) this is shape\n",
      "801/801 [==============================] - 11s 14ms/step - loss: 21.4914 - mae: 35.9666 - loss_D_p: 0.1999 - loss_phi: 9.3665 - loss_p: 10.3521 - loss_mass_tau: 0.6493 - loss_mass_Higgs: 0.9236 - val_loss: 22.6047 - val_mae: 40.4999 - val_loss_D_p: 0.2638 - val_loss_phi: 6.6261 - val_loss_p: 14.2397 - val_loss_mass_tau: 0.5607 - val_loss_mass_Higgs: 0.9144\n",
      "Epoch 2/25\n",
      "801/801 [==============================] - 9s 11ms/step - loss: 18.0316 - mae: 35.5651 - loss_D_p: 0.1723 - loss_phi: 7.4531 - loss_p: 9.1253 - loss_mass_tau: 0.4572 - loss_mass_Higgs: 0.8237 - val_loss: 23.1890 - val_mae: 40.0402 - val_loss_D_p: 0.2494 - val_loss_phi: 8.4787 - val_loss_p: 12.9860 - val_loss_mass_tau: 0.5661 - val_loss_mass_Higgs: 0.9088\n",
      "Epoch 3/25\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 17.3989 - mae: 35.3190 - loss_D_p: 0.1653 - loss_phi: 7.2553 - loss_p: 8.7242 - loss_mass_tau: 0.4374 - loss_mass_Higgs: 0.8167 - val_loss: 19.2290 - val_mae: 39.4056 - val_loss_D_p: 0.2236 - val_loss_phi: 5.3516 - val_loss_p: 12.4537 - val_loss_mass_tau: 0.3668 - val_loss_mass_Higgs: 0.8332\n",
      "Epoch 4/25\n",
      "801/801 [==============================] - 10s 13ms/step - loss: 16.9582 - mae: 35.2320 - loss_D_p: 0.1608 - loss_phi: 7.1146 - loss_p: 8.4660 - loss_mass_tau: 0.4110 - loss_mass_Higgs: 0.8058 - val_loss: 19.8292 - val_mae: 39.6748 - val_loss_D_p: 0.2312 - val_loss_phi: 6.0311 - val_loss_p: 12.1903 - val_loss_mass_tau: 0.4798 - val_loss_mass_Higgs: 0.8969\n",
      "Epoch 5/25\n",
      "801/801 [==============================] - 10s 13ms/step - loss: 16.7369 - mae: 35.1557 - loss_D_p: 0.1583 - loss_phi: 7.0790 - loss_p: 8.2929 - loss_mass_tau: 0.4049 - loss_mass_Higgs: 0.8018 - val_loss: 22.1817 - val_mae: 39.2005 - val_loss_D_p: 0.2226 - val_loss_phi: 8.9100 - val_loss_p: 11.7450 - val_loss_mass_tau: 0.4487 - val_loss_mass_Higgs: 0.8554\n",
      "Epoch 6/25\n",
      "801/801 [==============================] - 11s 14ms/step - loss: 17.8262 - mae: 35.1606 - loss_D_p: 0.1640 - loss_phi: 7.9933 - loss_p: 8.3377 - loss_mass_tau: 0.4996 - loss_mass_Higgs: 0.8316 - val_loss: 22.0138 - val_mae: 39.3904 - val_loss_D_p: 0.2254 - val_loss_phi: 8.6540 - val_loss_p: 11.7675 - val_loss_mass_tau: 0.5087 - val_loss_mass_Higgs: 0.8581\n",
      "Epoch 7/25\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 16.5484 - mae: 35.0464 - loss_D_p: 0.1582 - loss_phi: 7.0320 - loss_p: 8.1319 - loss_mass_tau: 0.4148 - loss_mass_Higgs: 0.8115 - val_loss: 18.7568 - val_mae: 39.2341 - val_loss_D_p: 0.2282 - val_loss_phi: 5.4512 - val_loss_p: 11.7093 - val_loss_mass_tau: 0.4785 - val_loss_mass_Higgs: 0.8896\n",
      "Epoch 8/25\n",
      "801/801 [==============================] - 9s 12ms/step - loss: 16.8472 - mae: 34.9586 - loss_D_p: 0.1581 - loss_phi: 7.3502 - loss_p: 8.0933 - loss_mass_tau: 0.4418 - loss_mass_Higgs: 0.8038 - val_loss: 18.0775 - val_mae: 39.2290 - val_loss_D_p: 0.2181 - val_loss_phi: 5.0746 - val_loss_p: 11.5346 - val_loss_mass_tau: 0.4130 - val_loss_mass_Higgs: 0.8373\n",
      "Epoch 9/25\n",
      "801/801 [==============================] - 9s 12ms/step - loss: 16.2209 - mae: 34.9831 - loss_D_p: 0.1547 - loss_phi: 6.8449 - loss_p: 8.0066 - loss_mass_tau: 0.4253 - loss_mass_Higgs: 0.7894 - val_loss: 17.0706 - val_mae: 39.1253 - val_loss_D_p: 0.2105 - val_loss_phi: 4.2374 - val_loss_p: 11.4034 - val_loss_mass_tau: 0.3961 - val_loss_mass_Higgs: 0.8232\n",
      "Epoch 10/25\n",
      "801/801 [==============================] - 12s 15ms/step - loss: 16.7716 - mae: 35.1349 - loss_D_p: 0.1560 - loss_phi: 7.3795 - loss_p: 7.9912 - loss_mass_tau: 0.4450 - loss_mass_Higgs: 0.8000 - val_loss: 20.1607 - val_mae: 39.5993 - val_loss_D_p: 0.2298 - val_loss_phi: 6.7735 - val_loss_p: 11.6076 - val_loss_mass_tau: 0.5956 - val_loss_mass_Higgs: 0.9542\n",
      "Epoch 11/25\n",
      "801/801 [==============================] - 10s 13ms/step - loss: 16.6741 - mae: 35.0972 - loss_D_p: 0.1555 - loss_phi: 7.2985 - loss_p: 7.9792 - loss_mass_tau: 0.4412 - loss_mass_Higgs: 0.7996 - val_loss: 18.1580 - val_mae: 39.2474 - val_loss_D_p: 0.2162 - val_loss_phi: 5.3599 - val_loss_p: 11.3047 - val_loss_mass_tau: 0.4435 - val_loss_mass_Higgs: 0.8337\n",
      "Epoch 12/25\n",
      "801/801 [==============================] - 14s 18ms/step - loss: 16.7844 - mae: 35.2350 - loss_D_p: 0.1546 - loss_phi: 7.4781 - loss_p: 7.9015 - loss_mass_tau: 0.4554 - loss_mass_Higgs: 0.7948 - val_loss: 19.8352 - val_mae: 39.3727 - val_loss_D_p: 0.2171 - val_loss_phi: 6.8755 - val_loss_p: 11.4010 - val_loss_mass_tau: 0.4730 - val_loss_mass_Higgs: 0.8686\n",
      "Epoch 13/25\n",
      "801/801 [==============================] - 6s 8ms/step - loss: 15.5136 - mae: 35.0914 - loss_D_p: 0.1507 - loss_phi: 6.3879 - loss_p: 7.8037 - loss_mass_tau: 0.3942 - loss_mass_Higgs: 0.7771 - val_loss: 16.9695 - val_mae: 39.2037 - val_loss_D_p: 0.2081 - val_loss_phi: 4.4554 - val_loss_p: 11.1277 - val_loss_mass_tau: 0.3396 - val_loss_mass_Higgs: 0.8388\n",
      "Epoch 14/25\n",
      "801/801 [==============================] - 6s 7ms/step - loss: 15.5102 - mae: 35.2161 - loss_D_p: 0.1491 - loss_phi: 6.4638 - loss_p: 7.7290 - loss_mass_tau: 0.3944 - loss_mass_Higgs: 0.7740 - val_loss: 18.9424 - val_mae: 39.3900 - val_loss_D_p: 0.2107 - val_loss_phi: 6.4184 - val_loss_p: 11.0225 - val_loss_mass_tau: 0.4489 - val_loss_mass_Higgs: 0.8418\n",
      "Epoch 15/25\n",
      "801/801 [==============================] - 8s 10ms/step - loss: 16.0413 - mae: 35.2621 - loss_D_p: 0.1498 - loss_phi: 6.9667 - loss_p: 7.7277 - loss_mass_tau: 0.4258 - loss_mass_Higgs: 0.7712 - val_loss: 19.0034 - val_mae: 39.6980 - val_loss_D_p: 0.2210 - val_loss_phi: 6.0345 - val_loss_p: 11.3644 - val_loss_mass_tau: 0.5337 - val_loss_mass_Higgs: 0.8498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n",
      "801/801 [==============================] - 9s 11ms/step - loss: 16.3590 - mae: 35.2795 - loss_D_p: 0.1524 - loss_phi: 7.2247 - loss_p: 7.7274 - loss_mass_tau: 0.4704 - loss_mass_Higgs: 0.7842 - val_loss: 17.1638 - val_mae: 39.3263 - val_loss_D_p: 0.2013 - val_loss_phi: 4.8219 - val_loss_p: 10.9182 - val_loss_mass_tau: 0.4124 - val_loss_mass_Higgs: 0.8099\n",
      "Epoch 17/25\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 16.4002 - mae: 35.2212 - loss_D_p: 0.1515 - loss_phi: 7.2860 - loss_p: 7.7149 - loss_mass_tau: 0.4620 - loss_mass_Higgs: 0.7858 - val_loss: 18.2102 - val_mae: 39.3203 - val_loss_D_p: 0.2129 - val_loss_phi: 5.5406 - val_loss_p: 11.1202 - val_loss_mass_tau: 0.4924 - val_loss_mass_Higgs: 0.8441\n",
      "Epoch 18/25\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 14.6750 - mae: 35.0221 - loss_D_p: 0.1465 - loss_phi: 5.7778 - loss_p: 7.6163 - loss_mass_tau: 0.3773 - loss_mass_Higgs: 0.7571 - val_loss: 20.3530 - val_mae: 39.2092 - val_loss_D_p: 0.2052 - val_loss_phi: 8.0100 - val_loss_p: 10.8520 - val_loss_mass_tau: 0.4517 - val_loss_mass_Higgs: 0.8340\n",
      "Epoch 19/25\n",
      "801/801 [==============================] - 9s 12ms/step - loss: 16.0298 - mae: 35.0200 - loss_D_p: 0.1496 - loss_phi: 7.0850 - loss_p: 7.5860 - loss_mass_tau: 0.4347 - loss_mass_Higgs: 0.7744 - val_loss: 18.8431 - val_mae: 39.2149 - val_loss_D_p: 0.2199 - val_loss_phi: 6.2966 - val_loss_p: 10.9473 - val_loss_mass_tau: 0.4899 - val_loss_mass_Higgs: 0.8893\n",
      "Epoch 20/25\n",
      "801/801 [==============================] - 9s 11ms/step - loss: 14.6714 - mae: 34.9267 - loss_D_p: 0.1442 - loss_phi: 5.8937 - loss_p: 7.5009 - loss_mass_tau: 0.3817 - loss_mass_Higgs: 0.7509 - val_loss: 17.3136 - val_mae: 39.0029 - val_loss_D_p: 0.2035 - val_loss_phi: 5.1422 - val_loss_p: 10.7124 - val_loss_mass_tau: 0.4409 - val_loss_mass_Higgs: 0.8146\n",
      "Epoch 21/25\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 14.6350 - mae: 34.9168 - loss_D_p: 0.1419 - loss_phi: 5.9377 - loss_p: 7.4341 - loss_mass_tau: 0.3768 - loss_mass_Higgs: 0.7443 - val_loss: 17.3571 - val_mae: 39.0994 - val_loss_D_p: 0.2029 - val_loss_phi: 5.0084 - val_loss_p: 10.9052 - val_loss_mass_tau: 0.4182 - val_loss_mass_Higgs: 0.8224\n",
      "Epoch 22/25\n",
      "801/801 [==============================] - 9s 11ms/step - loss: 14.6558 - mae: 34.9893 - loss_D_p: 0.1443 - loss_phi: 5.9079 - loss_p: 7.4570 - loss_mass_tau: 0.3884 - loss_mass_Higgs: 0.7581 - val_loss: 17.7925 - val_mae: 39.2923 - val_loss_D_p: 0.2021 - val_loss_phi: 5.4784 - val_loss_p: 10.9003 - val_loss_mass_tau: 0.4188 - val_loss_mass_Higgs: 0.7930\n",
      "Epoch 23/25\n",
      "801/801 [==============================] - 9s 12ms/step - loss: 14.8236 - mae: 35.0942 - loss_D_p: 0.1443 - loss_phi: 6.1333 - loss_p: 7.3891 - loss_mass_tau: 0.3989 - loss_mass_Higgs: 0.7581 - val_loss: 19.0630 - val_mae: 39.3358 - val_loss_D_p: 0.2176 - val_loss_phi: 6.7269 - val_loss_p: 10.6483 - val_loss_mass_tau: 0.5841 - val_loss_mass_Higgs: 0.8861\n",
      "Epoch 24/25\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 14.7711 - mae: 35.0604 - loss_D_p: 0.1425 - loss_phi: 6.1146 - loss_p: 7.3718 - loss_mass_tau: 0.3980 - loss_mass_Higgs: 0.7442 - val_loss: 17.0117 - val_mae: 39.1991 - val_loss_D_p: 0.1988 - val_loss_phi: 5.0655 - val_loss_p: 10.5650 - val_loss_mass_tau: 0.3955 - val_loss_mass_Higgs: 0.7869\n",
      "Epoch 25/25\n",
      "801/801 [==============================] - 11s 13ms/step - loss: 15.3445 - mae: 34.9911 - loss_D_p: 0.1441 - loss_phi: 6.7179 - loss_p: 7.3223 - loss_mass_tau: 0.4150 - loss_mass_Higgs: 0.7451 - val_loss: 16.9065 - val_mae: 39.0293 - val_loss_D_p: 0.1977 - val_loss_phi: 4.9240 - val_loss_p: 10.5836 - val_loss_mass_tau: 0.3941 - val_loss_mass_Higgs: 0.8070\n"
     ]
    }
   ],
   "source": [
    "# Start training here: would work better with small dataset !\n",
    "# Here copying the result from the KIT paper\n",
    "\n",
    "smear_px, smear_py = one_d(0), one_d(0)   #the smearing of the detector, we don't know yet what it is\n",
    "\n",
    "ref = [#smear_px,py                      #0\n",
    "       #one_d(1.776),                      #1\n",
    "       #df4[\"metx\"],                   #2\n",
    "       #df4[\"mety\"],                   #3\n",
    "       Mom4_to_tf(tau_1_vis.e),       #4\n",
    "       Mom4_to_tf(tau_1_vis.p_x),     #5\n",
    "       Mom4_to_tf(tau_1_vis.p_y),     #6\n",
    "       Mom4_to_tf(tau_1_vis.p_z),     #7\n",
    "       Mom4_to_tf(tau_2_vis.e),       #8\n",
    "       Mom4_to_tf(tau_2_vis.p_x),     #9 \n",
    "       Mom4_to_tf(tau_2_vis.p_y),     #10\n",
    "       Mom4_to_tf(tau_2_vis.p_z),     #11\n",
    "       #one_d(125),                    #12\n",
    "       #Mom4_to_tf(nu_1.e),            #13       corresponding to          #0\n",
    "       Mom4_to_tf(nu_1.p_x),          #14                                 #1\n",
    "       Mom4_to_tf(nu_1.p_y),          #15                                 #2\n",
    "       Mom4_to_tf(nu_1.p_z),          #16                                 #3\n",
    "       #Mom4_to_tf(nu_2.e),            #17                                 #4\n",
    "       Mom4_to_tf(nu_2.p_x),          #18                                 #5\n",
    "       Mom4_to_tf(nu_2.p_y),          #19                                 #6\n",
    "       Mom4_to_tf(nu_2.p_z),          #20                                 #7\n",
    "]\n",
    "\n",
    "\n",
    "#i_smear_px = 0\n",
    "#i_tau_mass = 0\n",
    "#i_smeared_met_px = 1\n",
    "#i_smeared_met_py = 2\n",
    "i_tau1_e = 0\n",
    "i_tau1_px = 1\n",
    "i_tau1_py = 2\n",
    "i_tau1_pz = 3\n",
    "i_tau2_e = 4\n",
    "i_tau2_px = 5\n",
    "i_tau2_py = 6\n",
    "i_tau2_pz = 7\n",
    "#i_gen_mass = 9\n",
    "#i_nu1_e = 12\n",
    "i_nu1_px = 8\n",
    "i_nu1_py = 9\n",
    "i_nu1_pz = 10\n",
    "#i_nu2_e = 16\n",
    "i_nu2_px = 11\n",
    "i_nu2_py = 12\n",
    "i_nu2_pz = 13\n",
    "\n",
    "####Very important, the batch size must be defined beforehand to get rid of two trainable params !\n",
    "B_size = 500#2**10\n",
    "\n",
    "m_Higgs_squared = 125**2\n",
    "\n",
    "\n",
    "m_tau_squared = 1.776**2\n",
    "\n",
    "def one_d_traning(val, shape_array):\n",
    "    return tf.constant(val, shape = shape_array.shape, dtype = np.float32)\n",
    "\n",
    "ref = tf.transpose(ref)\n",
    "\n",
    "def loss_D_p (y_true, y_pred):\n",
    "    global ratio_all\n",
    "    #calculting the difference between the the components, need to add the smearing of detector eventually\n",
    "    target_components = [i_nu1_px, i_nu1_py, i_nu1_pz, i_nu2_px, i_nu2_py, i_nu2_pz]\n",
    "    target_components_diff_list = []\n",
    "    for i in target_components: target_components_diff_list.append((y_true[:,i]-y_pred[:,i])**2)\n",
    "    dxyz = 0\n",
    "    for d in target_components_diff_list: dxyz+=d\n",
    "    return ratio_all * dxyz #tone it down cause it takes too much space\n",
    "\n",
    "def energy_nu (y_true, y_pred, number):\n",
    "    if number == 1:\n",
    "        return tf.sqrt(y_pred[:, i_nu1_px]**2 + y_pred[:, i_nu1_py]**2 + y_pred[:, i_nu1_pz]**2)\n",
    "    if number == 2:\n",
    "        return tf.sqrt(y_pred[:, i_nu2_px]**2 + y_pred[:, i_nu2_py]**2 + y_pred[:, i_nu2_pz]**2)\n",
    "\n",
    "def loss_mass_tau(y_true, y_pred):\n",
    "    #now we try only to use the y_pred for neutrino info, this is I guess their way of \n",
    "    #only training for neutrino info whilst keeping nice structure\n",
    "    #we are always assuming m=0\n",
    "    # note, we are taking y_tau as exact, we could choose not to...\n",
    "    global ratio_tau\n",
    "    E1 = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) \n",
    "    P1_squared = (y_true[:, i_tau1_px] + y_pred[:, i_nu1_px])**2 + (y_true[:, i_tau1_py] + y_pred[:, i_nu1_py])**2 + (y_true[:, i_tau1_pz] + y_pred[:, i_nu1_pz])**2\n",
    "    R1 = (E1**2 - P1_squared -  m_tau_squared)/m_Higgs_squared\n",
    "    \n",
    "    E2 = y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2) \n",
    "    P2_squared = (y_true[:, i_tau2_px] + y_pred[:, i_nu2_px])**2 + (y_true[:, i_tau2_py] + y_pred[:, i_nu2_py])**2 + (y_true[:, i_tau2_pz] + y_pred[:, i_nu2_pz])**2\n",
    "    R2 = (E2**2 - P2_squared - m_tau_squared)/m_Higgs_squared\n",
    "    return ratio_tau * (tf.math.abs(R1) + tf.abs(R2))\n",
    "\n",
    "\n",
    "def loss_mass_Higgs(y_true, y_pred):\n",
    "    global ratio_H\n",
    "    print(tf.convert_to_tensor(y_pred[:, 0]).shape, 'this is shape')\n",
    "    EH = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) + y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2)\n",
    "    px_H = y_true[:, i_tau1_px] + y_true[:, i_tau2_px] + y_pred[:, i_nu1_px] + y_pred[:, i_nu2_px]\n",
    "    py_H = y_true[:, i_tau1_py] + y_true[:, i_tau2_py] + y_pred[:, i_nu1_py] + y_pred[:, i_nu2_py]\n",
    "    pz_H = y_true[:, i_tau1_pz] + y_true[:, i_tau2_pz] + y_pred[:, i_nu1_pz] + y_pred[:, i_nu2_pz]\n",
    "#      y_true[:, i_gen_mass]**2\n",
    "    return ratio_H* tf.abs((EH**2-px_H**2-py_H**2-pz_H**2 -m_Higgs_squared)/m_Higgs_squared)\n",
    "\n",
    "\n",
    "def loss_phi(y_true, y_pred):\n",
    "    global ratio_phi\n",
    "    phi_diff_1 = (tf.math.atan2(y_pred[:, i_nu1_py],y_pred[:, i_nu1_px])-tf.math.atan2(y_true[:, i_nu1_py],y_true[:, i_nu1_px]))**2\n",
    "    phi_diff_2 = (tf.math.atan2(y_pred[:, i_nu2_py],y_pred[:, i_nu2_px])-tf.math.atan2(y_true[:, i_nu2_py],y_true[:, i_nu2_px]))**2\n",
    "\n",
    "    \n",
    "    return ratio_phi*tf.convert_to_tensor(phi_diff_1+phi_diff_2) #tone it up\n",
    "\n",
    "\n",
    "def loss_p(y_true, y_pred):\n",
    "    global ratio_p\n",
    "    delta_p1 = (energy_nu(y_true,y_pred, 1) - tf.sqrt(y_true[:, i_nu1_px]**2 + y_true[:, i_nu1_py]**2 + y_true[:, i_nu1_pz]**2))**2\n",
    "    delta_p2 = (energy_nu(y_true,y_pred, 2) - tf.sqrt(y_true[:, i_nu2_px]**2 + y_true[:, i_nu2_py]**2 + y_true[:, i_nu2_pz]**2))**2\n",
    "    \n",
    "    return ratio_p*tf.convert_to_tensor(delta_p1+delta_p2) #tone it up\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "\n",
    "    dxyz = loss_D_p(y_true, y_pred)\n",
    "    #dmet = loss_dmet(y_true, y_pred)\n",
    "    #dPTtaus = loss_dPTtaus(y_true, y_pred)\n",
    "    dphi = loss_phi(y_true,y_pred)\n",
    "    dtau = loss_mass_tau(y_true, y_pred)\n",
    "    dHiggs = loss_mass_Higgs(y_true, y_pred)\n",
    "    dp = loss_p(y_true, y_pred)\n",
    "    #dM = loss_dM_had(y_true, y_pred)\n",
    "\n",
    "    return dxyz + dtau + dHiggs + dphi + dp\n",
    "\n",
    "\n",
    "nu_1_training = np.array(nu_1)[0]\n",
    "\n",
    "print(nu_1_training.shape)\n",
    "\n",
    "n = -1\n",
    "\n",
    "x = np.array([\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              df4[\"metx\"],\n",
    "              df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              ip_x_1, \n",
    "              ip_y_1, \n",
    "              ip_z_1, \n",
    "              ip_x_2, \n",
    "              ip_y_2, \n",
    "              ip_z_2,\n",
    "              df4[\"ip_sig_2\"],\n",
    "              df4[\"ip_sig_1\"],\n",
    "              #Mom4_to_tf(nu_1.p_x),\n",
    "              #df4[\"ip_x_1\"], \n",
    "              #df4[\"ip_y_1\"], \n",
    "              #df4[\"ip_z_1\"],        #leading impact parameter\n",
    "              #df4[\"ip_x_2\"], \n",
    "              #df4[\"ip_y_2\"], \n",
    "              #df4[\"ip_z_2\"],\n",
    "              df4[\"met\"],  \n",
    "              sv_x_1, \n",
    "              sv_y_1, \n",
    "              sv_z_1,\n",
    "              #sv_x_2, \n",
    "              #sv_y_2, \n",
    "              #sv_z_2\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "ratio_tau = 10\n",
    "ratio_H = 4\n",
    "ratio_all = 1/10000\n",
    "ratio_p = 10/1500\n",
    "ratio_phi = 4\n",
    "\n",
    "\n",
    "trainFrac = 0.7\n",
    "numTrain = int(trainFrac*x.shape[0])\n",
    "x_train = x[:numTrain]\n",
    "y_train = y[:numTrain]\n",
    "\n",
    "x_val = x[numTrain:]\n",
    "y_val = y[numTrain:]\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "x2 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning\")(input_1)\n",
    "#,kernel_regularizer=tf.keras.regularizers.L1L2(2)\n",
    "#kernel_regularizer=tf.keras.regularizers.L2(0.2),\n",
    "#x4 = tf.keras.layers.Dropout(0.5, name=\"dropout\")(x2)\n",
    "# x6 = tf.keras.layers.Dense(11, activation = 'relu', name=\"learning1\")(x2)\n",
    "\n",
    "#x5 = tf.keras.layers.Dropout(0.4, name=\"dropout\")(x201\n",
    "# x3 = tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), name=\"learning2\")(x2)\n",
    "\n",
    "x3 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning2\")(x2)\n",
    "\n",
    "\n",
    "# kernel_regularizer=tf.keras.regularizers.L1L2(2)\n",
    "# kernel_regularizer=tf.keras.regularizers.L2(0.2),\n",
    "x4 = tf.keras.layers.Dropout(0.1, name=\"dropout2\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(300, activation = 'elu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "output = tf.keras.layers.Dense(14, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "y = ref\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae', loss_D_p, loss_phi, loss_p, loss_mass_tau, loss_mass_Higgs])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "    epochs=25,\n",
    "    batch_size = B_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All means\n",
      "All means\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Here verify the eta component\n",
    "\n",
    "\n",
    "def checks(x_array, length):\n",
    "    \n",
    "    regressed_array = model({\"lab_frame\": x_array})\n",
    "\n",
    "    #length = 0#numTrain\n",
    "\n",
    "    E_1 = norm([regressed_array[:, i_nu1_py],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz]])\n",
    "    E_2 = norm([regressed_array[:, i_nu2_py],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz]])\n",
    "\n",
    "\n",
    "    nu_1_regressed = Momentum4(E_1,regressed_array[:, i_nu1_px],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz])\n",
    "    nu_2_regressed = Momentum4(E_2,regressed_array[:, i_nu2_px],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz])\n",
    "\n",
    "\n",
    "    print('All means')\n",
    "\n",
    "    hist1 = nu_1_regressed.p_x\n",
    "    hist2 = Mom4_to_tf(nu_1.p_x)[length:]\n",
    "    hist_a = np.array(hist2-hist1) #\n",
    "    hist1 = nu_2_regressed.p_x\n",
    "    hist2 = Mom4_to_tf(nu_2.p_x)[length:]\n",
    "    hist_b = np.array(hist2-hist1) #\n",
    "    hist1 = nu_1_regressed.p_y\n",
    "    hist2 = Mom4_to_tf(nu_1.p_y)[length:]\n",
    "    hist_c = np.array(hist2-hist1) #\n",
    "    hist1 = nu_2_regressed.p_y\n",
    "    hist2 = Mom4_to_tf(nu_2.p_y)[length:]\n",
    "    hist_l = np.array(hist2-hist1) #\n",
    "    hist1 = nu_1_regressed.p_z\n",
    "    hist2 = Mom4_to_tf(nu_1.p_z)[length:]\n",
    "    hist_d = np.array(hist2-hist1) #\n",
    "    hist1 = nu_2_regressed.p_z\n",
    "    hist2 = Mom4_to_tf(nu_2.p_z)[length:]\n",
    "    hist_e = np.array(hist2-hist1) #\n",
    "    hist1 = nu_1_regressed.phi\n",
    "    hist2 = Mom4_to_tf(nu_1.phi)[length:]\n",
    "    hist_f = np.array(hist2-hist1) #\n",
    "    hist1 = nu_2_regressed.phi\n",
    "    hist2 = Mom4_to_tf(nu_2.phi)[length:]\n",
    "    hist_g = np.array(hist2-hist1) #\n",
    "    hist1 = nu_1_regressed.eta\n",
    "    hist2 = Mom4_to_tf(nu_1.eta)[length:]\n",
    "    hist_h = np.array(hist2-hist1) #\n",
    "    hist1 = nu_2_regressed.eta\n",
    "    hist2 = Mom4_to_tf(nu_2.eta)[length:]\n",
    "    hist_i = np.array(hist2-hist1) #\n",
    "    hist1 = nu_1_regressed.p\n",
    "    hist2 = Mom4_to_tf(nu_1.p)[length:]\n",
    "    hist_j = np.array(hist2-hist1) #\n",
    "    hist1 = nu_2_regressed.p\n",
    "    hist2 = Mom4_to_tf(nu_2.p)[length:]\n",
    "    hist_k = np.array(hist2-hist1) #\n",
    "\n",
    "    return [hist_a.mean(), hist_b.mean(), hist_c.mean(), hist_d.mean(), hist_l.mean(), hist_e.mean(), \n",
    "            hist_f.mean(), hist_g.mean(), hist_h.mean(), hist_i.mean(), hist_j.mean(), hist_k.mean()], [hist_a.std(), hist_b.std(), hist_c.std(), hist_d.std(), hist_l.std(), hist_e.std(), \n",
    "            hist_f.std(), hist_g.std(), hist_h.mean(), hist_i.std(), hist_j.std(), hist_k.std()]\n",
    "\n",
    "    \n",
    "\n",
    "mean_x_val, std_x_val = checks(x_val, numTrain)\n",
    "mean_x, std_x = checks(x, 0)\n",
    "\n",
    "\n",
    "    \n",
    "table = [mean_x_val, std_x_val, mean_x, std_x]\n",
    "\n",
    "table = np.array(table).T\n",
    "\n",
    "np.savetxt('check.txt', table, fmt = '%.2f')\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('means')\n",
    "\n",
    "# print('%.2f'%hist_a.mean())\n",
    "# print('%.2f'%hist_b.mean())\n",
    "# print('%.2f'%hist_c.mean())\n",
    "# print('%.2f'%hist_d.mean())\n",
    "# print('%.2f'%hist_b.mean())\n",
    "# print('%.2f'%hist_e.mean())\n",
    "# print('%.2f'%hist_f.mean())\n",
    "# print('%.2f'%hist_g.mean())\n",
    "# print('%.2f'%hist_h.mean())\n",
    "# print('%.2f'%hist_i.mean())\n",
    "# print('%.2f'%hist_j.mean())\n",
    "# print('%.2f'%hist_k.mean())\n",
    "\n",
    "\n",
    "# print ('stds')\n",
    "\n",
    "# print('%.2f'%hist_a.std())\n",
    "# print('%.2f'%hist_b.std())\n",
    "# print('%.2f'%hist_c.std())\n",
    "# print('%.2f'%hist_d.std())\n",
    "# print('%.2f'%hist_b.std())\n",
    "# print('%.2f'%hist_e.std())\n",
    "# print('%.2f'%hist_f.std())\n",
    "# print('%.2f'%hist_g.std())\n",
    "# print('%.2f'%hist_h.std())\n",
    "# print('%.2f'%hist_i.std())\n",
    "# print('%.2f'%hist_j.std())\n",
    "# print('%.2f'%hist_k.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then Try simple NN to regress only the phi component of the two neutrinos simultaneously\n",
    "\n",
    "x = [Mom4_to_tf(tau_1_vis.e),\n",
    "     Mom4_to_tf(tau_1_vis.p_x),\n",
    "     Mom4_to_tf(tau_1_vis.p_y),\n",
    "     Mom4_to_tf(tau_1_vis.p_z),\n",
    "     Mom4_to_tf(tau_1_vis.phi),\n",
    "     Mom4_to_tf(tau_2_vis.e),\n",
    "     Mom4_to_tf(tau_2_vis.p_x),\n",
    "     Mom4_to_tf(tau_2_vis.p_y),\n",
    "     Mom4_to_tf(tau_2_vis.p_z),\n",
    "     Mom4_to_tf(tau_2_vis.phi),\n",
    "     df4['met'],\n",
    "     df4['metx'],\n",
    "     df4['mety']\n",
    "    ]\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = [Mom4_to_tf(nu_1.phi), Mom4_to_tf(nu_2.phi)]\n",
    "\n",
    "y = tf.transpose(y)\n",
    "\n",
    "\n",
    "# def loss_fn_phi(y_true, y_pred):\n",
    "#     return (tf.math.cos(y_true)-tf.math.cos(y_pred))**2\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(64, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(64, activation = 'relu', name=\"learning2\")(x2)\n",
    "x4 = tf.keras.layers.Dropout(0.1, name=\"dropout\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "# t = tau_1_vis_loss\n",
    "# y_ = y.T\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss_fn_phi = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(loss = loss_fn_phi, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the 'quality check' section\n",
    "fig = plt.figure(figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-300-o,\\ninputs exclude mets, output exclude met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -6])\n",
    "hist2 = np.array(y_val[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -5])\n",
    "hist2 = np.array(y_val[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -4])\n",
    "hist2 = np.array(y_val[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -3])\n",
    "hist2 = np.array(y_val[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -2])\n",
    "hist2 = np.array(y_val[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -1])\n",
    "hist2 = np.array(y_val[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('4_vect_qual_500_500_B500_R_val.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here verify the phi component\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "hist1 = tf.math.atan2(model({\"lab_frame\": x_val})[:, i_nu1_py], model({\"lab_frame\": x_val})[:, i_nu1_px])\n",
    "hist2 = Mom4_to_tf(nu_1.phi)[numTrain:]\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_phi_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "hist1 = tf.math.atan2(model({\"lab_frame\": x_val})[:, i_nu2_py], model({\"lab_frame\": x_val})[:, i_nu2_px])\n",
    "hist2 = Mom4_to_tf(nu_2.phi)[numTrain:]\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_phi_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-2,2)\n",
    "plt.xlabel('Difference of phi component')\n",
    "\n",
    "plt.savefig('Phi_qual_500_500_B500_R_val.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here verify the eta component\n",
    "\n",
    "regressed_array = model({\"lab_frame\": x})\n",
    "\n",
    "E_1 = norm([regressed_array[:, i_nu1_py],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz]])\n",
    "E_2 = norm([regressed_array[:, i_nu2_py],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz]])\n",
    "\n",
    "\n",
    "nu_1_regressed = Momentum4(E_1,regressed_array[:, i_nu1_px],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz])\n",
    "nu_2_regressed = Momentum4(E_2,regressed_array[:, i_nu2_px],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz])\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "hist1 = nu_1_regressed.p_x\n",
    "hist2 = Mom4_to_tf(nu_1.p_x)#[numTrain:]\n",
    "hist_b = np.array(hist2-hist1) #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_eta_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "hist1 = nu_2_regressed.p_y\n",
    "hist2 = Mom4_to_tf(nu_2.p_y)#[numTrain:]\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_eta_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xlim(-2,2)\n",
    "plt.xlabel('Difference of phi component')\n",
    "\n",
    "plt.savefig('Phi_qual_500_500_B500_phi_all.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2\n",
    "\n",
    "         \n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "1356/1356 [==============================] - ETA: 0s - loss: 1917.6036 - mae: 39.9942WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "1356/1356 [==============================] - 15s 11ms/step - loss: 1917.6036 - mae: 39.9942 - val_loss: 2610.1282 - val_mae: 43.4733\n",
      "Epoch 2/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1699.7272 - mae: 39.6700 - val_loss: 2597.8945 - val_mae: 43.7552\n",
      "Epoch 3/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1677.6960 - mae: 39.7335 - val_loss: 2549.5977 - val_mae: 43.7872\n",
      "Epoch 4/50\n",
      "1356/1356 [==============================] - 14s 11ms/step - loss: 1663.7881 - mae: 39.7429 - val_loss: 2485.2976 - val_mae: 43.4555\n",
      "Epoch 5/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1651.1306 - mae: 39.8377 - val_loss: 2494.3550 - val_mae: 43.6106\n",
      "Epoch 6/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1643.9099 - mae: 39.8467 - val_loss: 2494.1179 - val_mae: 43.6432\n",
      "Epoch 7/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1634.3356 - mae: 39.8797 - val_loss: 2473.9553 - val_mae: 43.6530\n",
      "Epoch 8/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1630.4716 - mae: 39.8815 - val_loss: 2447.2979 - val_mae: 43.7359\n",
      "Epoch 9/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1622.0109 - mae: 39.8931 - val_loss: 2495.3511 - val_mae: 43.7103\n",
      "Epoch 10/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1621.7166 - mae: 39.9172 - val_loss: 2460.1260 - val_mae: 43.6831\n",
      "Epoch 11/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1612.5052 - mae: 39.9166 - val_loss: 2489.4316 - val_mae: 43.7947\n",
      "Epoch 12/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1608.5955 - mae: 39.9081 - val_loss: 2459.4163 - val_mae: 43.7122\n",
      "Epoch 13/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1608.4413 - mae: 39.9142 - val_loss: 2539.3528 - val_mae: 43.5760\n",
      "Epoch 14/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1605.4712 - mae: 39.9686 - val_loss: 2489.7297 - val_mae: 43.7217\n",
      "Epoch 15/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1597.7333 - mae: 39.9655 - val_loss: 2440.1936 - val_mae: 43.7354\n",
      "Epoch 16/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1597.1824 - mae: 39.9943 - val_loss: 2457.0183 - val_mae: 43.6589\n",
      "Epoch 17/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1594.3080 - mae: 40.0058 - val_loss: 2459.9612 - val_mae: 43.7222\n",
      "Epoch 18/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1594.0132 - mae: 40.0000 - val_loss: 2500.2651 - val_mae: 43.9251\n",
      "Epoch 19/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1583.8054 - mae: 40.0577 - val_loss: 2491.0281 - val_mae: 43.9807\n",
      "Epoch 20/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1583.2651 - mae: 40.0868 - val_loss: 2514.5571 - val_mae: 43.5606\n",
      "Epoch 21/50\n",
      "1356/1356 [==============================] - 17s 12ms/step - loss: 1577.4895 - mae: 40.0679 - val_loss: 2529.6833 - val_mae: 44.0536\n",
      "Epoch 22/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1576.4879 - mae: 40.1033 - val_loss: 2492.8384 - val_mae: 43.8942\n",
      "Epoch 23/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1571.1874 - mae: 40.1227 - val_loss: 2472.2207 - val_mae: 44.0561\n",
      "Epoch 24/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1571.9403 - mae: 40.1778 - val_loss: 2503.5684 - val_mae: 43.8279\n",
      "Epoch 25/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1569.0905 - mae: 40.1779 - val_loss: 2468.4170 - val_mae: 43.9947\n",
      "Epoch 26/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1565.1615 - mae: 40.2022 - val_loss: 2486.9824 - val_mae: 43.9508\n",
      "Epoch 27/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1558.7700 - mae: 40.2188 - val_loss: 2475.9065 - val_mae: 43.9619\n",
      "Epoch 28/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1558.1907 - mae: 40.2382 - val_loss: 2498.5112 - val_mae: 44.0257\n",
      "Epoch 29/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1552.7136 - mae: 40.2349 - val_loss: 2520.7000 - val_mae: 44.1866\n",
      "Epoch 30/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1550.0940 - mae: 40.2576 - val_loss: 2521.6653 - val_mae: 44.0729\n",
      "Epoch 31/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1544.7440 - mae: 40.2847 - val_loss: 2521.4771 - val_mae: 44.0893\n",
      "Epoch 32/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1541.3024 - mae: 40.3077 - val_loss: 2508.6287 - val_mae: 44.3325\n",
      "Epoch 33/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1540.9872 - mae: 40.3195 - val_loss: 2512.3147 - val_mae: 44.1629\n",
      "Epoch 34/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1533.9865 - mae: 40.3493 - val_loss: 2603.9446 - val_mae: 44.5622\n",
      "Epoch 35/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1531.9338 - mae: 40.3968 - val_loss: 2521.9021 - val_mae: 44.1406\n",
      "Epoch 36/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1533.7352 - mae: 40.3765 - val_loss: 2516.8896 - val_mae: 44.1622\n",
      "Epoch 37/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1524.0305 - mae: 40.4048 - val_loss: 2547.2910 - val_mae: 44.3408\n",
      "Epoch 38/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1518.9465 - mae: 40.4270 - val_loss: 2515.4958 - val_mae: 44.2166\n",
      "Epoch 39/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1518.4982 - mae: 40.4616 - val_loss: 2521.7163 - val_mae: 44.2120\n",
      "Epoch 40/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1514.5626 - mae: 40.4721 - val_loss: 2532.1399 - val_mae: 44.0862\n",
      "Epoch 41/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1511.2383 - mae: 40.4589 - val_loss: 2528.5022 - val_mae: 44.4758\n",
      "Epoch 42/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1508.4866 - mae: 40.5074 - val_loss: 2568.1223 - val_mae: 44.6798\n",
      "Epoch 43/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1507.3695 - mae: 40.5364 - val_loss: 2515.4834 - val_mae: 44.3125\n",
      "Epoch 44/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1500.6742 - mae: 40.5202 - val_loss: 2521.2170 - val_mae: 44.2737\n",
      "Epoch 45/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1492.9366 - mae: 40.5404 - val_loss: 2560.5671 - val_mae: 44.5046\n",
      "Epoch 46/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1492.0536 - mae: 40.5596 - val_loss: 2558.2803 - val_mae: 44.4379\n",
      "Epoch 47/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1490.9824 - mae: 40.6010 - val_loss: 2512.8352 - val_mae: 44.4544\n",
      "Epoch 48/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1488.2098 - mae: 40.6221 - val_loss: 2557.1802 - val_mae: 44.5234\n",
      "Epoch 49/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1477.4401 - mae: 40.6374 - val_loss: 2577.4529 - val_mae: 44.5623\n",
      "Epoch 50/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1483.1232 - mae: 40.6474 - val_loss: 2570.5508 - val_mae: 44.5914\n"
     ]
    }
   ],
   "source": [
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              #df4[\"metx\"],\n",
    "              #df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              #df4[\"met\"]\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(300, activation = 'elu', name=\"learning2\")(x2)\n",
    "x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(16, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "#tf.keras.losses.MeanSquaredError() #common to the 4 iterations\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 500-300-500-o,\\ninputs exclude mets, output exclude met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('Quality_500_300_500_1_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Traning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.22657775e-05 8.39220302e-05 7.18750016e-06 ... 6.93282782e-05\n",
      " 1.77967231e-05 9.17172201e-06], shape=(571700,), dtype=float32)\n",
      "tf.Tensor([0.02934769 0.20486112 0.01050362 ... 0.12745337 0.02100937 0.00848406], shape=(571700,), dtype=float32)\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 571700, 19) for input Tensor(\"lab_frame_4:0\", shape=(None, 571700, 19), dtype=float32), but it was called on an input with incompatible shape (None, 19).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 571700, 19) for input Tensor(\"lab_frame_4:0\", shape=(None, 571700, 19), dtype=float32), but it was called on an input with incompatible shape (None, 19).\n",
      "800/801 [============================>.] - ETA: 0s - loss: 1329.1543 - mae: 38.1922WARNING:tensorflow:Model was constructed with shape (None, 571700, 19) for input Tensor(\"lab_frame_4:0\", shape=(None, 571700, 19), dtype=float32), but it was called on an input with incompatible shape (None, 19).\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 1329.0811 - mae: 38.1916 - val_loss: nan - val_mae: nan\n",
      "Epoch 2/50\n",
      "582/801 [====================>.........] - ETA: 2s - loss: 1029.9915 - mae: 37.2705"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-6673a7632ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m history = model.fit(x, y, validation_split = 0.3,\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     batch_size = 500)\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here copying the result from the KIT paper\n",
    "\n",
    "smear_px, smear_py = one_d(0), one_d(0)   #the smearing of the detector, we don't know yet what it is\n",
    "\n",
    "ref = [#smear_px,py                      #0\n",
    "       one_d(1.776),                      #1\n",
    "       df4[\"metx\"],                   #2\n",
    "       df4[\"mety\"],                   #3\n",
    "       Mom4_to_tf(tau_1_vis.e),       #4\n",
    "       Mom4_to_tf(tau_1_vis.p_x),     #5\n",
    "       Mom4_to_tf(tau_1_vis.p_y),     #6\n",
    "       Mom4_to_tf(tau_1_vis.p_z),     #7\n",
    "       Mom4_to_tf(tau_2_vis.e),       #8\n",
    "       Mom4_to_tf(tau_2_vis.p_x),     #9 \n",
    "       Mom4_to_tf(tau_2_vis.p_y),     #10\n",
    "       Mom4_to_tf(tau_2_vis.p_z),     #11\n",
    "       one_d(125),                    #12\n",
    "       #Mom4_to_tf(nu_1.e),            #13       corresponding to          #0\n",
    "       Mom4_to_tf(nu_1.p_x),          #14                                 #1\n",
    "       Mom4_to_tf(nu_1.p_y),          #15                                 #2\n",
    "       Mom4_to_tf(nu_1.p_z),          #16                                 #3\n",
    "       #Mom4_to_tf(nu_2.e),            #17                                 #4\n",
    "       Mom4_to_tf(nu_2.p_x),          #18                                 #5\n",
    "       Mom4_to_tf(nu_2.p_y),          #19                                 #6\n",
    "       Mom4_to_tf(nu_2.p_z),          #20                                 #7\n",
    "]\n",
    "\n",
    "\n",
    "#i_smear_px = 0\n",
    "i_tau_mass = 0\n",
    "i_smeared_met_px = 1\n",
    "i_smeared_met_py = 2\n",
    "i_tau1_e = 3\n",
    "i_tau1_px = 4\n",
    "i_tau1_py = 5\n",
    "i_tau1_pz = 6\n",
    "i_tau2_e = 7\n",
    "i_tau2_px = 8\n",
    "i_tau2_py = 9\n",
    "i_tau2_pz = 10\n",
    "i_gen_mass = 11\n",
    "#i_nu1_e = 12\n",
    "i_nu1_px = 12\n",
    "i_nu1_py = 13\n",
    "i_nu1_pz = 14\n",
    "#i_nu2_e = 16\n",
    "i_nu2_px = 15\n",
    "i_nu2_py = 16\n",
    "i_nu2_pz = 17\n",
    "\n",
    "\n",
    "m_tau_squared = tf.transpose(one_d(1.776)**2)\n",
    "\n",
    "ref = tf.transpose(ref)\n",
    "\n",
    "def loss_D_p (y_true, y_pred):\n",
    "    #calculting the difference between the the components, need to add the smearing of detector eventually\n",
    "    target_components = [i_nu1_px, i_nu1_py, i_nu1_pz, i_nu2_px, i_nu2_py, i_nu2_pz]\n",
    "    target_components_diff_list = []\n",
    "    for i in target_components: target_components_diff_list.append((y_true[:,i]-y_pred[:,i])**2)\n",
    "    dxyz = 0\n",
    "    for d in target_components_diff_list: dxyz+=d\n",
    "    return dxyz\n",
    "\n",
    "def energy_nu (y_true, y_pred, number):\n",
    "    if number == 1:\n",
    "        return tf.sqrt(y_pred[:, i_nu1_px]**2 + y_pred[:, i_nu1_py]**2 + y_pred[:, i_nu1_pz]**2)\n",
    "    if number == 2:\n",
    "        return tf.sqrt(y_pred[:, i_nu2_px]**2 + y_pred[:, i_nu2_py]**2 + y_pred[:, i_nu2_pz]**2)\n",
    "\n",
    "def loss_mass_tau(y_true, y_pred):\n",
    "    #now we try only to use the y_pred for neutrino info, this is I guess their way of \n",
    "    #only training for neutrino info whilst keeping nice structure\n",
    "    #we are always assuming m=0\n",
    "    # note, we are taking y_tau as exact, we could choose not to...\n",
    "    E1 = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) \n",
    "    P1_squared = (y_true[:, i_tau1_px] + y_pred[:, i_nu1_px])**2 + (y_true[:, i_tau1_py] + y_pred[:, i_nu1_py])**2 + (y_true[:, i_tau1_pz] + y_pred[:, i_nu1_pz])**2\n",
    "    R1 = (E1**2 - P1_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    \n",
    "    E2 = y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2) \n",
    "    P2_squared = (y_true[:, i_tau2_px] + y_pred[:, i_nu2_px])**2 + (y_true[:, i_tau2_py] + y_pred[:, i_nu2_py])**2 + (y_true[:, i_tau2_pz] + y_pred[:, i_nu2_pz])**2\n",
    "    R2 = (E2**2 - P2_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    return tf.math.abs(R1) + tf.abs(R2)\n",
    "\n",
    "\n",
    "def loss_mass_Higgs(y_true, y_pred):\n",
    "    EH = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) + y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2)\n",
    "    px_H = y_true[:, i_tau1_px] + y_true[:, i_tau2_px] + y_pred[:, i_nu1_px] + y_pred[:, i_nu2_px]\n",
    "    py_H = y_true[:, i_tau1_py] + y_true[:, i_tau2_py] + y_pred[:, i_nu1_py] + y_pred[:, i_nu2_py]\n",
    "    pz_H = y_true[:, i_tau1_pz] + y_true[:, i_tau2_pz] + y_pred[:, i_nu1_pz] + y_pred[:, i_nu2_pz]\n",
    "    \n",
    "    return tf.abs((EH**2-px_H**2-py_H**2-pz_H**2 - y_true[:, i_gen_mass]**2)/(y_true[:,i_gen_mass])**2)\n",
    "    \n",
    "\n",
    "print(loss_mass_tau(ref,ref))\n",
    "print(loss_mass_Higgs(ref,ref))\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "\n",
    "    dxyz = loss_D_p(y_true, y_pred)\n",
    "    #dmet = loss_dmet(y_true, y_pred)\n",
    "    #dPTtaus = loss_dPTtaus(y_true, y_pred)\n",
    "    dtau = loss_mass_tau(y_true, y_pred)\n",
    "    dHiggs = loss_mass_Higgs(y_true, y_pred)\n",
    "    #dM = loss_dM_had(y_true, y_pred)\n",
    "\n",
    "    return dxyz + dtau + dHiggs\n",
    "\n",
    "\n",
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              df4[\"metx\"],\n",
    "              df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              df4[\"met\"],\n",
    "\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning3\")(x2)\n",
    "#x4 = tf.keras.layers.Dense(100, activation = 'elu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(18, name=\"output\")(x3)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "model.summary\n",
    "\n",
    "#tf.keras.losses.MeanSquaredError() #common to the 4 iterations\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "\n",
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-o,\\ninputs include mets, output include met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('Quality_300_300_include_met_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0.1561270904944474 0\n"
     ]
    }
   ],
   "source": [
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "\n",
    "print(len(model({\"lab_frame\": x})[1]))\n",
    "hist2 = np.array(y[:, -2])\n",
    "need = \"nu_1_E\"\n",
    "dd = 0\n",
    "\n",
    "def frac(d = -2):\n",
    "    difference = y[:, 0]-model({\"lab_frame\": x})[:, 0]\n",
    "    difference = np.reshape(difference, [-1])\n",
    "    l = np.where(abs(difference)<=10**(d),1,0)\n",
    "    print(float(float(np.sum(l))/len(l)), d)\n",
    "    return float(float(np.sum(l))/len(l))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(hist1, bins = 100, alpha = 0.5,label = \"NN %s component : fraction($\\Delta$<$10^{%.1f}$)=%.3f \"%(need, dd, frac(dd)))\n",
    "plt.hist(hist2, bins = 100, alpha = 0.5,label = 'True %s - Features: phi_CP_1 (fixed)'%need)      \n",
    "plt.ylabel(\"Frequency\", fontsize = 'x-large')\n",
    "plt.xlabel(\"%s\"%(need), fontsize = 'x-large')\n",
    "plt.grid()\n",
    "#plt.xlim(0,400)\n",
    "plt.legend()#prop = {'size', 10})\n",
    "plt.savefig('neutrino_next_2_300_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculation:\n",
    "    \"\"\"\n",
    "    Class for calculating the aco_angle variables\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        #this is the function doing all the calculations manually just takes as input the dataframe\n",
    "        #The different *initial* 4 vectors, (E,px,py,pz)\n",
    "        self.pi_1 = np.array([df[\"pi_E_1\"],df[\"pi_px_1\"],df[\"pi_py_1\"],df[\"pi_pz_1\"]])\n",
    "        self.pi_2 = np.array([df[\"pi_E_2\"],df[\"pi_px_2\"],df[\"pi_py_2\"],df[\"pi_pz_2\"]])\n",
    "\n",
    "        self.pi0_1 = np.array([df[\"pi0_E_1\"],df[\"pi0_px_1\"],df[\"pi0_py_1\"],df[\"pi0_pz_1\"]])\n",
    "        self.pi0_2 = np.array([df[\"pi0_E_2\"],df[\"pi0_px_2\"],df[\"pi0_py_2\"],df[\"pi0_pz_2\"]])\n",
    "\n",
    "        #Charged and neutral pion momenta\n",
    "        self.pi_1_4Mom = Momentum4(df[\"pi_E_1\"],df[\"pi_px_1\"],df[\"pi_py_1\"],df[\"pi_pz_1\"])\n",
    "        self.pi_2_4Mom = Momentum4(df[\"pi_E_2\"],df[\"pi_px_2\"],df[\"pi_py_2\"],df[\"pi_pz_2\"])\n",
    "\n",
    "        #Same for the pi0\n",
    "        self.pi0_1_4Mom = Momentum4(df[\"pi0_E_1\"],df[\"pi0_px_1\"],df[\"pi0_py_1\"],df[\"pi0_pz_1\"])\n",
    "        self.pi0_2_4Mom = Momentum4(df[\"pi0_E_2\"],df[\"pi0_px_2\"],df[\"pi0_py_2\"],df[\"pi0_pz_2\"])\n",
    "\n",
    "        self.impact_param_1 = Momentum4(np.zeros(len(df[\"ip_x_1\"])),df[\"ip_x_1\"],df[\"ip_y_1\"],df[\"ip_z_1\"])\n",
    "        self.impact_param_2 = Momentum4(np.zeros(len(df[\"ip_x_2\"])),df[\"ip_x_2\"],df[\"ip_y_2\"],df[\"ip_z_2\"])\n",
    "\n",
    "        #comment or uncomment depending on which aco_angle you want \n",
    "        #self.pi0_1_4Mom = self.impact_param_1\n",
    "        #self.pi0_2_4Mom = self.impact_param_2\n",
    "\n",
    "        #This is the COM frame of the two charged pions w.r.t. which we'll boost\n",
    "        self.ref_COM_4Mom = Momentum4(self.pi_1_4Mom+self.pi_2_4Mom)\n",
    "        boost = Momentum4(self.ref_COM_4Mom[0], -self.ref_COM_4Mom[1], -self.ref_COM_4Mom[2], -self.ref_COM_4Mom[3])\n",
    "        \n",
    "        \n",
    "        boost = -self.ref_COM_4Mom\n",
    "        #energies=[df4[\"pi_E_1\"],df4[\"pi_E_2\"],df4[\"pi0_E_1\"],df4[\"pi0_E_2\"]]\n",
    "\n",
    "        #Lorentz boost everything in the ZMF of the two charged pions\n",
    "        self.pi0_1_4Mom_star = self.pi0_1_4Mom.boost_particle(boost)\n",
    "        self.pi0_2_4Mom_star = self.pi0_2_4Mom.boost_particle(boost)\n",
    "\n",
    "        #Lorentz boost everything in the ZMF of the two neutral pions\n",
    "        self.pi_1_4Mom_star = self.pi_1_4Mom.boost_particle(boost)\n",
    "        self.pi_2_4Mom_star = self.pi_2_4Mom.boost_particle(boost)\n",
    "\n",
    "\n",
    "        #calculating the perpependicular component\n",
    "        pi0_1_3Mom_star_perp=cross_product(self.pi0_1_4Mom_star[1:], self.pi_1_4Mom_star[1:])\n",
    "        pi0_2_3Mom_star_perp=cross_product(self.pi0_2_4Mom_star[1:], self.pi_2_4Mom_star[1:])\n",
    "\n",
    "        #Now normalise:\n",
    "        pi0_1_3Mom_star_perp=pi0_1_3Mom_star_perp/norm(pi0_1_3Mom_star_perp)\n",
    "        pi0_2_3Mom_star_perp=pi0_2_3Mom_star_perp/norm(pi0_2_3Mom_star_perp)\n",
    "\n",
    "        self.pi0_1_4Mom_star_perp = [self.pi0_1_4Mom_star[0], pi0_1_3Mom_star_perp[0], \n",
    "                                     pi0_1_3Mom_star_perp[1], pi0_1_3Mom_star_perp[2]]\n",
    "\n",
    "        self.pi0_2_4Mom_star_perp = [self.pi0_1_4Mom_star[0], pi0_2_3Mom_star_perp[0], \n",
    "                                     pi0_2_3Mom_star_perp[1], pi0_2_3Mom_star_perp[2]]\n",
    "\n",
    "        #Calculating phi_star\n",
    "        self.phi_CP_unshifted = np.arccos(dot_product(pi0_1_3Mom_star_perp,pi0_2_3Mom_star_perp))\n",
    "        \n",
    "        print(self.phi_CP_unshifted[:10],'This is phi_CP')\n",
    "\n",
    "        self.phi_CP = self.phi_CP_unshifted\n",
    "        \n",
    "        print(pi0_1_3Mom_star_perp[:,23], 'this is pi0_1_3mom')\n",
    "\n",
    "        #The energy ratios\n",
    "        self.y_T = np.array(df['y_1_1']*df['y_1_2'])\n",
    "\n",
    "        #The O variable\n",
    "        cross = np.array(np.cross(pi0_1_3Mom_star_perp.transpose(),pi0_2_3Mom_star_perp.transpose()).transpose())\n",
    "        self.bigO = dot_product(self.pi_2_4Mom_star[1:],cross)\n",
    "        \n",
    "        print(self.bigO[:10], '\\n this is big0')\n",
    "\n",
    "        #perform the shift w.r.t. O* sign\n",
    "        \n",
    "        \n",
    "        #phi_CP=np.where(self.bigO>=0, 2*np.pi-self.phi_CP_unshifted, self.phi_CP_unshifted)\n",
    "        self.phi_CP_1 = np.where(self.bigO>=0, 2*np.pi-self.phi_CP_unshifted, self.phi_CP_unshifted)\n",
    "        \n",
    "        print(self.phi_CP_1[:10], '\\n this is after first shft')\n",
    "\n",
    "       # self.phi_CP_2 = np.where(self.y_T<=0, self.phi_CP+np.pi, self.phi_CP-np.pi)\n",
    "\n",
    "        #additionnal shift that needs to be done do see differences between odd and even scenarios, with y=Energy ratios\n",
    "        self.phi_CP = np.where(self.y_T>=0, np.where(self.phi_CP_1<np.pi, self.phi_CP_1+np.pi, self.phi_CP_1-np.pi), self.phi_CP_1)\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        print(self.phi_CP[:10], 'this is full')\n",
    "        \n",
    "        self.y = df[\"aco_angle_1\"]\n",
    "    \n",
    "    def checks(self):\n",
    "\n",
    "        target = [self.df[\"aco_angle_1\"]]#self.df[\"aco_angle_7\"]]\n",
    "        y = tf.transpose(tf.convert_to_tensor(target, dtype=np.float32))\n",
    "\n",
    "        inputs = [self.pi0_1_4Mom, self.pi_1_4Mom, self.pi0_2_4Mom, self.pi_2_4Mom]\n",
    "        x = tf.convert_to_tensor(inputs, dtype=np.float32)\n",
    "        x = tf.transpose(x, [2, 0, 1])\n",
    "        \n",
    "        k = tf.convert_to_tensor([\n",
    "                          self.impact_param_1[0], self.impact_param_1[1], self.impact_param_1[2], self.impact_param_1[3],\n",
    "                          #self.pi0_1_4Mom[0], self.pi0_1_4Mom[1], self.pi0_1_4Mom[2], self.pi0_1_4Mom[3],\n",
    "                          self.pi_1_4Mom[0], self.pi_1_4Mom[1], self.pi_1_4Mom[2], self.pi_1_4Mom[3],\n",
    "                          #self.pi0_2_4Mom[0], self.pi0_2_4Mom[1], self.pi0_2_4Mom[2], self.pi0_2_4Mom[3],\n",
    "                          self.impact_param_2[0], self.impact_param_2[1], self.impact_param_2[2], self.impact_param_2[3],\n",
    "                          self.pi_2_4Mom[0], self.pi_2_4Mom[1], self.pi_2_4Mom[2], self.pi_2_4Mom[3]],\n",
    "                         dtype=np.float32)\n",
    "\n",
    "# the extra info we are giving\n",
    "        l = tf.convert_to_tensor([self.y_T], dtype=np.float32)\n",
    "\n",
    "        return x,y,k,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_1 = nu_1 + tau_1_vis\n",
    "tau_2 = nu_2 + tau_2_vis\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1.m,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (np.array(tau_1.m).mean(), np.array(tau_1.m).std()))\n",
    "plt.hist(np.array(tau_2.m),  bins = 1000, alpha = 0.5, label = 'dot product |tau_2_vis| and |nu_2|\\nmean=%.2f, std=%.2f'% (np.array(tau_2.m).mean(), np.array(tau_2.m).std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0,6)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Mass of sum of rho decay products and tau neutrino', fontsize = 'large')\n",
    "plt.title('Checking that tau neutrino and visible decay products\\nsum up to tau (RM9999)', fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.show()\n",
    "plt.savefig('sum_to_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-71f6fbb4e571>:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "Higgs = tau_1 + tau_2 \n",
    "plt.figure()\n",
    "plt.hist(Higgs.m,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (np.array(Higgs.m).mean(), np.array(Higgs.m).std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xlim(0,6)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Mass of the sum of the two taus', fontsize = 'large')\n",
    "plt.title('Checking that the two taus\\nsum up to Higgs (RM9999)', fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.show()\n",
    "plt.savefig('sum_to_Higgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99919824 0.99922621 0.99976824 ... 0.99905886 0.99952941 0.99982143]\n",
      "[0.99770581 0.99983855 0.99924261 0.98171425 0.9989211  0.99978589\n",
      " 0.99934793 0.99860659 0.99942672 0.99745414]\n"
     ]
    }
   ],
   "source": [
    "#check of the colinearity approximation\n",
    "\n",
    "#Next up: colinarity, tau mass (Kingsley checked) and then Higgs. \n",
    "\n",
    "dot_prod_1 = np.einsum('ia,ia->a',tau_1_vis[1:, ...], nu_1[1:, ...])\n",
    "dot_prod_1 = dot_prod_1/(norm(tau_1_vis[1:, ...])* norm(nu_1[1:, ...]))\n",
    "\n",
    "dot_prod_2 = np.einsum('ia,ia->a',tau_2_vis[1:, ...], nu_2[1:, ...])\n",
    "dot_prod_2 = dot_prod_2/(norm(tau_2_vis[1:, ...])* norm(nu_2[1:, ...]))\n",
    "\n",
    "print(dot_prod_1)\n",
    "print(dot_prod_2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(dot_prod_1,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (dot_prod_1.mean(), dot_prod_1.std()))\n",
    "plt.hist(dot_prod_2,  bins = 1000, alpha = 0.5, label = 'dot product |tau_2_vis| and |nu_2|\\nmean=%.2f, std=%.2f'% (dot_prod_2.mean(), dot_prod_2.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0.95,1.05)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Dot product between rho decay products and tau neutrino', fontsize = 'large')\n",
    "plt.title('Checking the colinarity approximation between\\ntau neutrino and visible decay products', fontsize = 'xx-large', weight = 'bold')\n",
    "\n",
    "plt.savefig('Colinearity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the colinear approximation ? can calculate the dot product between the visible decay products and the nus\n",
    "check_x_1 = np.array(nu_1.p_x-tau_1_vis.p_x)#dot_product(tau_1_vis[1:]/norm(tau_1_vis[1:]), nu_1[1:]/norm(nu_1[1:])))\n",
    "check_x_2 = np.array(nu_2.p_x-tau_2_vis.p_x)#dot_product(tau_2_vis[1:]/norm(tau_2_vis[1:]), nu_2[1:]/norm(nu_2[1:])))\n",
    "check_y_1 = np.array(nu_1.p_y-tau_1_vis.p_y)\n",
    "check_y_2 = np.array(nu_2.p_y-tau_2_vis.p_y)\n",
    "\n",
    "\n",
    "check_z_1 = []\n",
    "check_z_2 = []\n",
    "\n",
    "for i in range (len(nu_1.p_z)):\n",
    "    if nu_1.p_z[i] != 9999:\n",
    "        check_z_1.append(nu_1.p_z[i]-tau_1_vis.p_z[i])\n",
    "    if nu_2.p_z[i] != 9999:\n",
    "        check_z_2.append(nu_2.p_z[i]-tau_2_vis.p_z[i])\n",
    "\n",
    "check_z_1 = np.array(check_z_1)\n",
    "check_z_2 = np.array(check_z_2)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1_vis.p_x - nu_1.p_x, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_px and nu_1_px\\nmean=%.2f, std=%.2f'% (check_x_1.mean(),check_x_1.std()))\n",
    "plt.hist(tau_2_vis.p_x - nu_2.p_x, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_px and nu_2_px\\nmean=%.2f, std=%.2f'% (check_x_2.mean(),check_x_2.std()))\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_x')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(tau_1_vis.p_y - nu_1.p_y, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_py and nu_1_py\\nmean=%.2f, std=%.2f'% (check_y_1.mean(),check_y_1.std()))\n",
    "plt.hist(tau_2_vis.p_y - nu_2.p_y, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_py and nu_2_py\\nmean=%.2f, std=%.2f'% (check_y_2.mean(),check_y_2.std()))\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_y')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(check_z_1, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_pz and nu_1_pz\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_z_1.mean(),check_z_1.std()))\n",
    "plt.hist(check_z_2, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_pz and nu_2_pz\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_z_2.mean(),check_z_2.std()))\n",
    "\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_z')\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_E_1 = []#dot_product(tau_1_vis[1:]/norm(tau_1_vis[1:]), nu_1[1:]/norm(nu_1[1:])))\n",
    "check_E_2 = []#dot_product(tau_2_vis[1:]/norm(tau_2_vis[1:]), nu_2[1:]/norm(nu_2[1:])))\n",
    "\n",
    "for i in range (len(nu_1.e)):\n",
    "    if nu_1.e[i]!=9999:\n",
    "        check_E_1.append(nu_1.e[i]-tau_1_vis.e[i])\n",
    "    if nu_2.e[i]!=9999:\n",
    "        check_E_2.append(nu_2.e[i]-tau_2_vis.e[i])\n",
    "\n",
    "check_E_1 = np.array(check_E_1)\n",
    "check_E_2 = np.array(check_E_2)\n",
    "plt.figure()\n",
    "plt.hist(check_E_1, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_E and nu_1_E\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_E_1.mean(),check_E_1.std()))\n",
    "plt.hist(check_E_2, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_E and nu_2_E\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_E_2.mean(),check_E_2.std()))\n",
    "\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino energy components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between energies', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-500,500)\n",
    "plt.legend()\n",
    "plt.savefig('Check_energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-75e86c0dc241>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig('Compare_nu_E_to_met')\n"
     ]
    }
   ],
   "source": [
    "sum_nu = nu_1 + nu_2\n",
    "\n",
    "met = np.array(df4[\"met\"])\n",
    "sum_energies=[]\n",
    "met_ref = []\n",
    "x = [0,500]\n",
    "\n",
    "for i in range (len(nu_1.e)):\n",
    "    if nu_1.e[i]!=9999 and nu_2.e[i]!=9999:\n",
    "        sum_energies.append(sum_nu.e[i])\n",
    "        met_ref.append(met[i])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(met_ref, sum_energies, 'gx')\n",
    "plt.plot(x,x, 'k--', label = 'y=x')\n",
    "plt.xlabel(\"Missing transverse energy\", fontsize = 'x-large')\n",
    "plt.ylabel(\"Energy of the sum of the neutrinos\", fontsize = 'x-large')\n",
    "plt.title(\"Sanity check, removing 9999 \\n gen level nu energies and met\", fontsize = 'xx-large', weight = 'bold')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0,500)\n",
    "plt.savefig('Compare_nu_E_to_met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-d78cf37700c1>:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "x_ref = np.array([0, 6])\n",
    "x_ref2 = [6,0]\n",
    "\n",
    "gen_phitt = np.array(df4[\"gen_phitt\"][:2000])*2*np.pi/180+np.pi\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_ref, x_ref, '--', label = 'x=y')\n",
    "plt.plot(x_ref, x_ref2, '--', label = 'y=-x')\n",
    "\n",
    "plt.hist2d(df4[\"aco_angle_1\"], df4[\"gen_phitt\"], bins=50)\n",
    "#plt.plot(df4[\"aco_angle_1\"][:2000], gen_phitt, 'bx', label='aco_angle_1')\n",
    "#plt.plot(df4[\"aco_angle_5\"][:100], gen_phitt, 'gx', label='aco_angle_5')\n",
    "#plt.plot(df4[\"aco_angle_6\"][:100], gen_phitt, 'rx', label='aco_angle_6')\n",
    "#plt.plot(df4[\"aco_angle_7\"][:100], gen_phitt, 'kx', label='aco_angle_7')\n",
    "#plt.xlabel(\"aco_angles\", fontsize = 'x-large')\n",
    "#plt.ylabel(\"gen_phitt, (rad & shifted)\", fontsize = 'x-large')\n",
    "plt.title(\"Sanity check,\\n gen_phitt against aco angles\", fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.grid()\n",
    "#plt.legend()\n",
    "plt.savefig('Gen_phitt-aco_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN, no custom loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "fig = plt.figure(figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-300-o,\\n Only regress phi Batch: 500')\n",
    "#ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, 0])\n",
    "hist2 = np.array(y[:, 0])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 1000, alpha = 0.5,label = \"True-Regressed nu_phi_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "# hist1 = np.array(model({\"lab_frame\": x})[:, 1])\n",
    "# hist2 = np.array(y[:, 1])\n",
    "# hist_b = hist2-hist1\n",
    "# plt.hist(hist_b, bins = 1000, alpha = 0.5,label = \"True-Regressed nu_phi_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-2,2)\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlabel('Difference in phi')\n",
    "\n",
    "plt.savefig('Regress_only_phi_C4.png')\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find all the tranings with the custom loss function and the full 4 vector of the neutrinos, here for book keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(968192,), dtype=float32, numpy=\n",
       "array([51.463295, 61.751213, 80.776825, ..., 44.725338, 57.42529 ,\n",
       "       80.36569 ], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mom4_to_tf(tau_1_vis.e) - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here start checking the colinearity method\n",
    "\n",
    "#first verify the spread of phi_nu compared to phi_vis\n",
    "hist = np.array(nu_1.phi - tau_1_vis.phi)\n",
    "hist1 = np.array(nu_2.phi - tau_2_vis.phi)\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "plt.hist(hist, bins = 1000, alpha=0.5, label = 'Difefrence between nu_phi_1 and vis_phi_1\\nMean:%.2f, Std:%.2f'%(hist.mean(), hist.std()))\n",
    "plt.hist(hist1, bins = 1000, alpha=0.5, label = 'Difefrence between nu_phi_2 and vis_phi_2\\nMean:%.2f, Std:%.2f'%(hist1.mean(), hist1.std()))\n",
    "plt.legend()\n",
    "plt.xlim(-1,1)\n",
    "plt.grid()\n",
    "plt.ylabel('Occurencies')\n",
    "plt.title('Check for phi')\n",
    "plt.xlabel('nu_phi - vis_phi')\n",
    "plt.savefig('Diff_nu_vis_phi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c5219455b4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtau_mass_dist_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c5219455b4fa>\u001b[0m in \u001b[0;36mtau_mass_dist_1\u001b[0;34m(y, y_pred)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msum_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msum_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m   \"\"\"\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6164\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6166\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6168\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul]"
     ]
    }
   ],
   "source": [
    "#quadratic distance to target - regress both at the same time, will be required for making sense out of the met\n",
    "\n",
    "#try to first work with aplha, only 1 variable - easier\n",
    "# y_1 is the first regressed value, I want them as [[],[]]\n",
    "#D_target = (apha_1 - y_1)**2 + (apha_2 - y_2)**2 \n",
    "\n",
    "def tau_mass_dist_1(y, y_pred):\n",
    "    global tau_1_vis  \n",
    "    \n",
    "    y = tf.transpose(y)\n",
    "    y_pred = tf.transpose(y_pred)\n",
    "    \n",
    "    \n",
    "    sum_energy = tf.transpose(tf.convert_to_tensor(tau_1_vis.e, dtype = 'float32'))+ tf.transpose(y_pred)[0] * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    sum_p = (1+ tf.transpose(y_pred)[0]) * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    \n",
    "    sum_energy_1 = tf.transpose(tf.convert_to_tensor(tau_1_vis.e, dtype = 'float32'))+ y[0] * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    sum_p_1 = (1+ tf.transpose(y[0])) * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    m_tau =  tf.transpose(tf.convert_to_tensor(np.ones(sum_p.shape) * 1.77, dtype = 'float32')) \n",
    "    \n",
    "    return tf.math.abs(sum_energy_1 + sum_energy)#tf.math.abs(sum_energy**2 - sum_p**2 - m_tau**2)  #tf.math.mod(sum_energy**2 - sum_p**2 - m_tau**2)\n",
    "\n",
    "\n",
    "def loss_fn(y, y_pred):\n",
    "#     y = tf.transpose(y)\n",
    "#     y_pred = tf.transpose(y_pred)\n",
    "    return tf.convert_to_tensor((y[0] - y_pred[0])**2 + (y[1] - y_pred[1])**2 + tau_mass_dist_1(y, y_pred))#, dtype = np.float32)\n",
    "\n",
    "\n",
    "#sum_energy**2 - sum_p**2 -\n",
    "\n",
    "# + y[1] * tf.convert_to_tensor(tau_2_vis.p)\n",
    "    \n",
    "    \n",
    "\n",
    "print (tau_mass_dist_1([alpha_1, alpha_2],[alpha_1, alpha_2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
