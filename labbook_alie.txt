LBN works better with a hidden layer afterwards

We will have each layer doing a specific portion of the steps to get to phi_CP

#layer.get_weights() #will give the weight of a layer

Using LBN to regress y_tau:
    Using boost_mode=LBN.PAIRS
    LBN output: E, px, py, pz
    Output is 1 node
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with 1 dense hidden layer with 32 nodes using relu: #Kingsley
        mse = 2.8e-04, val_mae = 0.0124, std(y_T) = 0.32
        Good performance comparatively
    Performance with 1 dense hidden layer with 32 nodes using relu: #Alie
        mse = 3.3895e-04, val_mae = 0.0148
        Good performance comparatively
        
        
        
What we will be looking at: #proposed geometry 1

Layer       |      Output
------------------------------------------------------------
Input       |    4 momenta of 4 pions
------------------------------------------------------------
LBN Layer   |  Boosted 4 momenta of 4 pions
------------------------------------------------------------
Layer 2     | 2lamda_perp, y_1 and y_2, pi_3Mom_star          #here we are doing in two steps but that's maybe not ideal, as we                                              
------------------------------------------------------------  #know y_T is quite well and easily reco from only the LBN layer
Layer 3     |  Phi_CP(unshifted), bigO, y_T
------------------------------------------------------------  
Output      | Phi_CP (shifted)


I will work on the step from layer 3 to output for now

#NOTE: Issue, looks like I am only traning on 3100 data points, trying to fix this now. Number of batches the data is split into


#STD of y

Using NN to regress y_T:
    Inputs phi_CP(unshifted), y_T,bigO
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: (1,a) #Phi_shifted_1Nodes_Layer2_S3-0_a.npy
        mse = 3.4094e-05, val_mae =  3.1544e-04
    Performance with 1 dense hidden layer with 32 nodes using relu: (2,b) #Phi_shifted_1Nodes_Layer2_S3-0_b.npy
        mse = 5.9164e-05, val_mae = 6.9235e-04
    Performance with 2 dense hidden layer with 8 nodes each using relu: 
        mse = , val_mae =  
    Performance with 2 dense hidden layer with 32 nodes each using relu: 
        mse = , val_mae = 

        
Using NN to regress phi_CP: #layer 3 to output
    Inputs phi_CP(unshifted), y_T,bigO
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: (1,a) #Phi_shifted_1Nodes_Layer2_S3-0_d.npy
        mse = 0.9389, val_mae =  0.8157
    Performance with 1 dense hidden layer with 32 nodes using relu: (2,b) #Phi_shifted_1Nodes_Layer2_S3-0_c.npy
        mse = 0.3035, val_mae = 0.3720
    Performance with 2 dense hidden layer with 32 nodes using relu: #much better
        mse = 0.0682, val_mae = 0.0689
        
        
Using NN to regress phi_CP: #layer 2 to output
    Inputs *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, *pi_2_4Mom_star,*pi_1_4Mom_star
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: (1,a) 
        mse = 3.2965, val_mae = 1.5702 
    Performance with 1 dense hidden layer with 32 nodes using relu: (2,b) 
        mse = 3.1388, val_mae = 1.5106
        
        
Using NN to regress phi_CP:  #layer 2 to output
    Inputs *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, y_1, y_2, *pi_2_3Mom_star #much better than previously
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: (1,a) #Phi_shifted_1Nodes_Layer2_S3-0_d.npy
        mse =  3.2497, val_mae = 1.5449 
    Performance with 1 dense hidden layer with 32 nodes using relu: (2,b) #Phi_shifted_1Nodes_Layer2_S3-0_c.npy
        mse = 3.0592, val_mae = 1.4556

Using NN to regress [phi_CP_unshifted, bigO, y_T]:    #layer 2 to 3, 
#std of here (y) 10.27587 #do we want everypoint in the vector
    Inputs *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, y_1, y_2, *pi_2_3Mom_star #much better than previously
    Output is 3 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: 
        mse =  66.9209, val_mae = 3.8623
    Performance with 1 dense hidden layer with 32 nodes using relu: #this looks somewhat good
        mse = 14.1092, val_mae = 1.8836
    Performance with 1 dense hidden layer with 64 nodes using relu: #looks a bit too heavy computing wise ?
        mse = 6.9051, val_mae = 1.1974
    Performance with 2 dense hidden layers with 32 nodes using relu:  
        mse = 2.9666, val_mae = 0.9865
    Performance with 2 dense hidden layers with 8 nodes using relu:  
        mse = 41.2099, val_mae = 2.9022
    Performance with 2 dense hidden layers with 48 nodes using relu:  #8s per epoch
        mse = 1.8864, val_mae = 0.6769
    Performance with 2 dense hidden layers with 64 nodes using relu:  #17s per epoch 
        mse = 1.6415, val_mae = 0.5709   #this might be the best ?
    Performance with 3 dense hidden layers with 32 nodes using relu:  #18s per epoch 
        mse = 2.6365, val_mae = 0.8500
        
#Now try and combine stuff together
we have the best case scenario for both from 2 to 3 and from 3 to output, now merge everything together. First do it badly
i.e. just include a lot of layers


Using NN to regress Phi_CP_shifted:    #layer 2 to output, botched geometry 
    Inputs: *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, df4['y_1_1'], df4['y_1_2'], *pi_2_4Mom_star[1:]
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    
    Layer arrangement 1:
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 3 nodes, (no activation), glue
                - dense, 32 nodes, 'relu'
                - dense, 1 node, (no activation), output
    Performance with above layer 1 arrangement:  #7s/epoch
        mse =  1.0887, val_mae = 0.6736
        
    Layer arrangement 2:
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 3 nodes, (no activation), glue
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 1 node, (no activation), output
    Performance with above layer 2 arrangement:  #7s/epoch
        mse =  1.2184, val_mae = 0.6649
        

        
note: freeze the layer 3 to output weights cause we know they are great in simple traning and then maybe we can also fic the layer 2 to layer 3 weights as well ?

note: could it be useful to have parallel layers ? fonctional API ! reuse cross product layer, will make it easier



Next: look at how to efficiently combine the two models (maybe save up the weights ?)
and also look at functionnal API and how to have parallel layers (ideally have a cross product layer in parrallel, that would be good). Functionnal API seem perfect for waht we want to do

In order to fic the weights of some layers of the model we can just train it and then fix the weights, check https://www.tensorflow.org/guide/keras/sequential_model

Big question, can we train and then add layers ? Will try that today so yeah it works, all good even !!
then freeze the top layers 

Using NN to regress Phi_CP_shifted:    #layer 2 to output, botched geometry 
    Inputs: *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, df4['y_1_1'], df4['y_1_2'], *pi_2_4Mom_star[1:]
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Layer arrangement 3:
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 3 nodes, (no activation), glue
		TRAINING + FREEZING (step 2 to step 3)  #ended at 4 (which is bad !)
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 1 node, (no activation), output
    Performance with above layer 3 arrangement:  #7s/epoch
        mse =  2.4661, val_mae = 1.3601

    Layer arrangement 4:
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 3 nodes, (no activation), glue
		TRAINING + NO FREEZING (step 2 to step 3) #ended at 4 (which is bad !)
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 1 node, (no activation), output
    Performance with above layer 4 arrangement:  #7s/epoch
        mse =  1.2760, val_mae =  0.7398

	Layer arrangement 5:
                - dense, 64 nodes, 'relu'
                - dense, 64 nodes, 'relu'
                - dense, 3 nodes, (no activation), glue
		TRAINING + FREEZING (step 2 to step 3)  #ended at mse= 2.5422
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 1 node, (no activation), output
    Performance with above layer 3 arrangement:  #7s/epoch
        mse = 2.2024, val_mae = 1.2758

    Layer arrangement 4:
                - dense, 64 nodes, 'relu'
                - dense, 64 nodes, 'relu'
                - dense, 3 nodes, (no activation), glue
		TRAINING + NO FREEZING (step 2 to step 3) #ended at mse= 2.1960
                - dense, 32 nodes, 'relu'
                - dense, 32 nodes, 'relu'
                - dense, 1 node, (no activation), output
    Performance with above layer 4 arrangement:  #7s/epoch
        mse =  1.1525, val_mae = 0.6885 #not much better 


#I think issue with first training !

carefull, we also need to include an output layer (carefull about shape (this is essentially the same as gluing stuff together)
#question, would definiing the input size of the tensors help the neural network (by telling it what it should be looking for?)




















Below: some more info but not as relevant

could we matrix product the weigths of glue layers and the next hidden layers ? that sound good ?
rectified linear == relu
maybe linear activation is no activation at all, to do the glue. 
        

        
Using NN to regress y_T:
    Inputs *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, *pi_2_4Mom_star,*pi_1_4Mom_star
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: (1,a)  #Phi_shifted_1Nodes_Layer2_S2-0_a.npy
        mse = 0.0391 , val_mae = 0.1377  
    Performance with 1 dense hidden layer with 32 nodes using relu: (2,b) #Phi_shifted_1Nodes_Layer2_S2-0_b.npy
        mse = 0.022, val_mae =  0.1015
    Performance with 2 dense hidden layer with 8 nodes each using relu: 
        mse = , val_mae =  
    Performance with 2 dense hidden layer with 32 nodes each using relu: 
        mse = , val_mae = 
        
    
Using NN to regress phi_CP(shifted):
    Inputs *pi0_2_4Mom_star_perp, *pi0_1_4Mom_star_perp, y_1, y_2, *pi_2_4Mom_star
    Output is 1 node
    Using 25 epochs on full dataset
    Using MAE for loss
    Performance with 1 dense hidden layer with 8 nodes using relu: (1,a)  #Phi_shifted_1Nodes_Layer2_S2-0_a.npy
        mse = 0.0391 , val_mae = 0.1377  
    Performance with 1 dense hidden layer with 32 nodes using relu: (2,b) #Phi_shifted_1Nodes_Layer2_S2-0_b.npy
        mse = 0.022, val_mae =  0.1015
    Performance with 2 dense hidden layer with 8 nodes each using relu: 
        mse = , val_mae =  
    Performance with 2 dense hidden layer with 32 nodes each using relu: 
        mse = , val_mae = 


#Getting the weights:
weight of the 1 dense hidden layer with 8 nodes using relu: (1)
[array([[ 3.23416293e-01,  1.23141207e-01, -4.17121142e-01,
         2.80202329e-01, -7.80454516e-01,  3.81436870e-02,
        -1.74066532e+00, -6.57379255e-02],
       [-1.01303354e-01,  5.63643873e-01,  4.73040879e-01,
        -3.17757607e-01, -3.14406422e-03, -4.93041181e-04,
         5.45158148e-01, -3.01195383e-01],
       [-2.84094691e-01,  8.24250758e-01, -1.65742946e+00,
         1.11330676e+00,  3.33809406e-02,  2.92117596e-01,
         1.52916265e+00, -4.40373719e-01]], dtype=float32), array([-0.00909845,  0.07273388, -0.00685287,  0.00458871, -0.05324199,
       -0.19093427,  0.04946018, -0.03902742], dtype=float32)]

weight of the 1 output dense layer with 1 nodes using relu: (1)
[array([[-5.8971965e-01],
       [ 2.2262394e-01],
       [-3.9154595e-01],
       [ 5.8291477e-01],
       [ 1.9602631e-01],
       [-1.4363849e-04],
       [ 1.0957868e-01],
       [-4.1662434e-01]], dtype=float32), array([-0.02429211], dtype=float32)]
        

        
then to save and load: 

np.save('Phi_shifted_1Nodes_Layer2.npy', model.get_layer("dense_1").get_weights())
print(np.load('Phi_shifted_1Nodes_Layer2.npy', allow_pickle=True))
        
        

# How we pretrain neural network all individual layers
model.get_layer("dense_14")
model.summary()
layer.set_weights()
model.get_weights() might be better ?

# Input layer: no activation
# linear combinaison - matrix algebra to mix two layers into 1
# will tensor flow optimise it away ?
# might need an inot layer for eveything we have trained one previously
