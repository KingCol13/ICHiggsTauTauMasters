### Results:
Using LBN to regress boosted Pi+/- and Pi0 4-momenta:
    Using 4 4-momenta output features, boost_mode=LBN.PAIRS
    Output is dense(16) reshaped to 4x4
    i.e. 4-momenta of each particle to boost
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with no hidden layers:
        mse = 220, mae=8.2
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 67, mae = 5
        mse = 64.5, mae=4.88 (repeat)
    Performance with 1 dense hidden layer with 32 nodes using sigmoid:
        mse = 116, mae = 6.3
    Performance with 2 dense hidden layers with 32 relu nodes each:
        mse = 41.8, mae=4.00
    Performance with 1 dense hidden layer with 64 relu nodes:
        mse = 45.1, mae=4.11
    Performance with 1 dense hidden layer with 64 nodes and 16 4-momenta from LBN:
        mse = 43.5, mae = 3.9
        mae was 3.5 in previous epoch
    Performance with 1 dense hidden layer with 64 nodes and 8 4-momenta from LBN:
        mse = 39.9, mae = 3.82
    Performance with 2 dense hidden layer with 32 nodes each and 8 4-momenta from LBN:
        mse = 48.2, mae = 4.26
    Performance with 1 dense hidden layer with 32 nodes and 11 4-momenta from LBN:
        mse = 52.0, mae = 4.35
    Performance with 1 dense hidden layer with 64 nodes and 11 4-momenta from LBN:
        mse = 36.3, mae = 3.67
        name = "LBN11-64-16_mae=3.67.hd5"
    Overfitting always negligible

Using regular NN to regress boosted Pi +/- and Pi0 4-momenta:
    25 epochs
    Input 4 lab frame 4-momenta, output 4 boosted 4-momenta
    16-32-32-16 (2 hidden dense layers using relu)
    I.E same as LBN test but replace LBN with 32 dense nodes
    Performance:
        mse = 59, mae = 4.8

Using LBN to regress y_tau:
    Using boost_mode=LBN.PAIRS
    Output is 1 node
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 2.8e-04, val_mae = 0.0124, std(y_T) = 0.32
        Good performance comparatively
   
Using LBN to regress normalised lambdas/IP_trans1,2:
    Using 4 4-momenta output features, boost_mode=LBN.PAIRS
    Output is dense(6) reshaped to 3x2
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 0.33, mae: 0.5, std(y) = 0.577
        guesses 0 vector each time
        
 
Using LBN to regress big_O:
    Using boost_mode=LBN.PAIRS
    Output is 1 node
    Using 25 epochs on full dataset
    Using MSE for loss
    big_O is much more complex than y_T, involving cross and dot products
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 312, mae = 14, std(big_O) = 17.7
        mae only a bit smaller than std, bad performance



Using NN to regress lambda_1,2, y_1, y_2, subleading pi 3 momenta

### Tips:
use layer.get_weights to visually inspect weights