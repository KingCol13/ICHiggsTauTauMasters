### Results:
Using LBN to regress boosted Pi+/- and Pi0 4-momenta:
    Using 4 4-momenta output features, boost_mode=LBN.PAIRS
    Output is dense(16) reshaped to 4x4
    i.e. 4-momenta of each particle to boost
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with no hidden layers:
        mse = 220, mae=8.2
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 67, mae = 5
        mse = 64.5, mae=4.88 (repeat)
    Performance with 1 dense hidden layer with 32 nodes using sigmoid:
        mse = 116, mae = 6.3
    Overfitting always negligible

Using regular NN to regress boosted Pi +/- and Pi0 4-momenta:
    25 epochs
    Input 4 lab frame 4-momenta, output 4 boosted 4-momenta
    16-32-32-16 (2 hidden dense layers using relu)
    I.E same as LBN test but replace LBN with 32 dense nodes
    Performance:
        mse = 59, mae = 4.8

Using LBN to regress y_tau:
    Using boost_mode=LBN.PAIRS
    Output is 1 node
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 2.8e-04, val_mae = 0.0124, std(y_T) = 0.32
        Good performance comparatively
   
Using LBN to regress normalised lambdas/IP_trans1,2:
    Using 4 4-momenta output features, boost_mode=LBN.PAIRS
    Output is dense(6) reshaped to 3x2
    Using 25 epochs on full dataset
    Using MSE for loss
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 0.33, mae: 0.5, std(y) = 0.577
        guesses 0 vector each time
        
 
Using LBN to regress big_O:
    Using boost_mode=LBN.PAIRS
    Output is 1 node
    Using 25 epochs on full dataset
    Using MSE for loss
    big_O is much more complex than y_T, involving cross and dot products
    Performance with 1 dense hidden layer with 32 nodes using relu:
        mse = 312, mae = 14, std(big_O) = 17.7
        mae only a bit smaller than std, bad performance


### Tips:
use layer.get_weights to visually inspect weights