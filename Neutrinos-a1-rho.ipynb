{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tree loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Task 2\n",
    "\n",
    "#This is a script for testing for neutrinos reconstruction\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"/eos/home-a/acraplet/.local/lib/python2.7/site-packages\")\n",
    "sys.path.append(\"/home/acraplet/Alie/Masters/ICHiggsTauTauMasters/\")\n",
    "import uproot \n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from lbn_modified3 import LBN, LBNLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#for some reason pylorentz is installed somewhere differently ?\n",
    "#sys.path.append(\"/eos/home-a/acraplet/.local/lib/python2.7/site-packages\")\n",
    "sys.path.append(\"/home/acraplet/Alie/Masters/ICHiggsTauTauMasters/\")\n",
    "from pylorentz import Momentum4\n",
    "from pylorentz import Vector4\n",
    "from pylorentz import Position4\n",
    "\n",
    "# loading the tree\n",
    "tree = uproot.open(\"/home/acraplet/Alie/Masters/MVAFILE_AllHiggs_tt.root\")[\"ntuple\"]\n",
    "#tree = uproot.open(\"/eos/user/d/dwinterb/SWAN_projects/Masters_CP/MVAFILE_GluGluHToTauTauUncorrelatedDecay_Filtered_tt_2018.root\")[\"ntuple\"]\n",
    "print(\"\\n Tree loaded\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1\n",
      "Check 1\n",
      "253651.99999999997 This is the length\n",
      "panda Data frame created \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_E_1</th>\n",
       "      <th>pi_px_1</th>\n",
       "      <th>pi_py_1</th>\n",
       "      <th>pi_pz_1</th>\n",
       "      <th>pi_E_2</th>\n",
       "      <th>pi_px_2</th>\n",
       "      <th>pi_py_2</th>\n",
       "      <th>pi_pz_2</th>\n",
       "      <th>pi0_E_1</th>\n",
       "      <th>pi0_px_1</th>\n",
       "      <th>...</th>\n",
       "      <th>aco_angle_6</th>\n",
       "      <th>aco_angle_5</th>\n",
       "      <th>aco_angle_7</th>\n",
       "      <th>tau_decay_mode_1</th>\n",
       "      <th>tau_decay_mode_2</th>\n",
       "      <th>mva_dm_1</th>\n",
       "      <th>mva_dm_2</th>\n",
       "      <th>rand</th>\n",
       "      <th>wt_cp_ps</th>\n",
       "      <th>wt_cp_sm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>65.231300</td>\n",
       "      <td>26.494356</td>\n",
       "      <td>-4.689923</td>\n",
       "      <td>59.423537</td>\n",
       "      <td>160.514209</td>\n",
       "      <td>-38.646184</td>\n",
       "      <td>-5.898623</td>\n",
       "      <td>155.680669</td>\n",
       "      <td>51.864747</td>\n",
       "      <td>21.536978</td>\n",
       "      <td>...</td>\n",
       "      <td>3.728793</td>\n",
       "      <td>2.965964</td>\n",
       "      <td>4.684982</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44.725757</td>\n",
       "      <td>-30.429750</td>\n",
       "      <td>23.534550</td>\n",
       "      <td>22.815108</td>\n",
       "      <td>22.741330</td>\n",
       "      <td>18.513075</td>\n",
       "      <td>-13.141459</td>\n",
       "      <td>-1.310242</td>\n",
       "      <td>21.501542</td>\n",
       "      <td>-15.305067</td>\n",
       "      <td>...</td>\n",
       "      <td>6.102568</td>\n",
       "      <td>3.638108</td>\n",
       "      <td>6.155486</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.265905</td>\n",
       "      <td>0.363530</td>\n",
       "      <td>1.118296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.725036</td>\n",
       "      <td>4.523446</td>\n",
       "      <td>-13.026000</td>\n",
       "      <td>-19.305948</td>\n",
       "      <td>6.473571</td>\n",
       "      <td>-0.937901</td>\n",
       "      <td>4.795869</td>\n",
       "      <td>-4.243540</td>\n",
       "      <td>72.251996</td>\n",
       "      <td>13.138452</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561243</td>\n",
       "      <td>2.103064</td>\n",
       "      <td>1.113700</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060319</td>\n",
       "      <td>1.931334</td>\n",
       "      <td>1.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>35.660877</td>\n",
       "      <td>-11.614452</td>\n",
       "      <td>32.884193</td>\n",
       "      <td>-7.443996</td>\n",
       "      <td>8.651832</td>\n",
       "      <td>-0.489045</td>\n",
       "      <td>-7.832403</td>\n",
       "      <td>-3.639921</td>\n",
       "      <td>23.767377</td>\n",
       "      <td>-7.785883</td>\n",
       "      <td>...</td>\n",
       "      <td>1.935885</td>\n",
       "      <td>3.377369</td>\n",
       "      <td>5.796683</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.497772</td>\n",
       "      <td>0.015402</td>\n",
       "      <td>1.961098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>22.177694</td>\n",
       "      <td>5.733469</td>\n",
       "      <td>-2.901378</td>\n",
       "      <td>-21.225927</td>\n",
       "      <td>49.329449</td>\n",
       "      <td>-21.140051</td>\n",
       "      <td>26.349024</td>\n",
       "      <td>-35.947214</td>\n",
       "      <td>128.773945</td>\n",
       "      <td>34.970319</td>\n",
       "      <td>...</td>\n",
       "      <td>4.315230</td>\n",
       "      <td>2.445645</td>\n",
       "      <td>3.899890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.787056</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>1.159358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pi_E_1    pi_px_1    pi_py_1    pi_pz_1      pi_E_2    pi_px_2  \\\n",
       "entry                                                                      \n",
       "14     65.231300  26.494356  -4.689923  59.423537  160.514209 -38.646184   \n",
       "19     44.725757 -30.429750  23.534550  22.815108   22.741330  18.513075   \n",
       "21     23.725036   4.523446 -13.026000 -19.305948    6.473571  -0.937901   \n",
       "24     35.660877 -11.614452  32.884193  -7.443996    8.651832  -0.489045   \n",
       "42     22.177694   5.733469  -2.901378 -21.225927   49.329449 -21.140051   \n",
       "\n",
       "         pi_py_2     pi_pz_2     pi0_E_1   pi0_px_1  ...  aco_angle_6  \\\n",
       "entry                                                ...                \n",
       "14     -5.898623  155.680669   51.864747  21.536978  ...     3.728793   \n",
       "19    -13.141459   -1.310242   21.501542 -15.305067  ...     6.102568   \n",
       "21      4.795869   -4.243540   72.251996  13.138452  ...     1.561243   \n",
       "24     -7.832403   -3.639921   23.767377  -7.785883  ...     1.935885   \n",
       "42     26.349024  -35.947214  128.773945  34.970319  ...     4.315230   \n",
       "\n",
       "       aco_angle_5  aco_angle_7  tau_decay_mode_1  tau_decay_mode_2  mva_dm_1  \\\n",
       "entry                                                                           \n",
       "14        2.965964     4.684982                 1                 1         2   \n",
       "19        3.638108     6.155486                 1                 1         2   \n",
       "21        2.103064     1.113700                 1                 1         2   \n",
       "24        3.377369     5.796683                 1                 1         2   \n",
       "42        2.445645     3.899890                 1                 1         2   \n",
       "\n",
       "       mva_dm_2      rand  wt_cp_ps  wt_cp_sm  \n",
       "entry                                          \n",
       "14            1  0.684114  1.000000  1.000000  \n",
       "19            1  0.265905  0.363530  1.118296  \n",
       "21            1  0.060319  1.931334  1.065400  \n",
       "24            1  0.497772  0.015402  1.961098  \n",
       "42            1  0.787056  0.040564  1.159358  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define what variables are to be read into the dataframe\n",
    "momenta_features = [ \"pi_E_1\", \"pi_px_1\", \"pi_py_1\", \"pi_pz_1\", #leading charged pi 4-momentum\n",
    "              \"pi_E_2\", \"pi_px_2\", \"pi_py_2\", \"pi_pz_2\", #subleading charged pi 4-momentum\n",
    "              \"pi0_E_1\",\"pi0_px_1\",\"pi0_py_1\",\"pi0_pz_1\", #leading neutral pi 4-momentum\n",
    "              \"pi0_E_2\",\"pi0_px_2\",\"pi0_py_2\",\"pi0_pz_2\", #subleading neutral pi 4-momentum\n",
    "              \"gen_nu_p_1\", \"gen_nu_phi_1\", \"gen_nu_eta_1\", #leading neutrino, gen level\n",
    "              \"gen_nu_p_2\", \"gen_nu_phi_2\", \"gen_nu_eta_2\", #subleading neutrino, gen level  \n",
    "              \"\"     \n",
    "                ] \n",
    "\n",
    "other_features = [ \"ip_x_1\", \"ip_y_1\", \"ip_z_1\",        #leading impact parameter\n",
    "                   \"ip_x_2\", \"ip_y_2\", \"ip_z_2\",        #subleading impact parameter\n",
    "                   \"y_1_1\", \"y_1_2\",\n",
    "                   \"gen_phitt\"\n",
    "                 ]    # ratios of energies\n",
    "\n",
    "target = [ \"met\", \"metx\", \"mety\", \"aco_angle_1\", \"aco_angle_6\", \"aco_angle_5\", \"aco_angle_7\"\n",
    "         ]  #acoplanarity angle\n",
    "    \n",
    "selectors = [ \"tau_decay_mode_1\",\"tau_decay_mode_2\",\n",
    "             \"mva_dm_1\",\"mva_dm_2\",\"rand\",\"wt_cp_ps\",\"wt_cp_sm\",\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "variables4=(momenta_features+other_features+target+selectors) #copying Kinglsey's way cause it is very clean\n",
    "print('Check 1')\n",
    "\n",
    "df4 = tree.pandas.df(variables4)\n",
    "\n",
    "print('Check 1')\n",
    "\n",
    "df4 = df4[\n",
    "      (df4[\"tau_decay_mode_1\"] == 1) \n",
    "    & (df4[\"tau_decay_mode_2\"] == 1) \n",
    "    & (df4[\"mva_dm_1\"] == 2) \n",
    "    & (df4[\"mva_dm_2\"] == 1)\n",
    "    & (df4[\"gen_nu_p_1\"] > -4000)\n",
    "    & (df4[\"gen_nu_p_2\"] > -4000)\n",
    "]\n",
    "\n",
    "print(0.7*len(df4),'This is the length') #up to here we are fine\n",
    "\n",
    "df_ps = df4[\n",
    "      (df4[\"rand\"]<df4[\"wt_cp_ps\"]/2)     #a data frame only including the pseudoscalars\n",
    "]\n",
    "\n",
    "df_sm = df4[\n",
    "      (df4[\"rand\"]<df4[\"wt_cp_sm\"]/2)     #data frame only including the scalars\n",
    "]\n",
    "\n",
    "print(\"panda Data frame created \\n\")\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will be mega slow so we do not want to do it each time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_product(vector3_1,vector3_2):\n",
    "    if len(vector3_1)!=3 or len(vector3_1)!=3:\n",
    "        print('These are not 3D arrays !')\n",
    "    x_perp_vector=vector3_1[1]*vector3_2[2]-vector3_1[2]*vector3_2[1]\n",
    "    y_perp_vector=vector3_1[2]*vector3_2[0]-vector3_1[0]*vector3_2[2]\n",
    "    z_perp_vector=vector3_1[0]*vector3_2[1]-vector3_1[1]*vector3_2[0]\n",
    "    return np.array([x_perp_vector,y_perp_vector,z_perp_vector])\n",
    "\n",
    "def dot_product(vector1,vector2):\n",
    "    if len(vector1)!=len(vector2):\n",
    "        raise Arrays_of_different_size\n",
    "    prod=0\n",
    "    for i in range(len(vector1)):\n",
    "        prod=prod+vector1[i]*vector2[i]\n",
    "    return prod\n",
    "\n",
    "\n",
    "def norm(vector):\n",
    "    if len(vector)!=3:\n",
    "        print('This is only for a 3d vector')\n",
    "    return np.sqrt(vector[0]**2+vector[1]**2+vector[2]**2)\n",
    "\n",
    "def remove9999 (Momenta4, leading):\n",
    "    if leading == 1:\n",
    "        nu_ref = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_1\"])), df4[\"gen_nu_eta_1\"], df4[\"gen_nu_phi_1\"], df4[\"gen_nu_p_1\"])\n",
    "    if leading == 2:\n",
    "        nu_ref = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_2\"])), df4[\"gen_nu_eta_2\"], df4[\"gen_nu_phi_2\"], df4[\"gen_nu_p_2\"])\n",
    "    \n",
    "    array = np.array(Momenta4).T\n",
    "    array = array[nu_ref.p_z != 9999]\n",
    "    array = array.T\n",
    "    return Momentum4(array[0], array[1], array[2], array[3])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the usefull 4 momenta\n",
    "\n",
    "#neutrinos refs, in E, px, py, pz form\n",
    "nu_1 = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_1\"])), df4[\"gen_nu_eta_1\"], df4[\"gen_nu_phi_1\"], df4[\"gen_nu_p_1\"])\n",
    "nu_2 = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_2\"])), df4[\"gen_nu_eta_2\"], df4[\"gen_nu_phi_2\"], df4[\"gen_nu_p_2\"])\n",
    "\n",
    "#Charged and neutral pion momenta\n",
    "pi_1_4Mom = Momentum4(df4[\"pi_E_1\"],df4[\"pi_px_1\"],df4[\"pi_py_1\"],df4[\"pi_pz_1\"])\n",
    "pi_2_4Mom = Momentum4(df4[\"pi_E_2\"],df4[\"pi_px_2\"],df4[\"pi_py_2\"],df4[\"pi_pz_2\"]) \n",
    "\n",
    "#Same for the pi0\n",
    "pi0_1_4Mom = Momentum4(df4[\"pi0_E_1\"],df4[\"pi0_px_1\"],df4[\"pi0_py_1\"],df4[\"pi0_pz_1\"])\n",
    "pi0_2_4Mom = Momentum4(df4[\"pi0_E_2\"],df4[\"pi0_px_2\"],df4[\"pi0_py_2\"],df4[\"pi0_pz_2\"])\n",
    "\n",
    "tau_1_vis = pi_1_4Mom + pi0_1_4Mom\n",
    "tau_2_vis = pi_2_4Mom + pi0_2_4Mom\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Here, start straining to regress neutrinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5. 5. 5. ... 5. 5. 5.], shape=(362360,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def one_d(val):\n",
    "    return tf.constant(val, shape = df4[\"pi_px_1\"].shape, dtype = np.float32)\n",
    "\n",
    "def Mom4_to_tf(Mom4_1D):\n",
    "    return tf.convert_to_tensor(Mom4_1D, dtype = 'float32')\n",
    "\n",
    "print (one_d (5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the nans of the ip values:\n",
    "# \"ip_x_1\", \"ip_y_1\", \"ip_z_1\",        #leading impact parameter\n",
    "#                    \"ip_x_2\", \"ip_y_2\", \"ip_z_2\"\n",
    "ip_x_1 = tf.where(tf.math.is_nan(df4[\"ip_x_1\"]), 0, df4[\"ip_x_1\"])\n",
    "ip_y_1 = tf.where(tf.math.is_nan(df4[\"ip_y_1\"]), 0, df4[\"ip_y_1\"])\n",
    "ip_z_1 = tf.where(tf.math.is_nan(df4[\"ip_z_1\"]), 0, df4[\"ip_z_1\"])\n",
    "ip_x_2 = tf.where(tf.math.is_nan(df4[\"ip_x_2\"]), 0, df4[\"ip_x_2\"])\n",
    "ip_y_2 = tf.where(tf.math.is_nan(df4[\"ip_y_2\"]), 0, df4[\"ip_y_2\"])\n",
    "ip_z_2 = tf.where(tf.math.is_nan(df4[\"ip_z_2\"]), 0, df4[\"ip_z_2\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362360,)\n",
      "Model: \"functional_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lab_frame (InputLayer)       [(None, 362360, 17)]      0         \n",
      "_________________________________________________________________\n",
      "learning (Dense)             (None, 362360, 300)       5400      \n",
      "_________________________________________________________________\n",
      "learning2 (Dense)            (None, 362360, 300)       90300     \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 362360, 300)       0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 362360, 14)        4214      \n",
      "=================================================================\n",
      "Total params: 99,914\n",
      "Trainable params: 99,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 362360, 17) for input Tensor(\"lab_frame_21:0\", shape=(None, 362360, 17), dtype=float32), but it was called on an input with incompatible shape (None, 17).\n",
      "WARNING:tensorflow:AutoGraph could not transform <function loss_D_p at 0x7f0c7b664310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpni_vrmi1.py, line 21)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function loss_D_p at 0x7f0c7b664310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpni_vrmi1.py, line 21)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "(None,) this is shape\n",
      "(None,) this is shape\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 362360, 17) for input Tensor(\"lab_frame_21:0\", shape=(None, 362360, 17), dtype=float32), but it was called on an input with incompatible shape (None, 17).\n",
      "(None,) this is shape\n",
      "(None,) this is shape\n",
      "503/508 [============================>.] - ETA: 0s - loss: 23.5999 - mae: 35.9624 - loss_D_p: 0.2144 - loss_phi: 10.4763 - loss_p: 11.1270 - loss_mass_tau: 0.7898 - loss_mass_Higgs: 0.9924WARNING:tensorflow:Model was constructed with shape (None, 362360, 17) for input Tensor(\"lab_frame_21:0\", shape=(None, 362360, 17), dtype=float32), but it was called on an input with incompatible shape (None, 17).\n",
      "(None,) this is shape\n",
      "(None,) this is shape\n",
      "508/508 [==============================] - 4s 7ms/step - loss: 23.5407 - mae: 35.9525 - loss_D_p: 0.2139 - loss_phi: 10.4414 - loss_p: 11.1078 - loss_mass_tau: 0.7866 - loss_mass_Higgs: 0.9910 - val_loss: 23.1596 - val_mae: 39.7068 - val_loss_D_p: 0.2571 - val_loss_phi: 6.9487 - val_loss_p: 14.5108 - val_loss_mass_tau: 0.5178 - val_loss_mass_Higgs: 0.9251\n",
      "Epoch 2/25\n",
      "508/508 [==============================] - 4s 9ms/step - loss: 17.6428 - mae: 35.3032 - loss_D_p: 0.1746 - loss_phi: 6.4036 - loss_p: 9.8363 - loss_mass_tau: 0.4041 - loss_mass_Higgs: 0.8241 - val_loss: 20.0932 - val_mae: 39.8645 - val_loss_D_p: 0.2399 - val_loss_phi: 4.8999 - val_loss_p: 13.6461 - val_loss_mass_tau: 0.4205 - val_loss_mass_Higgs: 0.8869\n",
      "Epoch 3/25\n",
      "508/508 [==============================] - 4s 7ms/step - loss: 16.7234 - mae: 35.3049 - loss_D_p: 0.1662 - loss_phi: 6.0115 - loss_p: 9.3753 - loss_mass_tau: 0.3626 - loss_mass_Higgs: 0.8078 - val_loss: 23.4784 - val_mae: 40.4214 - val_loss_D_p: 0.2720 - val_loss_phi: 7.4207 - val_loss_p: 14.0915 - val_loss_mass_tau: 0.7550 - val_loss_mass_Higgs: 0.9392\n",
      "Epoch 4/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 18.0277 - mae: 35.2053 - loss_D_p: 0.1685 - loss_phi: 7.3790 - loss_p: 9.1981 - loss_mass_tau: 0.4525 - loss_mass_Higgs: 0.8296 - val_loss: 20.2656 - val_mae: 39.6629 - val_loss_D_p: 0.2319 - val_loss_phi: 5.8889 - val_loss_p: 12.7486 - val_loss_mass_tau: 0.4863 - val_loss_mass_Higgs: 0.9099\n",
      "Epoch 5/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 17.5668 - mae: 35.1516 - loss_D_p: 0.1634 - loss_phi: 7.2286 - loss_p: 8.9337 - loss_mass_tau: 0.4207 - loss_mass_Higgs: 0.8205 - val_loss: 21.5374 - val_mae: 39.7357 - val_loss_D_p: 0.2303 - val_loss_phi: 7.3941 - val_loss_p: 12.5349 - val_loss_mass_tau: 0.4953 - val_loss_mass_Higgs: 0.8829\n",
      "Epoch 6/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 17.5925 - mae: 35.2355 - loss_D_p: 0.1636 - loss_phi: 7.3811 - loss_p: 8.7764 - loss_mass_tau: 0.4418 - loss_mass_Higgs: 0.8296 - val_loss: 19.6755 - val_mae: 39.4247 - val_loss_D_p: 0.2332 - val_loss_phi: 5.6209 - val_loss_p: 12.3739 - val_loss_mass_tau: 0.5529 - val_loss_mass_Higgs: 0.8946\n",
      "Epoch 7/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 16.9829 - mae: 35.2834 - loss_D_p: 0.1620 - loss_phi: 6.9062 - loss_p: 8.6566 - loss_mass_tau: 0.4319 - loss_mass_Higgs: 0.8262 - val_loss: 19.2694 - val_mae: 39.4414 - val_loss_D_p: 0.2208 - val_loss_phi: 5.6177 - val_loss_p: 12.1431 - val_loss_mass_tau: 0.4107 - val_loss_mass_Higgs: 0.8771\n",
      "Epoch 8/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 17.1996 - mae: 35.4302 - loss_D_p: 0.1628 - loss_phi: 7.1518 - loss_p: 8.5988 - loss_mass_tau: 0.4518 - loss_mass_Higgs: 0.8343 - val_loss: 19.0722 - val_mae: 39.4845 - val_loss_D_p: 0.2210 - val_loss_phi: 5.5098 - val_loss_p: 12.0310 - val_loss_mass_tau: 0.4365 - val_loss_mass_Higgs: 0.8739\n",
      "Epoch 9/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 16.2205 - mae: 35.2416 - loss_D_p: 0.1577 - loss_phi: 6.3470 - loss_p: 8.4793 - loss_mass_tau: 0.4222 - loss_mass_Higgs: 0.8144 - val_loss: 19.2239 - val_mae: 39.7497 - val_loss_D_p: 0.2220 - val_loss_phi: 5.5660 - val_loss_p: 12.1103 - val_loss_mass_tau: 0.4591 - val_loss_mass_Higgs: 0.8665\n",
      "Epoch 10/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 17.6869 - mae: 35.5436 - loss_D_p: 0.1620 - loss_phi: 7.6955 - loss_p: 8.4883 - loss_mass_tau: 0.4955 - loss_mass_Higgs: 0.8455 - val_loss: 19.7164 - val_mae: 39.7204 - val_loss_D_p: 0.2214 - val_loss_phi: 6.1594 - val_loss_p: 11.9937 - val_loss_mass_tau: 0.4611 - val_loss_mass_Higgs: 0.8808\n",
      "Epoch 11/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 18.0107 - mae: 35.2288 - loss_D_p: 0.1614 - loss_phi: 8.0670 - loss_p: 8.4651 - loss_mass_tau: 0.4852 - loss_mass_Higgs: 0.8320 - val_loss: 22.8769 - val_mae: 39.5931 - val_loss_D_p: 0.2424 - val_loss_phi: 8.7961 - val_loss_p: 12.0699 - val_loss_mass_tau: 0.8425 - val_loss_mass_Higgs: 0.9260\n",
      "Epoch 12/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 17.0059 - mae: 35.0989 - loss_D_p: 0.1615 - loss_phi: 7.1157 - loss_p: 8.4057 - loss_mass_tau: 0.4845 - loss_mass_Higgs: 0.8385 - val_loss: 19.8966 - val_mae: 39.2804 - val_loss_D_p: 0.2254 - val_loss_phi: 6.5236 - val_loss_p: 11.6782 - val_loss_mass_tau: 0.5537 - val_loss_mass_Higgs: 0.9158\n",
      "Epoch 13/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 16.0850 - mae: 34.8599 - loss_D_p: 0.1556 - loss_phi: 6.4839 - loss_p: 8.2261 - loss_mass_tau: 0.4162 - loss_mass_Higgs: 0.8031 - val_loss: 19.8916 - val_mae: 39.2158 - val_loss_D_p: 0.2305 - val_loss_phi: 6.2844 - val_loss_p: 11.7882 - val_loss_mass_tau: 0.6982 - val_loss_mass_Higgs: 0.8902\n",
      "Epoch 14/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 16.4944 - mae: 34.9591 - loss_D_p: 0.1562 - loss_phi: 6.8581 - loss_p: 8.2429 - loss_mass_tau: 0.4284 - loss_mass_Higgs: 0.8087 - val_loss: 18.9589 - val_mae: 39.4540 - val_loss_D_p: 0.2181 - val_loss_phi: 5.6472 - val_loss_p: 11.7431 - val_loss_mass_tau: 0.4926 - val_loss_mass_Higgs: 0.8580\n",
      "Epoch 15/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 16.5452 - mae: 34.8593 - loss_D_p: 0.1550 - loss_phi: 6.9919 - loss_p: 8.1584 - loss_mass_tau: 0.4339 - loss_mass_Higgs: 0.8060 - val_loss: 19.1408 - val_mae: 39.4420 - val_loss_D_p: 0.2172 - val_loss_phi: 5.9258 - val_loss_p: 11.6255 - val_loss_mass_tau: 0.5130 - val_loss_mass_Higgs: 0.8592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 16.0628 - mae: 34.8377 - loss_D_p: 0.1562 - loss_phi: 6.4922 - loss_p: 8.1454 - loss_mass_tau: 0.4500 - loss_mass_Higgs: 0.8190 - val_loss: 21.3673 - val_mae: 39.3553 - val_loss_D_p: 0.2287 - val_loss_phi: 7.9712 - val_loss_p: 11.6525 - val_loss_mass_tau: 0.6425 - val_loss_mass_Higgs: 0.8724\n",
      "Epoch 17/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 16.7956 - mae: 34.9025 - loss_D_p: 0.1593 - loss_phi: 7.1436 - loss_p: 8.1941 - loss_mass_tau: 0.4755 - loss_mass_Higgs: 0.8232 - val_loss: 18.5735 - val_mae: 39.1095 - val_loss_D_p: 0.2119 - val_loss_phi: 5.6030 - val_loss_p: 11.4766 - val_loss_mass_tau: 0.4236 - val_loss_mass_Higgs: 0.8584\n",
      "Epoch 18/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 18.1515 - mae: 34.9997 - loss_D_p: 0.1624 - loss_phi: 8.3967 - loss_p: 8.2128 - loss_mass_tau: 0.5364 - loss_mass_Higgs: 0.8432 - val_loss: 18.8013 - val_mae: 38.9883 - val_loss_D_p: 0.2246 - val_loss_phi: 5.5727 - val_loss_p: 11.5773 - val_loss_mass_tau: 0.5316 - val_loss_mass_Higgs: 0.8951\n",
      "Epoch 19/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 15.8026 - mae: 34.7801 - loss_D_p: 0.1572 - loss_phi: 6.2540 - loss_p: 8.1255 - loss_mass_tau: 0.4525 - loss_mass_Higgs: 0.8135 - val_loss: 17.0696 - val_mae: 38.8530 - val_loss_D_p: 0.2087 - val_loss_phi: 4.2393 - val_loss_p: 11.3509 - val_loss_mass_tau: 0.4132 - val_loss_mass_Higgs: 0.8575\n",
      "Epoch 20/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 15.0109 - mae: 34.7284 - loss_D_p: 0.1502 - loss_phi: 5.7223 - loss_p: 7.9657 - loss_mass_tau: 0.3862 - loss_mass_Higgs: 0.7866 - val_loss: 23.8438 - val_mae: 39.4651 - val_loss_D_p: 0.2441 - val_loss_phi: 9.8396 - val_loss_p: 11.6433 - val_loss_mass_tau: 1.0977 - val_loss_mass_Higgs: 1.0191\n",
      "Epoch 21/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 15.6986 - mae: 34.7713 - loss_D_p: 0.1548 - loss_phi: 6.2551 - loss_p: 8.0112 - loss_mass_tau: 0.4634 - loss_mass_Higgs: 0.8142 - val_loss: 18.2868 - val_mae: 38.7384 - val_loss_D_p: 0.2104 - val_loss_phi: 5.4916 - val_loss_p: 11.2976 - val_loss_mass_tau: 0.4236 - val_loss_mass_Higgs: 0.8636\n",
      "Epoch 22/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 16.1795 - mae: 34.7885 - loss_D_p: 0.1542 - loss_phi: 6.7527 - loss_p: 8.0068 - loss_mass_tau: 0.4524 - loss_mass_Higgs: 0.8134 - val_loss: 18.2943 - val_mae: 39.0652 - val_loss_D_p: 0.2167 - val_loss_phi: 5.3817 - val_loss_p: 11.2581 - val_loss_mass_tau: 0.5779 - val_loss_mass_Higgs: 0.8599\n",
      "Epoch 23/25\n",
      "508/508 [==============================] - 3s 6ms/step - loss: 17.7459 - mae: 34.9568 - loss_D_p: 0.1584 - loss_phi: 8.1879 - loss_p: 8.0522 - loss_mass_tau: 0.5172 - loss_mass_Higgs: 0.8302 - val_loss: 19.1562 - val_mae: 39.4703 - val_loss_D_p: 0.2207 - val_loss_phi: 6.1975 - val_loss_p: 11.2585 - val_loss_mass_tau: 0.5860 - val_loss_mass_Higgs: 0.8935\n",
      "Epoch 24/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 16.0406 - mae: 35.0324 - loss_D_p: 0.1550 - loss_phi: 6.6601 - loss_p: 7.9468 - loss_mass_tau: 0.4689 - loss_mass_Higgs: 0.8098 - val_loss: 22.5628 - val_mae: 39.1256 - val_loss_D_p: 0.2098 - val_loss_phi: 9.8697 - val_loss_p: 11.1377 - val_loss_mass_tau: 0.5033 - val_loss_mass_Higgs: 0.8423\n",
      "Epoch 25/25\n",
      "508/508 [==============================] - 3s 7ms/step - loss: 17.5779 - mae: 35.1063 - loss_D_p: 0.1562 - loss_phi: 8.1059 - loss_p: 7.9804 - loss_mass_tau: 0.5186 - loss_mass_Higgs: 0.8168 - val_loss: 23.6716 - val_mae: 39.2012 - val_loss_D_p: 0.2191 - val_loss_phi: 10.6675 - val_loss_p: 11.3009 - val_loss_mass_tau: 0.5828 - val_loss_mass_Higgs: 0.9013\n"
     ]
    }
   ],
   "source": [
    "# Start training here: would work better with small dataset !\n",
    "# Here copying the result from the KIT paper\n",
    "\n",
    "smear_px, smear_py = one_d(0), one_d(0)   #the smearing of the detector, we don't know yet what it is\n",
    "\n",
    "ref = [#smear_px,py                      #0\n",
    "       #one_d(1.776),                      #1\n",
    "       #df4[\"metx\"],                   #2\n",
    "       #df4[\"mety\"],                   #3\n",
    "       Mom4_to_tf(tau_1_vis.e),       #4\n",
    "       Mom4_to_tf(tau_1_vis.p_x),     #5\n",
    "       Mom4_to_tf(tau_1_vis.p_y),     #6\n",
    "       Mom4_to_tf(tau_1_vis.p_z),     #7\n",
    "       Mom4_to_tf(tau_2_vis.e),       #8\n",
    "       Mom4_to_tf(tau_2_vis.p_x),     #9 \n",
    "       Mom4_to_tf(tau_2_vis.p_y),     #10\n",
    "       Mom4_to_tf(tau_2_vis.p_z),     #11\n",
    "       #one_d(125),                    #12\n",
    "       #Mom4_to_tf(nu_1.e),            #13       corresponding to          #0\n",
    "       Mom4_to_tf(nu_1.p_x),          #14                                 #1\n",
    "       Mom4_to_tf(nu_1.p_y),          #15                                 #2\n",
    "       Mom4_to_tf(nu_1.p_z),          #16                                 #3\n",
    "       #Mom4_to_tf(nu_2.e),            #17                                 #4\n",
    "       Mom4_to_tf(nu_2.p_x),          #18                                 #5\n",
    "       Mom4_to_tf(nu_2.p_y),          #19                                 #6\n",
    "       Mom4_to_tf(nu_2.p_z),          #20                                 #7\n",
    "]\n",
    "\n",
    "\n",
    "#i_smear_px = 0\n",
    "#i_tau_mass = 0\n",
    "#i_smeared_met_px = 1\n",
    "#i_smeared_met_py = 2\n",
    "i_tau1_e = 0\n",
    "i_tau1_px = 1\n",
    "i_tau1_py = 2\n",
    "i_tau1_pz = 3\n",
    "i_tau2_e = 4\n",
    "i_tau2_px = 5\n",
    "i_tau2_py = 6\n",
    "i_tau2_pz = 7\n",
    "#i_gen_mass = 9\n",
    "#i_nu1_e = 12\n",
    "i_nu1_px = 8\n",
    "i_nu1_py = 9\n",
    "i_nu1_pz = 10\n",
    "#i_nu2_e = 16\n",
    "i_nu2_px = 11\n",
    "i_nu2_py = 12\n",
    "i_nu2_pz = 13\n",
    "\n",
    "####Very important, the batch size must be defined beforehand to get rid of two trainable params !\n",
    "B_size = 500#2**10\n",
    "\n",
    "m_Higgs_squared = 125**2\n",
    "\n",
    "\n",
    "m_tau_squared = 1.776**2\n",
    "\n",
    "def one_d_traning(val, shape_array):\n",
    "    return tf.constant(val, shape = shape_array.shape, dtype = np.float32)\n",
    "\n",
    "ref = tf.transpose(ref)\n",
    "\n",
    "def loss_D_p (y_true, y_pred):\n",
    "    global ratio_all\n",
    "    #calculting the difference between the the components, need to add the smearing of detector eventually\n",
    "    target_components = [i_nu1_px, i_nu1_py, i_nu1_pz, i_nu2_px, i_nu2_py, i_nu2_pz]\n",
    "    target_components_diff_list = []\n",
    "    for i in target_components: target_components_diff_list.append((y_true[:,i]-y_pred[:,i])**2)\n",
    "    dxyz = 0\n",
    "    for d in target_components_diff_list: dxyz+=d\n",
    "    return ratio_all * dxyz #tone it down cause it takes too much space\n",
    "\n",
    "def energy_nu (y_true, y_pred, number):\n",
    "    if number == 1:\n",
    "        return tf.sqrt(y_pred[:, i_nu1_px]**2 + y_pred[:, i_nu1_py]**2 + y_pred[:, i_nu1_pz]**2)\n",
    "    if number == 2:\n",
    "        return tf.sqrt(y_pred[:, i_nu2_px]**2 + y_pred[:, i_nu2_py]**2 + y_pred[:, i_nu2_pz]**2)\n",
    "\n",
    "def loss_mass_tau(y_true, y_pred):\n",
    "    #now we try only to use the y_pred for neutrino info, this is I guess their way of \n",
    "    #only training for neutrino info whilst keeping nice structure\n",
    "    #we are always assuming m=0\n",
    "    # note, we are taking y_tau as exact, we could choose not to...\n",
    "    global ratio_tau\n",
    "    E1 = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) \n",
    "    P1_squared = (y_true[:, i_tau1_px] + y_pred[:, i_nu1_px])**2 + (y_true[:, i_tau1_py] + y_pred[:, i_nu1_py])**2 + (y_true[:, i_tau1_pz] + y_pred[:, i_nu1_pz])**2\n",
    "    R1 = (E1**2 - P1_squared -  m_tau_squared)/m_Higgs_squared\n",
    "    \n",
    "    E2 = y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2) \n",
    "    P2_squared = (y_true[:, i_tau2_px] + y_pred[:, i_nu2_px])**2 + (y_true[:, i_tau2_py] + y_pred[:, i_nu2_py])**2 + (y_true[:, i_tau2_pz] + y_pred[:, i_nu2_pz])**2\n",
    "    R2 = (E2**2 - P2_squared - m_tau_squared)/m_Higgs_squared\n",
    "    return ratio_tau * (tf.math.abs(R1) + tf.abs(R2))\n",
    "\n",
    "\n",
    "def loss_mass_Higgs(y_true, y_pred):\n",
    "    global ratio_H\n",
    "    print(tf.convert_to_tensor(y_pred[:, 0]).shape, 'this is shape')\n",
    "    EH = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) + y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2)\n",
    "    px_H = y_true[:, i_tau1_px] + y_true[:, i_tau2_px] + y_pred[:, i_nu1_px] + y_pred[:, i_nu2_px]\n",
    "    py_H = y_true[:, i_tau1_py] + y_true[:, i_tau2_py] + y_pred[:, i_nu1_py] + y_pred[:, i_nu2_py]\n",
    "    pz_H = y_true[:, i_tau1_pz] + y_true[:, i_tau2_pz] + y_pred[:, i_nu1_pz] + y_pred[:, i_nu2_pz]\n",
    "#      y_true[:, i_gen_mass]**2\n",
    "    return ratio_H* tf.abs((EH**2-px_H**2-py_H**2-pz_H**2 -m_Higgs_squared)/m_Higgs_squared)\n",
    "\n",
    "\n",
    "def loss_phi(y_true, y_pred):\n",
    "    global ratio_phi\n",
    "    phi_diff_1 = (tf.math.atan2(y_pred[:, i_nu1_py],y_pred[:, i_nu1_px])-tf.math.atan2(y_true[:, i_nu1_py],y_true[:, i_nu1_px]))**2\n",
    "    phi_diff_2 = (tf.math.atan2(y_pred[:, i_nu2_py],y_pred[:, i_nu2_px])-tf.math.atan2(y_true[:, i_nu2_py],y_true[:, i_nu2_px]))**2\n",
    "\n",
    "    \n",
    "    return ratio_phi*tf.convert_to_tensor(phi_diff_1+phi_diff_2) #tone it up\n",
    "\n",
    "\n",
    "def loss_p(y_true, y_pred):\n",
    "    global ratio_p\n",
    "    delta_p1 = (energy_nu(y_true,y_pred, 1) - tf.sqrt(y_true[:, i_nu1_px]**2 + y_true[:, i_nu1_py]**2 + y_true[:, i_nu1_pz]**2))**2\n",
    "    delta_p2 = (energy_nu(y_true,y_pred, 2) - tf.sqrt(y_true[:, i_nu2_px]**2 + y_true[:, i_nu2_py]**2 + y_true[:, i_nu2_pz]**2))**2\n",
    "    \n",
    "    return ratio_p*tf.convert_to_tensor(delta_p1+delta_p2) #tone it up\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "\n",
    "    dxyz = loss_D_p(y_true, y_pred)\n",
    "    #dmet = loss_dmet(y_true, y_pred)\n",
    "    #dPTtaus = loss_dPTtaus(y_true, y_pred)\n",
    "    dphi = loss_phi(y_true,y_pred)\n",
    "    dtau = loss_mass_tau(y_true, y_pred)\n",
    "    dHiggs = loss_mass_Higgs(y_true, y_pred)\n",
    "    dp = loss_p(y_true, y_pred)\n",
    "    #dM = loss_dM_had(y_true, y_pred)\n",
    "\n",
    "    return dxyz + dtau + dHiggs + dphi + dp\n",
    "\n",
    "\n",
    "nu_1_training = np.array(nu_1)[0]\n",
    "\n",
    "print(nu_1_training.shape)\n",
    "\n",
    "n = -1\n",
    "\n",
    "x = np.array([\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              df4[\"metx\"],\n",
    "              df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              ip_x_1, \n",
    "              ip_y_1, \n",
    "              ip_z_1, \n",
    "              ip_x_2, \n",
    "              ip_y_2, \n",
    "              ip_z_2,\n",
    "              #Mom4_to_tf(nu_1.p_x),\n",
    "              #df4[\"ip_x_1\"], \n",
    "              #df4[\"ip_y_1\"], \n",
    "              #df4[\"ip_z_1\"],        #leading impact parameter\n",
    "              #df4[\"ip_x_2\"], \n",
    "              #df4[\"ip_y_2\"], \n",
    "              #df4[\"ip_z_2\"],\n",
    "              df4[\"met\"],          \n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "ratio_tau = 10\n",
    "ratio_H = 4\n",
    "ratio_all = 1/10000\n",
    "ratio_p = 10/1500\n",
    "ratio_phi = 4\n",
    "\n",
    "\n",
    "trainFrac = 0.7\n",
    "numTrain = int(trainFrac*x.shape[0])\n",
    "x_train = x[:numTrain]\n",
    "y_train = y[:numTrain]\n",
    "\n",
    "x_val = x[numTrain:]\n",
    "y_val = y[numTrain:]\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "x2 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning\")(input_1)\n",
    "#,kernel_regularizer=tf.keras.regularizers.L1L2(2)\n",
    "#kernel_regularizer=tf.keras.regularizers.L2(0.2),\n",
    "#x4 = tf.keras.layers.Dropout(0.5, name=\"dropout\")(x2)\n",
    "# x6 = tf.keras.layers.Dense(11, activation = 'relu', name=\"learning1\")(x2)\n",
    "\n",
    "#x5 = tf.keras.layers.Dropout(0.4, name=\"dropout\")(x201\n",
    "# x3 = tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), name=\"learning2\")(x2)\n",
    "\n",
    "x3 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning2\")(x2)\n",
    "\n",
    "\n",
    "# kernel_regularizer=tf.keras.regularizers.L1L2(2)\n",
    "# kernel_regularizer=tf.keras.regularizers.L2(0.2),\n",
    "x4 = tf.keras.layers.Dropout(0.1, name=\"dropout2\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(300, activation = 'elu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "output = tf.keras.layers.Dense(14, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "y = ref\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae', loss_D_p, loss_phi, loss_p, loss_mass_tau, loss_mass_Higgs])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "    epochs=25,\n",
    "    batch_size = B_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All means\n",
      "means\n",
      "-3.03\n",
      "-1.43\n",
      "0.43\n",
      "0.04\n",
      "-1.43\n",
      "-0.43\n",
      "0.03\n",
      "-0.04\n",
      "0.02\n",
      "-0.01\n",
      "2.94\n",
      "3.76\n",
      "stds\n",
      "15.87\n",
      "13.41\n",
      "12.92\n",
      "20.00\n",
      "13.41\n",
      "20.68\n",
      "0.85\n",
      "1.25\n",
      "0.55\n",
      "0.55\n",
      "25.26\n",
      "25.44\n"
     ]
    }
   ],
   "source": [
    "#Here verify the eta component\n",
    "\n",
    "regressed_array = model({\"lab_frame\": x})\n",
    "\n",
    "length = 0#numTrain\n",
    "\n",
    "E_1 = norm([regressed_array[:, i_nu1_py],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz]])\n",
    "E_2 = norm([regressed_array[:, i_nu2_py],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz]])\n",
    "\n",
    "\n",
    "nu_1_regressed = Momentum4(E_1,regressed_array[:, i_nu1_px],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz])\n",
    "nu_2_regressed = Momentum4(E_2,regressed_array[:, i_nu2_px],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz])\n",
    "\n",
    "\n",
    "print('All means')\n",
    "\n",
    "hist1 = nu_1_regressed.p_x\n",
    "hist2 = Mom4_to_tf(nu_1.p_x)[length:]\n",
    "hist_a = np.array(hist2-hist1) #\n",
    "hist1 = nu_2_regressed.p_x\n",
    "hist2 = Mom4_to_tf(nu_2.p_x)[length:]\n",
    "hist_b = np.array(hist2-hist1) #\n",
    "hist1 = nu_1_regressed.p_y\n",
    "hist2 = Mom4_to_tf(nu_1.p_y)[length:]\n",
    "hist_c = np.array(hist2-hist1) #\n",
    "hist1 = nu_2_regressed.p_y\n",
    "hist2 = Mom4_to_tf(nu_2.p_y)[length:]\n",
    "hist_b = np.array(hist2-hist1) #\n",
    "hist1 = nu_1_regressed.p_z\n",
    "hist2 = Mom4_to_tf(nu_1.p_z)[length:]\n",
    "hist_d = np.array(hist2-hist1) #\n",
    "hist1 = nu_2_regressed.p_z\n",
    "hist2 = Mom4_to_tf(nu_2.p_z)[length:]\n",
    "hist_e = np.array(hist2-hist1) #\n",
    "hist1 = nu_1_regressed.phi\n",
    "hist2 = Mom4_to_tf(nu_1.phi)[length:]\n",
    "hist_f = np.array(hist2-hist1) #\n",
    "hist1 = nu_2_regressed.phi\n",
    "hist2 = Mom4_to_tf(nu_2.phi)[length:]\n",
    "hist_g = np.array(hist2-hist1) #\n",
    "hist1 = nu_1_regressed.eta\n",
    "hist2 = Mom4_to_tf(nu_1.eta)[length:]\n",
    "hist_h = np.array(hist2-hist1) #\n",
    "hist1 = nu_2_regressed.eta\n",
    "hist2 = Mom4_to_tf(nu_2.eta)[length:]\n",
    "hist_i = np.array(hist2-hist1) #\n",
    "hist1 = nu_1_regressed.p\n",
    "hist2 = Mom4_to_tf(nu_1.p)[length:]\n",
    "hist_j = np.array(hist2-hist1) #\n",
    "hist1 = nu_2_regressed.p\n",
    "hist2 = Mom4_to_tf(nu_2.p)[length:]\n",
    "hist_k = np.array(hist2-hist1) #\n",
    "\n",
    "print('means')\n",
    "\n",
    "print('%.2f'%hist_a.mean())\n",
    "print('%.2f'%hist_b.mean())\n",
    "print('%.2f'%hist_c.mean())\n",
    "print('%.2f'%hist_d.mean())\n",
    "print('%.2f'%hist_b.mean())\n",
    "print('%.2f'%hist_e.mean())\n",
    "print('%.2f'%hist_f.mean())\n",
    "print('%.2f'%hist_g.mean())\n",
    "print('%.2f'%hist_h.mean())\n",
    "print('%.2f'%hist_i.mean())\n",
    "print('%.2f'%hist_j.mean())\n",
    "print('%.2f'%hist_k.mean())\n",
    "\n",
    "\n",
    "print ('stds')\n",
    "\n",
    "print('%.2f'%hist_a.std())\n",
    "print('%.2f'%hist_b.std())\n",
    "print('%.2f'%hist_c.std())\n",
    "print('%.2f'%hist_d.std())\n",
    "print('%.2f'%hist_b.std())\n",
    "print('%.2f'%hist_e.std())\n",
    "print('%.2f'%hist_f.std())\n",
    "print('%.2f'%hist_g.std())\n",
    "print('%.2f'%hist_h.std())\n",
    "print('%.2f'%hist_i.std())\n",
    "print('%.2f'%hist_j.std())\n",
    "print('%.2f'%hist_k.std())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then Try simple NN to regress only the phi component of the two neutrinos simultaneously\n",
    "\n",
    "x = [Mom4_to_tf(tau_1_vis.e),\n",
    "     Mom4_to_tf(tau_1_vis.p_x),\n",
    "     Mom4_to_tf(tau_1_vis.p_y),\n",
    "     Mom4_to_tf(tau_1_vis.p_z),\n",
    "     Mom4_to_tf(tau_1_vis.phi),\n",
    "     Mom4_to_tf(tau_2_vis.e),\n",
    "     Mom4_to_tf(tau_2_vis.p_x),\n",
    "     Mom4_to_tf(tau_2_vis.p_y),\n",
    "     Mom4_to_tf(tau_2_vis.p_z),\n",
    "     Mom4_to_tf(tau_2_vis.phi),\n",
    "     df4['met'],\n",
    "     df4['metx'],\n",
    "     df4['mety']\n",
    "    ]\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = [Mom4_to_tf(nu_1.phi), Mom4_to_tf(nu_2.phi)]\n",
    "\n",
    "y = tf.transpose(y)\n",
    "\n",
    "\n",
    "# def loss_fn_phi(y_true, y_pred):\n",
    "#     return (tf.math.cos(y_true)-tf.math.cos(y_pred))**2\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(64, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(64, activation = 'relu', name=\"learning2\")(x2)\n",
    "x4 = tf.keras.layers.Dropout(0.1, name=\"dropout\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "# t = tau_1_vis_loss\n",
    "# y_ = y.T\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss_fn_phi = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(loss = loss_fn_phi, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the 'quality check' section\n",
    "fig = plt.figure(figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-300-o,\\ninputs exclude mets, output exclude met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -6])\n",
    "hist2 = np.array(y_val[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -5])\n",
    "hist2 = np.array(y_val[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -4])\n",
    "hist2 = np.array(y_val[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -3])\n",
    "hist2 = np.array(y_val[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -2])\n",
    "hist2 = np.array(y_val[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x_val})[:, -1])\n",
    "hist2 = np.array(y_val[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('4_vect_qual_500_500_B500_R_val.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here verify the phi component\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "hist1 = tf.math.atan2(model({\"lab_frame\": x_val})[:, i_nu1_py], model({\"lab_frame\": x_val})[:, i_nu1_px])\n",
    "hist2 = Mom4_to_tf(nu_1.phi)[numTrain:]\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_phi_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "hist1 = tf.math.atan2(model({\"lab_frame\": x_val})[:, i_nu2_py], model({\"lab_frame\": x_val})[:, i_nu2_px])\n",
    "hist2 = Mom4_to_tf(nu_2.phi)[numTrain:]\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_phi_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-2,2)\n",
    "plt.xlabel('Difference of phi component')\n",
    "\n",
    "plt.savefig('Phi_qual_500_500_B500_R_val.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here verify the eta component\n",
    "\n",
    "regressed_array = model({\"lab_frame\": x})\n",
    "\n",
    "E_1 = norm([regressed_array[:, i_nu1_py],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz]])\n",
    "E_2 = norm([regressed_array[:, i_nu2_py],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz]])\n",
    "\n",
    "\n",
    "nu_1_regressed = Momentum4(E_1,regressed_array[:, i_nu1_px],regressed_array[:, i_nu1_py], regressed_array[:, i_nu1_pz])\n",
    "nu_2_regressed = Momentum4(E_2,regressed_array[:, i_nu2_px],regressed_array[:, i_nu2_py], regressed_array[:, i_nu2_pz])\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "hist1 = nu_1_regressed.p_x\n",
    "hist2 = Mom4_to_tf(nu_1.p_x)#[numTrain:]\n",
    "hist_b = np.array(hist2-hist1) #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_eta_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "hist1 = nu_2_regressed.p_y\n",
    "hist2 = Mom4_to_tf(nu_2.p_y)#[numTrain:]\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_eta_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xlim(-2,2)\n",
    "plt.xlabel('Difference of phi component')\n",
    "\n",
    "plt.savefig('Phi_qual_500_500_B500_phi_all.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2\n",
    "\n",
    "         \n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "1356/1356 [==============================] - ETA: 0s - loss: 1917.6036 - mae: 39.9942WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "1356/1356 [==============================] - 15s 11ms/step - loss: 1917.6036 - mae: 39.9942 - val_loss: 2610.1282 - val_mae: 43.4733\n",
      "Epoch 2/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1699.7272 - mae: 39.6700 - val_loss: 2597.8945 - val_mae: 43.7552\n",
      "Epoch 3/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1677.6960 - mae: 39.7335 - val_loss: 2549.5977 - val_mae: 43.7872\n",
      "Epoch 4/50\n",
      "1356/1356 [==============================] - 14s 11ms/step - loss: 1663.7881 - mae: 39.7429 - val_loss: 2485.2976 - val_mae: 43.4555\n",
      "Epoch 5/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1651.1306 - mae: 39.8377 - val_loss: 2494.3550 - val_mae: 43.6106\n",
      "Epoch 6/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1643.9099 - mae: 39.8467 - val_loss: 2494.1179 - val_mae: 43.6432\n",
      "Epoch 7/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1634.3356 - mae: 39.8797 - val_loss: 2473.9553 - val_mae: 43.6530\n",
      "Epoch 8/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1630.4716 - mae: 39.8815 - val_loss: 2447.2979 - val_mae: 43.7359\n",
      "Epoch 9/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1622.0109 - mae: 39.8931 - val_loss: 2495.3511 - val_mae: 43.7103\n",
      "Epoch 10/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1621.7166 - mae: 39.9172 - val_loss: 2460.1260 - val_mae: 43.6831\n",
      "Epoch 11/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1612.5052 - mae: 39.9166 - val_loss: 2489.4316 - val_mae: 43.7947\n",
      "Epoch 12/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1608.5955 - mae: 39.9081 - val_loss: 2459.4163 - val_mae: 43.7122\n",
      "Epoch 13/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1608.4413 - mae: 39.9142 - val_loss: 2539.3528 - val_mae: 43.5760\n",
      "Epoch 14/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1605.4712 - mae: 39.9686 - val_loss: 2489.7297 - val_mae: 43.7217\n",
      "Epoch 15/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1597.7333 - mae: 39.9655 - val_loss: 2440.1936 - val_mae: 43.7354\n",
      "Epoch 16/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1597.1824 - mae: 39.9943 - val_loss: 2457.0183 - val_mae: 43.6589\n",
      "Epoch 17/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1594.3080 - mae: 40.0058 - val_loss: 2459.9612 - val_mae: 43.7222\n",
      "Epoch 18/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1594.0132 - mae: 40.0000 - val_loss: 2500.2651 - val_mae: 43.9251\n",
      "Epoch 19/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1583.8054 - mae: 40.0577 - val_loss: 2491.0281 - val_mae: 43.9807\n",
      "Epoch 20/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1583.2651 - mae: 40.0868 - val_loss: 2514.5571 - val_mae: 43.5606\n",
      "Epoch 21/50\n",
      "1356/1356 [==============================] - 17s 12ms/step - loss: 1577.4895 - mae: 40.0679 - val_loss: 2529.6833 - val_mae: 44.0536\n",
      "Epoch 22/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1576.4879 - mae: 40.1033 - val_loss: 2492.8384 - val_mae: 43.8942\n",
      "Epoch 23/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1571.1874 - mae: 40.1227 - val_loss: 2472.2207 - val_mae: 44.0561\n",
      "Epoch 24/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1571.9403 - mae: 40.1778 - val_loss: 2503.5684 - val_mae: 43.8279\n",
      "Epoch 25/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1569.0905 - mae: 40.1779 - val_loss: 2468.4170 - val_mae: 43.9947\n",
      "Epoch 26/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1565.1615 - mae: 40.2022 - val_loss: 2486.9824 - val_mae: 43.9508\n",
      "Epoch 27/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1558.7700 - mae: 40.2188 - val_loss: 2475.9065 - val_mae: 43.9619\n",
      "Epoch 28/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1558.1907 - mae: 40.2382 - val_loss: 2498.5112 - val_mae: 44.0257\n",
      "Epoch 29/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1552.7136 - mae: 40.2349 - val_loss: 2520.7000 - val_mae: 44.1866\n",
      "Epoch 30/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1550.0940 - mae: 40.2576 - val_loss: 2521.6653 - val_mae: 44.0729\n",
      "Epoch 31/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1544.7440 - mae: 40.2847 - val_loss: 2521.4771 - val_mae: 44.0893\n",
      "Epoch 32/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1541.3024 - mae: 40.3077 - val_loss: 2508.6287 - val_mae: 44.3325\n",
      "Epoch 33/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1540.9872 - mae: 40.3195 - val_loss: 2512.3147 - val_mae: 44.1629\n",
      "Epoch 34/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1533.9865 - mae: 40.3493 - val_loss: 2603.9446 - val_mae: 44.5622\n",
      "Epoch 35/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1531.9338 - mae: 40.3968 - val_loss: 2521.9021 - val_mae: 44.1406\n",
      "Epoch 36/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1533.7352 - mae: 40.3765 - val_loss: 2516.8896 - val_mae: 44.1622\n",
      "Epoch 37/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1524.0305 - mae: 40.4048 - val_loss: 2547.2910 - val_mae: 44.3408\n",
      "Epoch 38/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1518.9465 - mae: 40.4270 - val_loss: 2515.4958 - val_mae: 44.2166\n",
      "Epoch 39/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1518.4982 - mae: 40.4616 - val_loss: 2521.7163 - val_mae: 44.2120\n",
      "Epoch 40/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1514.5626 - mae: 40.4721 - val_loss: 2532.1399 - val_mae: 44.0862\n",
      "Epoch 41/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1511.2383 - mae: 40.4589 - val_loss: 2528.5022 - val_mae: 44.4758\n",
      "Epoch 42/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1508.4866 - mae: 40.5074 - val_loss: 2568.1223 - val_mae: 44.6798\n",
      "Epoch 43/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1507.3695 - mae: 40.5364 - val_loss: 2515.4834 - val_mae: 44.3125\n",
      "Epoch 44/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1500.6742 - mae: 40.5202 - val_loss: 2521.2170 - val_mae: 44.2737\n",
      "Epoch 45/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1492.9366 - mae: 40.5404 - val_loss: 2560.5671 - val_mae: 44.5046\n",
      "Epoch 46/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1492.0536 - mae: 40.5596 - val_loss: 2558.2803 - val_mae: 44.4379\n",
      "Epoch 47/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1490.9824 - mae: 40.6010 - val_loss: 2512.8352 - val_mae: 44.4544\n",
      "Epoch 48/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1488.2098 - mae: 40.6221 - val_loss: 2557.1802 - val_mae: 44.5234\n",
      "Epoch 49/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1477.4401 - mae: 40.6374 - val_loss: 2577.4529 - val_mae: 44.5623\n",
      "Epoch 50/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1483.1232 - mae: 40.6474 - val_loss: 2570.5508 - val_mae: 44.5914\n"
     ]
    }
   ],
   "source": [
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              #df4[\"metx\"],\n",
    "              #df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              #df4[\"met\"]\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(300, activation = 'elu', name=\"learning2\")(x2)\n",
    "x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(16, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "#tf.keras.losses.MeanSquaredError() #common to the 4 iterations\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 500-300-500-o,\\ninputs exclude mets, output exclude met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('Quality_500_300_500_1_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Traning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.6240624e-04 8.4828280e-05 5.4249998e-05 ... 2.8547027e-05 9.8437504e-06\n",
      " 1.7312501e-05], shape=(968192,), dtype=float32)\n",
      "tf.Tensor([0.08989737 0.0308425  0.00908413 ... 0.04957712 0.01530869 0.05492   ], shape=(968192,), dtype=float32)\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame_6:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame_6:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1354/1356 [============================>.] - ETA: 0s - loss: 1543.7555 - mae: 38.5546WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame_6:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1543.2177 - mae: 38.5539 - val_loss: 1841.2581 - val_mae: 42.0406\n",
      "Epoch 2/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 1214.5883 - mae: 37.8601 - val_loss: 1733.1243 - val_mae: 41.7674\n",
      "Epoch 3/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1147.7802 - mae: 37.7313 - val_loss: 1692.9059 - val_mae: 41.6186\n",
      "Epoch 4/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 1111.9667 - mae: 37.6784 - val_loss: 1676.6332 - val_mae: 41.6002\n",
      "Epoch 5/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1091.7142 - mae: 37.6597 - val_loss: 1594.8550 - val_mae: 41.5146\n",
      "Epoch 6/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1071.4916 - mae: 37.6533 - val_loss: 1624.4427 - val_mae: 41.6492\n",
      "Epoch 7/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 1060.8175 - mae: 37.6789 - val_loss: 1565.7533 - val_mae: 41.6062\n",
      "Epoch 8/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1049.4861 - mae: 37.6720 - val_loss: 1548.3080 - val_mae: 41.6144\n",
      "Epoch 9/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1035.5745 - mae: 37.6883 - val_loss: 1574.4279 - val_mae: 41.6558\n",
      "Epoch 10/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1025.9240 - mae: 37.7154 - val_loss: 1532.8997 - val_mae: 41.6905\n",
      "Epoch 11/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1017.7857 - mae: 37.7298 - val_loss: 1542.9863 - val_mae: 41.5890\n",
      "Epoch 12/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1013.5701 - mae: 37.7499 - val_loss: 1534.5054 - val_mae: 41.6585\n",
      "Epoch 13/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1004.5600 - mae: 37.7595 - val_loss: 1521.7397 - val_mae: 41.7052\n",
      "Epoch 14/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 996.6945 - mae: 37.7616 - val_loss: 1512.1178 - val_mae: 41.6455\n",
      "Epoch 15/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 991.7304 - mae: 37.7807 - val_loss: 1545.9613 - val_mae: 41.7425\n",
      "Epoch 16/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 986.1164 - mae: 37.7876 - val_loss: 1545.2850 - val_mae: 41.7869\n",
      "Epoch 17/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 980.5477 - mae: 37.8080 - val_loss: 1539.1650 - val_mae: 41.9055\n",
      "Epoch 18/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 974.1382 - mae: 37.7916 - val_loss: 1548.4375 - val_mae: 41.7828\n",
      "Epoch 19/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 969.2938 - mae: 37.8233 - val_loss: 1544.8883 - val_mae: 41.9162\n",
      "Epoch 20/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 963.5911 - mae: 37.8358 - val_loss: 1510.0526 - val_mae: 41.8160\n",
      "Epoch 21/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 959.3618 - mae: 37.8543 - val_loss: 1520.6190 - val_mae: 41.7609\n",
      "Epoch 22/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 957.3499 - mae: 37.8484 - val_loss: 1521.5811 - val_mae: 41.8323\n",
      "Epoch 23/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 947.8010 - mae: 37.8469 - val_loss: 1513.6431 - val_mae: 41.9332\n",
      "Epoch 24/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 948.0798 - mae: 37.8761 - val_loss: 1515.9496 - val_mae: 41.9018\n",
      "Epoch 25/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 938.1719 - mae: 37.8894 - val_loss: 1508.4991 - val_mae: 41.7926\n",
      "Epoch 26/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 937.1107 - mae: 37.9071 - val_loss: 1566.4767 - val_mae: 41.9987\n",
      "Epoch 27/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 939.6241 - mae: 37.9263 - val_loss: 1500.1003 - val_mae: 41.9180\n",
      "Epoch 28/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 925.2584 - mae: 37.9196 - val_loss: 1511.9523 - val_mae: 41.9032\n",
      "Epoch 29/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 927.2836 - mae: 37.9401 - val_loss: 1511.0701 - val_mae: 42.1022\n",
      "Epoch 30/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 920.6776 - mae: 37.9480 - val_loss: 1544.0540 - val_mae: 42.0636\n",
      "Epoch 31/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 916.6577 - mae: 37.9651 - val_loss: 1507.0012 - val_mae: 41.9874\n",
      "Epoch 32/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 911.2335 - mae: 37.9713 - val_loss: 1541.2793 - val_mae: 41.8976\n",
      "Epoch 33/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 910.1839 - mae: 37.9544 - val_loss: 1503.1342 - val_mae: 42.0761\n",
      "Epoch 34/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 904.4823 - mae: 37.9951 - val_loss: 1494.4731 - val_mae: 42.0667\n",
      "Epoch 35/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 902.5694 - mae: 38.0226 - val_loss: 1534.6825 - val_mae: 42.0785\n",
      "Epoch 36/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 900.7153 - mae: 38.0402 - val_loss: 1524.1418 - val_mae: 42.1261\n",
      "Epoch 37/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 896.0626 - mae: 38.0637 - val_loss: 1512.2312 - val_mae: 42.0815\n",
      "Epoch 38/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 890.6423 - mae: 38.0869 - val_loss: 1520.8789 - val_mae: 42.1243\n",
      "Epoch 39/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 887.1625 - mae: 38.0842 - val_loss: 1506.0841 - val_mae: 42.1827\n",
      "Epoch 40/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 888.6796 - mae: 38.0946 - val_loss: 1531.1791 - val_mae: 42.2469\n",
      "Epoch 41/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 884.8829 - mae: 38.0983 - val_loss: 1526.8054 - val_mae: 42.2663\n",
      "Epoch 42/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 879.3757 - mae: 38.0917 - val_loss: 1534.4884 - val_mae: 42.1807\n",
      "Epoch 43/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 878.3714 - mae: 38.1250 - val_loss: 1533.6588 - val_mae: 42.2208\n",
      "Epoch 44/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 874.6194 - mae: 38.1180 - val_loss: 1573.0096 - val_mae: 42.4353\n",
      "Epoch 45/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 875.4766 - mae: 38.1182 - val_loss: 1532.5330 - val_mae: 42.1181\n",
      "Epoch 46/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 868.9577 - mae: 38.1175 - val_loss: 1533.0015 - val_mae: 42.3348\n",
      "Epoch 47/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 865.9069 - mae: 38.1334 - val_loss: 1535.6298 - val_mae: 42.3280\n",
      "Epoch 48/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 862.1082 - mae: 38.1332 - val_loss: 1526.9556 - val_mae: 42.2272\n",
      "Epoch 49/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 860.8137 - mae: 38.1473 - val_loss: 1558.7573 - val_mae: 42.3685\n",
      "Epoch 50/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 861.0936 - mae: 38.1611 - val_loss: 1559.0499 - val_mae: 42.3285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-65dac8553fdb>:163: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(2,1,1)\n",
      "<ipython-input-17-65dac8553fdb>:185: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(2,1,2)\n"
     ]
    }
   ],
   "source": [
    "# Here copying the result from the KIT paper\n",
    "\n",
    "smear_px, smear_py = one_d(0), one_d(0)   #the smearing of the detector, we don't know yet what it is\n",
    "\n",
    "ref = [#smear_px,py                      #0\n",
    "       one_d(1.776),                      #1\n",
    "       df4[\"metx\"],                   #2\n",
    "       df4[\"mety\"],                   #3\n",
    "       Mom4_to_tf(tau_1_vis.e),       #4\n",
    "       Mom4_to_tf(tau_1_vis.p_x),     #5\n",
    "       Mom4_to_tf(tau_1_vis.p_y),     #6\n",
    "       Mom4_to_tf(tau_1_vis.p_z),     #7\n",
    "       Mom4_to_tf(tau_2_vis.e),       #8\n",
    "       Mom4_to_tf(tau_2_vis.p_x),     #9 \n",
    "       Mom4_to_tf(tau_2_vis.p_y),     #10\n",
    "       Mom4_to_tf(tau_2_vis.p_z),     #11\n",
    "       one_d(125),                    #12\n",
    "       #Mom4_to_tf(nu_1.e),            #13       corresponding to          #0\n",
    "       Mom4_to_tf(nu_1.p_x),          #14                                 #1\n",
    "       Mom4_to_tf(nu_1.p_y),          #15                                 #2\n",
    "       Mom4_to_tf(nu_1.p_z),          #16                                 #3\n",
    "       #Mom4_to_tf(nu_2.e),            #17                                 #4\n",
    "       Mom4_to_tf(nu_2.p_x),          #18                                 #5\n",
    "       Mom4_to_tf(nu_2.p_y),          #19                                 #6\n",
    "       Mom4_to_tf(nu_2.p_z),          #20                                 #7\n",
    "]\n",
    "\n",
    "\n",
    "#i_smear_px = 0\n",
    "i_tau_mass = 0\n",
    "i_smeared_met_px = 1\n",
    "i_smeared_met_py = 2\n",
    "i_tau1_e = 3\n",
    "i_tau1_px = 4\n",
    "i_tau1_py = 5\n",
    "i_tau1_pz = 6\n",
    "i_tau2_e = 7\n",
    "i_tau2_px = 8\n",
    "i_tau2_py = 9\n",
    "i_tau2_pz = 10\n",
    "i_gen_mass = 11\n",
    "#i_nu1_e = 12\n",
    "i_nu1_px = 12\n",
    "i_nu1_py = 13\n",
    "i_nu1_pz = 14\n",
    "#i_nu2_e = 16\n",
    "i_nu2_px = 15\n",
    "i_nu2_py = 16\n",
    "i_nu2_pz = 17\n",
    "\n",
    "\n",
    "m_tau_squared = tf.transpose(one_d(1.776)**2)\n",
    "\n",
    "ref = tf.transpose(ref)\n",
    "\n",
    "def loss_D_p (y_true, y_pred):\n",
    "    #calculting the difference between the the components, need to add the smearing of detector eventually\n",
    "    target_components = [i_nu1_px, i_nu1_py, i_nu1_pz, i_nu2_px, i_nu2_py, i_nu2_pz]\n",
    "    target_components_diff_list = []\n",
    "    for i in target_components: target_components_diff_list.append((y_true[:,i]-y_pred[:,i])**2)\n",
    "    dxyz = 0\n",
    "    for d in target_components_diff_list: dxyz+=d\n",
    "    return dxyz\n",
    "\n",
    "def energy_nu (y_true, y_pred, number):\n",
    "    if number == 1:\n",
    "        return tf.sqrt(y_pred[:, i_nu1_px]**2 + y_pred[:, i_nu1_py]**2 + y_pred[:, i_nu1_pz]**2)\n",
    "    if number == 2:\n",
    "        return tf.sqrt(y_pred[:, i_nu2_px]**2 + y_pred[:, i_nu2_py]**2 + y_pred[:, i_nu2_pz]**2)\n",
    "\n",
    "def loss_mass_tau(y_true, y_pred):\n",
    "    #now we try only to use the y_pred for neutrino info, this is I guess their way of \n",
    "    #only training for neutrino info whilst keeping nice structure\n",
    "    #we are always assuming m=0\n",
    "    # note, we are taking y_tau as exact, we could choose not to...\n",
    "    E1 = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) \n",
    "    P1_squared = (y_true[:, i_tau1_px] + y_pred[:, i_nu1_px])**2 + (y_true[:, i_tau1_py] + y_pred[:, i_nu1_py])**2 + (y_true[:, i_tau1_pz] + y_pred[:, i_nu1_pz])**2\n",
    "    R1 = (E1**2 - P1_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    \n",
    "    E2 = y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2) \n",
    "    P2_squared = (y_true[:, i_tau2_px] + y_pred[:, i_nu2_px])**2 + (y_true[:, i_tau2_py] + y_pred[:, i_nu2_py])**2 + (y_true[:, i_tau2_pz] + y_pred[:, i_nu2_pz])**2\n",
    "    R2 = (E2**2 - P2_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    return tf.math.abs(R1) + tf.abs(R2)\n",
    "\n",
    "\n",
    "def loss_mass_Higgs(y_true, y_pred):\n",
    "    EH = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) + y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2)\n",
    "    px_H = y_true[:, i_tau1_px] + y_true[:, i_tau2_px] + y_pred[:, i_nu1_px] + y_pred[:, i_nu2_px]\n",
    "    py_H = y_true[:, i_tau1_py] + y_true[:, i_tau2_py] + y_pred[:, i_nu1_py] + y_pred[:, i_nu2_py]\n",
    "    pz_H = y_true[:, i_tau1_pz] + y_true[:, i_tau2_pz] + y_pred[:, i_nu1_pz] + y_pred[:, i_nu2_pz]\n",
    "    \n",
    "    return tf.abs((EH**2-px_H**2-py_H**2-pz_H**2 - y_true[:, i_gen_mass]**2)/(y_true[:,i_gen_mass])**2)\n",
    "    \n",
    "\n",
    "print(loss_mass_tau(ref,ref))\n",
    "print(loss_mass_Higgs(ref,ref))\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "\n",
    "    dxyz = loss_D_p(y_true, y_pred)\n",
    "    #dmet = loss_dmet(y_true, y_pred)\n",
    "    #dPTtaus = loss_dPTtaus(y_true, y_pred)\n",
    "    dtau = loss_mass_tau(y_true, y_pred)\n",
    "    dHiggs = loss_mass_Higgs(y_true, y_pred)\n",
    "    #dM = loss_dM_had(y_true, y_pred)\n",
    "\n",
    "    return dxyz + dtau + dHiggs\n",
    "\n",
    "\n",
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              df4[\"metx\"],\n",
    "              df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              df4[\"met\"]\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning3\")(x2)\n",
    "#x4 = tf.keras.layers.Dense(100, activation = 'elu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(18, name=\"output\")(x3)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "model.summary\n",
    "\n",
    "#tf.keras.losses.MeanSquaredError() #common to the 4 iterations\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "\n",
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-o,\\ninputs include mets, output include met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('Quality_300_300_include_met_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0.1561270904944474 0\n"
     ]
    }
   ],
   "source": [
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "\n",
    "print(len(model({\"lab_frame\": x})[1]))\n",
    "hist2 = np.array(y[:, -2])\n",
    "need = \"nu_1_E\"\n",
    "dd = 0\n",
    "\n",
    "def frac(d = -2):\n",
    "    difference = y[:, 0]-model({\"lab_frame\": x})[:, 0]\n",
    "    difference = np.reshape(difference, [-1])\n",
    "    l = np.where(abs(difference)<=10**(d),1,0)\n",
    "    print(float(float(np.sum(l))/len(l)), d)\n",
    "    return float(float(np.sum(l))/len(l))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(hist1, bins = 100, alpha = 0.5,label = \"NN %s component : fraction($\\Delta$<$10^{%.1f}$)=%.3f \"%(need, dd, frac(dd)))\n",
    "plt.hist(hist2, bins = 100, alpha = 0.5,label = 'True %s - Features: phi_CP_1 (fixed)'%need)      \n",
    "plt.ylabel(\"Frequency\", fontsize = 'x-large')\n",
    "plt.xlabel(\"%s\"%(need), fontsize = 'x-large')\n",
    "plt.grid()\n",
    "#plt.xlim(0,400)\n",
    "plt.legend()#prop = {'size', 10})\n",
    "plt.savefig('neutrino_next_2_300_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculation:\n",
    "    \"\"\"\n",
    "    Class for calculating the aco_angle variables\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        #this is the function doing all the calculations manually just takes as input the dataframe\n",
    "        #The different *initial* 4 vectors, (E,px,py,pz)\n",
    "        self.pi_1 = np.array([df[\"pi_E_1\"],df[\"pi_px_1\"],df[\"pi_py_1\"],df[\"pi_pz_1\"]])\n",
    "        self.pi_2 = np.array([df[\"pi_E_2\"],df[\"pi_px_2\"],df[\"pi_py_2\"],df[\"pi_pz_2\"]])\n",
    "\n",
    "        self.pi0_1 = np.array([df[\"pi0_E_1\"],df[\"pi0_px_1\"],df[\"pi0_py_1\"],df[\"pi0_pz_1\"]])\n",
    "        self.pi0_2 = np.array([df[\"pi0_E_2\"],df[\"pi0_px_2\"],df[\"pi0_py_2\"],df[\"pi0_pz_2\"]])\n",
    "\n",
    "        #Charged and neutral pion momenta\n",
    "        self.pi_1_4Mom = Momentum4(df[\"pi_E_1\"],df[\"pi_px_1\"],df[\"pi_py_1\"],df[\"pi_pz_1\"])\n",
    "        self.pi_2_4Mom = Momentum4(df[\"pi_E_2\"],df[\"pi_px_2\"],df[\"pi_py_2\"],df[\"pi_pz_2\"])\n",
    "\n",
    "        #Same for the pi0\n",
    "        self.pi0_1_4Mom = Momentum4(df[\"pi0_E_1\"],df[\"pi0_px_1\"],df[\"pi0_py_1\"],df[\"pi0_pz_1\"])\n",
    "        self.pi0_2_4Mom = Momentum4(df[\"pi0_E_2\"],df[\"pi0_px_2\"],df[\"pi0_py_2\"],df[\"pi0_pz_2\"])\n",
    "\n",
    "        self.impact_param_1 = Momentum4(np.zeros(len(df[\"ip_x_1\"])),df[\"ip_x_1\"],df[\"ip_y_1\"],df[\"ip_z_1\"])\n",
    "        self.impact_param_2 = Momentum4(np.zeros(len(df[\"ip_x_2\"])),df[\"ip_x_2\"],df[\"ip_y_2\"],df[\"ip_z_2\"])\n",
    "\n",
    "        #comment or uncomment depending on which aco_angle you want \n",
    "        #self.pi0_1_4Mom = self.impact_param_1\n",
    "        #self.pi0_2_4Mom = self.impact_param_2\n",
    "\n",
    "        #This is the COM frame of the two charged pions w.r.t. which we'll boost\n",
    "        self.ref_COM_4Mom = Momentum4(self.pi_1_4Mom+self.pi_2_4Mom)\n",
    "        boost = Momentum4(self.ref_COM_4Mom[0], -self.ref_COM_4Mom[1], -self.ref_COM_4Mom[2], -self.ref_COM_4Mom[3])\n",
    "        \n",
    "        \n",
    "        boost = -self.ref_COM_4Mom\n",
    "        #energies=[df4[\"pi_E_1\"],df4[\"pi_E_2\"],df4[\"pi0_E_1\"],df4[\"pi0_E_2\"]]\n",
    "\n",
    "        #Lorentz boost everything in the ZMF of the two charged pions\n",
    "        self.pi0_1_4Mom_star = self.pi0_1_4Mom.boost_particle(boost)\n",
    "        self.pi0_2_4Mom_star = self.pi0_2_4Mom.boost_particle(boost)\n",
    "\n",
    "        #Lorentz boost everything in the ZMF of the two neutral pions\n",
    "        self.pi_1_4Mom_star = self.pi_1_4Mom.boost_particle(boost)\n",
    "        self.pi_2_4Mom_star = self.pi_2_4Mom.boost_particle(boost)\n",
    "\n",
    "\n",
    "        #calculating the perpependicular component\n",
    "        pi0_1_3Mom_star_perp=cross_product(self.pi0_1_4Mom_star[1:], self.pi_1_4Mom_star[1:])\n",
    "        pi0_2_3Mom_star_perp=cross_product(self.pi0_2_4Mom_star[1:], self.pi_2_4Mom_star[1:])\n",
    "\n",
    "        #Now normalise:\n",
    "        pi0_1_3Mom_star_perp=pi0_1_3Mom_star_perp/norm(pi0_1_3Mom_star_perp)\n",
    "        pi0_2_3Mom_star_perp=pi0_2_3Mom_star_perp/norm(pi0_2_3Mom_star_perp)\n",
    "\n",
    "        self.pi0_1_4Mom_star_perp = [self.pi0_1_4Mom_star[0], pi0_1_3Mom_star_perp[0], \n",
    "                                     pi0_1_3Mom_star_perp[1], pi0_1_3Mom_star_perp[2]]\n",
    "\n",
    "        self.pi0_2_4Mom_star_perp = [self.pi0_1_4Mom_star[0], pi0_2_3Mom_star_perp[0], \n",
    "                                     pi0_2_3Mom_star_perp[1], pi0_2_3Mom_star_perp[2]]\n",
    "\n",
    "        #Calculating phi_star\n",
    "        self.phi_CP_unshifted = np.arccos(dot_product(pi0_1_3Mom_star_perp,pi0_2_3Mom_star_perp))\n",
    "        \n",
    "        print(self.phi_CP_unshifted[:10],'This is phi_CP')\n",
    "\n",
    "        self.phi_CP = self.phi_CP_unshifted\n",
    "        \n",
    "        print(pi0_1_3Mom_star_perp[:,23], 'this is pi0_1_3mom')\n",
    "\n",
    "        #The energy ratios\n",
    "        self.y_T = np.array(df['y_1_1']*df['y_1_2'])\n",
    "\n",
    "        #The O variable\n",
    "        cross = np.array(np.cross(pi0_1_3Mom_star_perp.transpose(),pi0_2_3Mom_star_perp.transpose()).transpose())\n",
    "        self.bigO = dot_product(self.pi_2_4Mom_star[1:],cross)\n",
    "        \n",
    "        print(self.bigO[:10], '\\n this is big0')\n",
    "\n",
    "        #perform the shift w.r.t. O* sign\n",
    "        \n",
    "        \n",
    "        #phi_CP=np.where(self.bigO>=0, 2*np.pi-self.phi_CP_unshifted, self.phi_CP_unshifted)\n",
    "        self.phi_CP_1 = np.where(self.bigO>=0, 2*np.pi-self.phi_CP_unshifted, self.phi_CP_unshifted)\n",
    "        \n",
    "        print(self.phi_CP_1[:10], '\\n this is after first shft')\n",
    "\n",
    "       # self.phi_CP_2 = np.where(self.y_T<=0, self.phi_CP+np.pi, self.phi_CP-np.pi)\n",
    "\n",
    "        #additionnal shift that needs to be done do see differences between odd and even scenarios, with y=Energy ratios\n",
    "        self.phi_CP = np.where(self.y_T>=0, np.where(self.phi_CP_1<np.pi, self.phi_CP_1+np.pi, self.phi_CP_1-np.pi), self.phi_CP_1)\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        print(self.phi_CP[:10], 'this is full')\n",
    "        \n",
    "        self.y = df[\"aco_angle_1\"]\n",
    "    \n",
    "    def checks(self):\n",
    "\n",
    "        target = [self.df[\"aco_angle_1\"]]#self.df[\"aco_angle_7\"]]\n",
    "        y = tf.transpose(tf.convert_to_tensor(target, dtype=np.float32))\n",
    "\n",
    "        inputs = [self.pi0_1_4Mom, self.pi_1_4Mom, self.pi0_2_4Mom, self.pi_2_4Mom]\n",
    "        x = tf.convert_to_tensor(inputs, dtype=np.float32)\n",
    "        x = tf.transpose(x, [2, 0, 1])\n",
    "        \n",
    "        k = tf.convert_to_tensor([\n",
    "                          self.impact_param_1[0], self.impact_param_1[1], self.impact_param_1[2], self.impact_param_1[3],\n",
    "                          #self.pi0_1_4Mom[0], self.pi0_1_4Mom[1], self.pi0_1_4Mom[2], self.pi0_1_4Mom[3],\n",
    "                          self.pi_1_4Mom[0], self.pi_1_4Mom[1], self.pi_1_4Mom[2], self.pi_1_4Mom[3],\n",
    "                          #self.pi0_2_4Mom[0], self.pi0_2_4Mom[1], self.pi0_2_4Mom[2], self.pi0_2_4Mom[3],\n",
    "                          self.impact_param_2[0], self.impact_param_2[1], self.impact_param_2[2], self.impact_param_2[3],\n",
    "                          self.pi_2_4Mom[0], self.pi_2_4Mom[1], self.pi_2_4Mom[2], self.pi_2_4Mom[3]],\n",
    "                         dtype=np.float32)\n",
    "\n",
    "# the extra info we are giving\n",
    "        l = tf.convert_to_tensor([self.y_T], dtype=np.float32)\n",
    "\n",
    "        return x,y,k,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_1 = nu_1 + tau_1_vis\n",
    "tau_2 = nu_2 + tau_2_vis\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1.m,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (np.array(tau_1.m).mean(), np.array(tau_1.m).std()))\n",
    "plt.hist(np.array(tau_2.m),  bins = 1000, alpha = 0.5, label = 'dot product |tau_2_vis| and |nu_2|\\nmean=%.2f, std=%.2f'% (np.array(tau_2.m).mean(), np.array(tau_2.m).std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0,6)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Mass of sum of rho decay products and tau neutrino', fontsize = 'large')\n",
    "plt.title('Checking that tau neutrino and visible decay products\\nsum up to tau (RM9999)', fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.show()\n",
    "plt.savefig('sum_to_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-71f6fbb4e571>:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "Higgs = tau_1 + tau_2 \n",
    "plt.figure()\n",
    "plt.hist(Higgs.m,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (np.array(Higgs.m).mean(), np.array(Higgs.m).std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xlim(0,6)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Mass of the sum of the two taus', fontsize = 'large')\n",
    "plt.title('Checking that the two taus\\nsum up to Higgs (RM9999)', fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.show()\n",
    "plt.savefig('sum_to_Higgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99919824 0.99922621 0.99976824 ... 0.99905886 0.99952941 0.99982143]\n",
      "[0.99770581 0.99983855 0.99924261 0.98171425 0.9989211  0.99978589\n",
      " 0.99934793 0.99860659 0.99942672 0.99745414]\n"
     ]
    }
   ],
   "source": [
    "#check of the colinearity approximation\n",
    "\n",
    "#Next up: colinarity, tau mass (Kingsley checked) and then Higgs. \n",
    "\n",
    "dot_prod_1 = np.einsum('ia,ia->a',tau_1_vis[1:, ...], nu_1[1:, ...])\n",
    "dot_prod_1 = dot_prod_1/(norm(tau_1_vis[1:, ...])* norm(nu_1[1:, ...]))\n",
    "\n",
    "dot_prod_2 = np.einsum('ia,ia->a',tau_2_vis[1:, ...], nu_2[1:, ...])\n",
    "dot_prod_2 = dot_prod_2/(norm(tau_2_vis[1:, ...])* norm(nu_2[1:, ...]))\n",
    "\n",
    "print(dot_prod_1)\n",
    "print(dot_prod_2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(dot_prod_1,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (dot_prod_1.mean(), dot_prod_1.std()))\n",
    "plt.hist(dot_prod_2,  bins = 1000, alpha = 0.5, label = 'dot product |tau_2_vis| and |nu_2|\\nmean=%.2f, std=%.2f'% (dot_prod_2.mean(), dot_prod_2.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0.95,1.05)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Dot product between rho decay products and tau neutrino', fontsize = 'large')\n",
    "plt.title('Checking the colinarity approximation between\\ntau neutrino and visible decay products', fontsize = 'xx-large', weight = 'bold')\n",
    "\n",
    "plt.savefig('Colinearity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the colinear approximation ? can calculate the dot product between the visible decay products and the nus\n",
    "check_x_1 = np.array(nu_1.p_x-tau_1_vis.p_x)#dot_product(tau_1_vis[1:]/norm(tau_1_vis[1:]), nu_1[1:]/norm(nu_1[1:])))\n",
    "check_x_2 = np.array(nu_2.p_x-tau_2_vis.p_x)#dot_product(tau_2_vis[1:]/norm(tau_2_vis[1:]), nu_2[1:]/norm(nu_2[1:])))\n",
    "check_y_1 = np.array(nu_1.p_y-tau_1_vis.p_y)\n",
    "check_y_2 = np.array(nu_2.p_y-tau_2_vis.p_y)\n",
    "\n",
    "\n",
    "check_z_1 = []\n",
    "check_z_2 = []\n",
    "\n",
    "for i in range (len(nu_1.p_z)):\n",
    "    if nu_1.p_z[i] != 9999:\n",
    "        check_z_1.append(nu_1.p_z[i]-tau_1_vis.p_z[i])\n",
    "    if nu_2.p_z[i] != 9999:\n",
    "        check_z_2.append(nu_2.p_z[i]-tau_2_vis.p_z[i])\n",
    "\n",
    "check_z_1 = np.array(check_z_1)\n",
    "check_z_2 = np.array(check_z_2)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1_vis.p_x - nu_1.p_x, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_px and nu_1_px\\nmean=%.2f, std=%.2f'% (check_x_1.mean(),check_x_1.std()))\n",
    "plt.hist(tau_2_vis.p_x - nu_2.p_x, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_px and nu_2_px\\nmean=%.2f, std=%.2f'% (check_x_2.mean(),check_x_2.std()))\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_x')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(tau_1_vis.p_y - nu_1.p_y, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_py and nu_1_py\\nmean=%.2f, std=%.2f'% (check_y_1.mean(),check_y_1.std()))\n",
    "plt.hist(tau_2_vis.p_y - nu_2.p_y, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_py and nu_2_py\\nmean=%.2f, std=%.2f'% (check_y_2.mean(),check_y_2.std()))\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_y')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(check_z_1, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_pz and nu_1_pz\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_z_1.mean(),check_z_1.std()))\n",
    "plt.hist(check_z_2, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_pz and nu_2_pz\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_z_2.mean(),check_z_2.std()))\n",
    "\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_z')\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_E_1 = []#dot_product(tau_1_vis[1:]/norm(tau_1_vis[1:]), nu_1[1:]/norm(nu_1[1:])))\n",
    "check_E_2 = []#dot_product(tau_2_vis[1:]/norm(tau_2_vis[1:]), nu_2[1:]/norm(nu_2[1:])))\n",
    "\n",
    "for i in range (len(nu_1.e)):\n",
    "    if nu_1.e[i]!=9999:\n",
    "        check_E_1.append(nu_1.e[i]-tau_1_vis.e[i])\n",
    "    if nu_2.e[i]!=9999:\n",
    "        check_E_2.append(nu_2.e[i]-tau_2_vis.e[i])\n",
    "\n",
    "check_E_1 = np.array(check_E_1)\n",
    "check_E_2 = np.array(check_E_2)\n",
    "plt.figure()\n",
    "plt.hist(check_E_1, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_E and nu_1_E\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_E_1.mean(),check_E_1.std()))\n",
    "plt.hist(check_E_2, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_E and nu_2_E\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_E_2.mean(),check_E_2.std()))\n",
    "\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino energy components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between energies', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-500,500)\n",
    "plt.legend()\n",
    "plt.savefig('Check_energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-75e86c0dc241>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig('Compare_nu_E_to_met')\n"
     ]
    }
   ],
   "source": [
    "sum_nu = nu_1 + nu_2\n",
    "\n",
    "met = np.array(df4[\"met\"])\n",
    "sum_energies=[]\n",
    "met_ref = []\n",
    "x = [0,500]\n",
    "\n",
    "for i in range (len(nu_1.e)):\n",
    "    if nu_1.e[i]!=9999 and nu_2.e[i]!=9999:\n",
    "        sum_energies.append(sum_nu.e[i])\n",
    "        met_ref.append(met[i])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(met_ref, sum_energies, 'gx')\n",
    "plt.plot(x,x, 'k--', label = 'y=x')\n",
    "plt.xlabel(\"Missing transverse energy\", fontsize = 'x-large')\n",
    "plt.ylabel(\"Energy of the sum of the neutrinos\", fontsize = 'x-large')\n",
    "plt.title(\"Sanity check, removing 9999 \\n gen level nu energies and met\", fontsize = 'xx-large', weight = 'bold')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0,500)\n",
    "plt.savefig('Compare_nu_E_to_met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-d78cf37700c1>:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "x_ref = np.array([0, 6])\n",
    "x_ref2 = [6,0]\n",
    "\n",
    "gen_phitt = np.array(df4[\"gen_phitt\"][:2000])*2*np.pi/180+np.pi\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_ref, x_ref, '--', label = 'x=y')\n",
    "plt.plot(x_ref, x_ref2, '--', label = 'y=-x')\n",
    "\n",
    "plt.hist2d(df4[\"aco_angle_1\"], df4[\"gen_phitt\"], bins=50)\n",
    "#plt.plot(df4[\"aco_angle_1\"][:2000], gen_phitt, 'bx', label='aco_angle_1')\n",
    "#plt.plot(df4[\"aco_angle_5\"][:100], gen_phitt, 'gx', label='aco_angle_5')\n",
    "#plt.plot(df4[\"aco_angle_6\"][:100], gen_phitt, 'rx', label='aco_angle_6')\n",
    "#plt.plot(df4[\"aco_angle_7\"][:100], gen_phitt, 'kx', label='aco_angle_7')\n",
    "#plt.xlabel(\"aco_angles\", fontsize = 'x-large')\n",
    "#plt.ylabel(\"gen_phitt, (rad & shifted)\", fontsize = 'x-large')\n",
    "plt.title(\"Sanity check,\\n gen_phitt against aco angles\", fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.grid()\n",
    "#plt.legend()\n",
    "plt.savefig('Gen_phitt-aco_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN, no custom loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "fig = plt.figure(figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-300-o,\\n Only regress phi Batch: 500')\n",
    "#ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, 0])\n",
    "hist2 = np.array(y[:, 0])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 1000, alpha = 0.5,label = \"True-Regressed nu_phi_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "# hist1 = np.array(model({\"lab_frame\": x})[:, 1])\n",
    "# hist2 = np.array(y[:, 1])\n",
    "# hist_b = hist2-hist1\n",
    "# plt.hist(hist_b, bins = 1000, alpha = 0.5,label = \"True-Regressed nu_phi_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-2,2)\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlabel('Difference in phi')\n",
    "\n",
    "plt.savefig('Regress_only_phi_C4.png')\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find all the tranings with the custom loss function and the full 4 vector of the neutrinos, here for book keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(968192,), dtype=float32, numpy=\n",
       "array([51.463295, 61.751213, 80.776825, ..., 44.725338, 57.42529 ,\n",
       "       80.36569 ], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mom4_to_tf(tau_1_vis.e) - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here start checking the colinearity method\n",
    "\n",
    "#first verify the spread of phi_nu compared to phi_vis\n",
    "hist = np.array(nu_1.phi - tau_1_vis.phi)\n",
    "hist1 = np.array(nu_2.phi - tau_2_vis.phi)\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "plt.hist(hist, bins = 1000, alpha=0.5, label = 'Difefrence between nu_phi_1 and vis_phi_1\\nMean:%.2f, Std:%.2f'%(hist.mean(), hist.std()))\n",
    "plt.hist(hist1, bins = 1000, alpha=0.5, label = 'Difefrence between nu_phi_2 and vis_phi_2\\nMean:%.2f, Std:%.2f'%(hist1.mean(), hist1.std()))\n",
    "plt.legend()\n",
    "plt.xlim(-1,1)\n",
    "plt.grid()\n",
    "plt.ylabel('Occurencies')\n",
    "plt.title('Check for phi')\n",
    "plt.xlabel('nu_phi - vis_phi')\n",
    "plt.savefig('Diff_nu_vis_phi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c5219455b4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtau_mass_dist_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c5219455b4fa>\u001b[0m in \u001b[0;36mtau_mass_dist_1\u001b[0;34m(y, y_pred)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msum_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msum_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m   \"\"\"\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6164\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6166\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6168\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul]"
     ]
    }
   ],
   "source": [
    "#quadratic distance to target - regress both at the same time, will be required for making sense out of the met\n",
    "\n",
    "#try to first work with aplha, only 1 variable - easier\n",
    "# y_1 is the first regressed value, I want them as [[],[]]\n",
    "#D_target = (apha_1 - y_1)**2 + (apha_2 - y_2)**2 \n",
    "\n",
    "def tau_mass_dist_1(y, y_pred):\n",
    "    global tau_1_vis  \n",
    "    \n",
    "    y = tf.transpose(y)\n",
    "    y_pred = tf.transpose(y_pred)\n",
    "    \n",
    "    \n",
    "    sum_energy = tf.transpose(tf.convert_to_tensor(tau_1_vis.e, dtype = 'float32'))+ tf.transpose(y_pred)[0] * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    sum_p = (1+ tf.transpose(y_pred)[0]) * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    \n",
    "    sum_energy_1 = tf.transpose(tf.convert_to_tensor(tau_1_vis.e, dtype = 'float32'))+ y[0] * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    sum_p_1 = (1+ tf.transpose(y[0])) * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    m_tau =  tf.transpose(tf.convert_to_tensor(np.ones(sum_p.shape) * 1.77, dtype = 'float32')) \n",
    "    \n",
    "    return tf.math.abs(sum_energy_1 + sum_energy)#tf.math.abs(sum_energy**2 - sum_p**2 - m_tau**2)  #tf.math.mod(sum_energy**2 - sum_p**2 - m_tau**2)\n",
    "\n",
    "\n",
    "def loss_fn(y, y_pred):\n",
    "#     y = tf.transpose(y)\n",
    "#     y_pred = tf.transpose(y_pred)\n",
    "    return tf.convert_to_tensor((y[0] - y_pred[0])**2 + (y[1] - y_pred[1])**2 + tau_mass_dist_1(y, y_pred))#, dtype = np.float32)\n",
    "\n",
    "\n",
    "#sum_energy**2 - sum_p**2 -\n",
    "\n",
    "# + y[1] * tf.convert_to_tensor(tau_2_vis.p)\n",
    "    \n",
    "    \n",
    "\n",
    "print (tau_mass_dist_1([alpha_1, alpha_2],[alpha_1, alpha_2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
