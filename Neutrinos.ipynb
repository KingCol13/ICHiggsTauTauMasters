{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tree loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Task 2\n",
    "\n",
    "#This is a script for testing for neutrinos reconstruction\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"/eos/home-a/acraplet/.local/lib/python2.7/site-packages\")\n",
    "sys.path.append(\"/home/acraplet/Alie/Masters/ICHiggsTauTauMasters/\")\n",
    "import uproot \n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from lbn_modified3 import LBN, LBNLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#for some reason pylorentz is installed somewhere differently ?\n",
    "#sys.path.append(\"/eos/home-a/acraplet/.local/lib/python2.7/site-packages\")\n",
    "sys.path.append(\"/home/acraplet/Alie/Masters/ICHiggsTauTauMasters/\")\n",
    "from pylorentz import Momentum4\n",
    "from pylorentz import Vector4\n",
    "from pylorentz import Position4\n",
    "\n",
    "# loading the tree\n",
    "tree = uproot.open(\"/home/acraplet/Alie/Masters/MVAFILE_AllHiggs_tt.root\")[\"ntuple\"]\n",
    "#tree = uproot.open(\"/eos/user/d/dwinterb/SWAN_projects/Masters_CP/MVAFILE_GluGluHToTauTauUncorrelatedDecay_Filtered_tt_2018.root\")[\"ntuple\"]\n",
    "print(\"\\n Tree loaded\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1\n",
      "Check 1\n",
      "677734.3999999999 This is the length\n",
      "panda Data frame created \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_E_1</th>\n",
       "      <th>pi_px_1</th>\n",
       "      <th>pi_py_1</th>\n",
       "      <th>pi_pz_1</th>\n",
       "      <th>pi_E_2</th>\n",
       "      <th>pi_px_2</th>\n",
       "      <th>pi_py_2</th>\n",
       "      <th>pi_pz_2</th>\n",
       "      <th>pi0_E_1</th>\n",
       "      <th>pi0_px_1</th>\n",
       "      <th>...</th>\n",
       "      <th>aco_angle_6</th>\n",
       "      <th>aco_angle_5</th>\n",
       "      <th>aco_angle_7</th>\n",
       "      <th>tau_decay_mode_1</th>\n",
       "      <th>tau_decay_mode_2</th>\n",
       "      <th>mva_dm_1</th>\n",
       "      <th>mva_dm_2</th>\n",
       "      <th>rand</th>\n",
       "      <th>wt_cp_ps</th>\n",
       "      <th>wt_cp_sm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45.423448</td>\n",
       "      <td>-13.747046</td>\n",
       "      <td>-38.825621</td>\n",
       "      <td>-19.153590</td>\n",
       "      <td>35.805782</td>\n",
       "      <td>8.526798</td>\n",
       "      <td>34.653880</td>\n",
       "      <td>2.904638</td>\n",
       "      <td>10.039846</td>\n",
       "      <td>-3.551914</td>\n",
       "      <td>...</td>\n",
       "      <td>3.327978</td>\n",
       "      <td>1.509938</td>\n",
       "      <td>4.475070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861532</td>\n",
       "      <td>0.950417</td>\n",
       "      <td>1.228852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24.506373</td>\n",
       "      <td>-14.056253</td>\n",
       "      <td>9.809321</td>\n",
       "      <td>-17.514046</td>\n",
       "      <td>19.587723</td>\n",
       "      <td>8.539313</td>\n",
       "      <td>-3.149378</td>\n",
       "      <td>-17.344192</td>\n",
       "      <td>41.244844</td>\n",
       "      <td>-24.443820</td>\n",
       "      <td>...</td>\n",
       "      <td>4.023599</td>\n",
       "      <td>3.710838</td>\n",
       "      <td>1.860006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932849</td>\n",
       "      <td>1.936855</td>\n",
       "      <td>0.124674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.319610</td>\n",
       "      <td>8.794122</td>\n",
       "      <td>0.774085</td>\n",
       "      <td>12.519393</td>\n",
       "      <td>17.469988</td>\n",
       "      <td>-9.876231</td>\n",
       "      <td>-3.253855</td>\n",
       "      <td>14.037575</td>\n",
       "      <td>69.457213</td>\n",
       "      <td>38.899682</td>\n",
       "      <td>...</td>\n",
       "      <td>2.489589</td>\n",
       "      <td>3.446808</td>\n",
       "      <td>2.793055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.132842</td>\n",
       "      <td>0.400455</td>\n",
       "      <td>1.461517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>94.211361</td>\n",
       "      <td>42.942276</td>\n",
       "      <td>-33.711103</td>\n",
       "      <td>-76.780750</td>\n",
       "      <td>3.656937</td>\n",
       "      <td>-2.129749</td>\n",
       "      <td>0.251363</td>\n",
       "      <td>-2.958835</td>\n",
       "      <td>25.585962</td>\n",
       "      <td>11.770386</td>\n",
       "      <td>...</td>\n",
       "      <td>2.071870</td>\n",
       "      <td>2.157516</td>\n",
       "      <td>1.106966</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514073</td>\n",
       "      <td>0.061072</td>\n",
       "      <td>0.059870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>25.899289</td>\n",
       "      <td>-22.245884</td>\n",
       "      <td>-12.141924</td>\n",
       "      <td>-5.333670</td>\n",
       "      <td>23.795390</td>\n",
       "      <td>21.402816</td>\n",
       "      <td>9.436143</td>\n",
       "      <td>-4.368043</td>\n",
       "      <td>23.216735</td>\n",
       "      <td>-20.268636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394414</td>\n",
       "      <td>2.106487</td>\n",
       "      <td>1.552719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504356</td>\n",
       "      <td>0.931771</td>\n",
       "      <td>0.654131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pi_E_1    pi_px_1    pi_py_1    pi_pz_1     pi_E_2    pi_px_2  \\\n",
       "entry                                                                     \n",
       "8      45.423448 -13.747046 -38.825621 -19.153590  35.805782   8.526798   \n",
       "25     24.506373 -14.056253   9.809321 -17.514046  19.587723   8.539313   \n",
       "27     15.319610   8.794122   0.774085  12.519393  17.469988  -9.876231   \n",
       "45     94.211361  42.942276 -33.711103 -76.780750   3.656937  -2.129749   \n",
       "50     25.899289 -22.245884 -12.141924  -5.333670  23.795390  21.402816   \n",
       "\n",
       "         pi_py_2    pi_pz_2    pi0_E_1   pi0_px_1  ...  aco_angle_6  \\\n",
       "entry                                              ...                \n",
       "8      34.653880   2.904638  10.039846  -3.551914  ...     3.327978   \n",
       "25     -3.149378 -17.344192  41.244844 -24.443820  ...     4.023599   \n",
       "27     -3.253855  14.037575  69.457213  38.899682  ...     2.489589   \n",
       "45      0.251363  -2.958835  25.585962  11.770386  ...     2.071870   \n",
       "50      9.436143  -4.368043  23.216735 -20.268636  ...     0.394414   \n",
       "\n",
       "       aco_angle_5  aco_angle_7  tau_decay_mode_1  tau_decay_mode_2  mva_dm_1  \\\n",
       "entry                                                                           \n",
       "8         1.509938     4.475070                 1                 1         1   \n",
       "25        3.710838     1.860006                 1                 1         1   \n",
       "27        3.446808     2.793055                 1                 1         1   \n",
       "45        2.157516     1.106966                 1                 1         1   \n",
       "50        2.106487     1.552719                 1                 1         1   \n",
       "\n",
       "       mva_dm_2      rand  wt_cp_ps  wt_cp_sm  \n",
       "entry                                          \n",
       "8             1  0.861532  0.950417  1.228852  \n",
       "25            1  0.932849  1.936855  0.124674  \n",
       "27            1  0.132842  0.400455  1.461517  \n",
       "45            1  0.514073  0.061072  0.059870  \n",
       "50            1  0.504356  0.931771  0.654131  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define what variables are to be read into the dataframe\n",
    "momenta_features = [ \"pi_E_1\", \"pi_px_1\", \"pi_py_1\", \"pi_pz_1\", #leading charged pi 4-momentum\n",
    "              \"pi_E_2\", \"pi_px_2\", \"pi_py_2\", \"pi_pz_2\", #subleading charged pi 4-momentum\n",
    "              \"pi0_E_1\",\"pi0_px_1\",\"pi0_py_1\",\"pi0_pz_1\", #leading neutral pi 4-momentum\n",
    "              \"pi0_E_2\",\"pi0_px_2\",\"pi0_py_2\",\"pi0_pz_2\", #subleading neutral pi 4-momentum\n",
    "              \"gen_nu_p_1\", \"gen_nu_phi_1\", \"gen_nu_eta_1\", #leading neutrino, gen level\n",
    "              \"gen_nu_p_2\", \"gen_nu_phi_2\", \"gen_nu_eta_2\" #subleading neutrino, gen level  \n",
    "                ] \n",
    "\n",
    "other_features = [ \"ip_x_1\", \"ip_y_1\", \"ip_z_1\",        #leading impact parameter\n",
    "                   \"ip_x_2\", \"ip_y_2\", \"ip_z_2\",        #subleading impact parameter\n",
    "                   \"y_1_1\", \"y_1_2\",\n",
    "                   \"gen_phitt\"\n",
    "                 ]    # ratios of energies\n",
    "\n",
    "target = [ \"met\", \"metx\", \"mety\", \"aco_angle_1\", \"aco_angle_6\", \"aco_angle_5\", \"aco_angle_7\"\n",
    "         ]  #acoplanarity angle\n",
    "    \n",
    "selectors = [ \"tau_decay_mode_1\",\"tau_decay_mode_2\",\n",
    "             \"mva_dm_1\",\"mva_dm_2\",\"rand\",\"wt_cp_ps\",\"wt_cp_sm\",\n",
    "            ]\n",
    "\n",
    "variables4=(momenta_features+other_features+target+selectors) #copying Kinglsey's way cause it is very clean\n",
    "print('Check 1')\n",
    "\n",
    "df4 = tree.pandas.df(variables4)\n",
    "\n",
    "print('Check 1')\n",
    "\n",
    "df4 = df4[\n",
    "      (df4[\"tau_decay_mode_1\"] == 1) \n",
    "    & (df4[\"tau_decay_mode_2\"] == 1) \n",
    "    & (df4[\"mva_dm_1\"] == 1) \n",
    "    & (df4[\"mva_dm_2\"] == 1)\n",
    "    & (df4[\"gen_nu_p_1\"] > -4000)\n",
    "    & (df4[\"gen_nu_p_2\"] > -4000)\n",
    "]\n",
    "\n",
    "print(0.7*len(df4),'This is the length') #up to here we are fine\n",
    "\n",
    "df_ps = df4[\n",
    "      (df4[\"rand\"]<df4[\"wt_cp_ps\"]/2)     #a data frame only including the pseudoscalars\n",
    "]\n",
    "\n",
    "df_sm = df4[\n",
    "      (df4[\"rand\"]<df4[\"wt_cp_sm\"]/2)     #data frame only including the scalars\n",
    "]\n",
    "\n",
    "print(\"panda Data frame created \\n\")\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will be mega slow so we do not want to do it each time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_product(vector3_1,vector3_2):\n",
    "    if len(vector3_1)!=3 or len(vector3_1)!=3:\n",
    "        print('These are not 3D arrays !')\n",
    "    x_perp_vector=vector3_1[1]*vector3_2[2]-vector3_1[2]*vector3_2[1]\n",
    "    y_perp_vector=vector3_1[2]*vector3_2[0]-vector3_1[0]*vector3_2[2]\n",
    "    z_perp_vector=vector3_1[0]*vector3_2[1]-vector3_1[1]*vector3_2[0]\n",
    "    return np.array([x_perp_vector,y_perp_vector,z_perp_vector])\n",
    "\n",
    "def dot_product(vector1,vector2):\n",
    "    if len(vector1)!=len(vector2):\n",
    "        raise Arrays_of_different_size\n",
    "    prod=0\n",
    "    for i in range(len(vector1)):\n",
    "        prod=prod+vector1[i]*vector2[i]\n",
    "    return prod\n",
    "\n",
    "\n",
    "def norm(vector):\n",
    "    if len(vector)!=3:\n",
    "        print('This is only for a 3d vector')\n",
    "    return np.sqrt(vector[0]**2+vector[1]**2+vector[2]**2)\n",
    "\n",
    "def remove9999 (Momenta4, leading):\n",
    "    if leading == 1:\n",
    "        nu_ref = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_1\"])), df4[\"gen_nu_eta_1\"], df4[\"gen_nu_phi_1\"], df4[\"gen_nu_p_1\"])\n",
    "    if leading == 2:\n",
    "        nu_ref = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_2\"])), df4[\"gen_nu_eta_2\"], df4[\"gen_nu_phi_2\"], df4[\"gen_nu_p_2\"])\n",
    "    \n",
    "    array = np.array(Momenta4).T\n",
    "    array = array[nu_ref.p_z != 9999]\n",
    "    array = array.T\n",
    "    return Momentum4(array[0], array[1], array[2], array[3])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.27779362  -7.9413372   19.9089624    1.47639641  -7.58880139\n",
      " -24.29430518   0.02879935  -5.44053274  -0.26077366 -13.76051982]\n",
      "[-17.29895991 -38.50007335  47.69380366  54.71266193 -42.51451951\n",
      " -77.34265654 -10.36364286 -31.34084211  -4.94295609 -54.23717798]  x components \n",
      "\n",
      "[-10.31827882   5.69031028   2.12723561  -1.20907966  -4.02439091\n",
      "  -2.20871047   1.07791451  -7.47940571   4.23899383  -6.62110983]\n",
      "[-47.21142448  25.17995375   3.27720515 -42.70481493 -22.43836819\n",
      "  -9.39283425  59.81469305 -45.52753998  54.36820086 -28.15426278]  y components \n",
      "\n",
      "[ -4.95716249 -10.51069943  29.52442858  -2.26809349  -1.12989077\n",
      "  31.60850948   0.41798185   0.9331691   -6.62401319 -16.2533514 ]\n",
      "[-23.37763147 -46.96506028  70.00709096 -97.64206319 -10.04156086\n",
      "  98.53366711  25.51788593   2.04571464 -75.74140259 -61.6938513 ]  z components \n",
      "\n",
      "[1.22095212 1.10447608 0.82341992 ... 0.78452161 0.62563757 0.76930124]\n"
     ]
    }
   ],
   "source": [
    "#define the usefull 4 momenta\n",
    "\n",
    "#neutrinos refs, in E, px, py, pz form\n",
    "nu_1 = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_1\"])), df4[\"gen_nu_eta_1\"], df4[\"gen_nu_phi_1\"], df4[\"gen_nu_p_1\"])\n",
    "nu_2 = Momentum4.m_eta_phi_p(np.zeros(len(df4[\"gen_nu_phi_2\"])), df4[\"gen_nu_eta_2\"], df4[\"gen_nu_phi_2\"], df4[\"gen_nu_p_2\"])\n",
    "\n",
    "#Charged and neutral pion momenta\n",
    "pi_1_4Mom = Momentum4(df4[\"pi_E_1\"],df4[\"pi_px_1\"],df4[\"pi_py_1\"],df4[\"pi_pz_1\"])\n",
    "pi_2_4Mom = Momentum4(df4[\"pi_E_2\"],df4[\"pi_px_2\"],df4[\"pi_py_2\"],df4[\"pi_pz_2\"]) \n",
    "\n",
    "#Same for the pi0\n",
    "pi0_1_4Mom = Momentum4(df4[\"pi0_E_1\"],df4[\"pi0_px_1\"],df4[\"pi0_py_1\"],df4[\"pi0_pz_1\"])\n",
    "pi0_2_4Mom = Momentum4(df4[\"pi0_E_2\"],df4[\"pi0_px_2\"],df4[\"pi0_py_2\"],df4[\"pi0_pz_2\"])\n",
    "\n",
    "tau_1_vis = pi_1_4Mom + pi0_1_4Mom\n",
    "tau_2_vis = pi_2_4Mom + pi0_2_4Mom\n",
    "\n",
    "\n",
    "print(nu_1.p_x[:10])\n",
    "print(tau_1_vis.p_x[:10], ' x components \\n')\n",
    "\n",
    "print(nu_1.p_y[:10])\n",
    "print(tau_1_vis.p_y[:10], ' y components \\n')\n",
    "\n",
    "print(nu_1.p_z[:10])\n",
    "print(tau_1_vis.p_z[:10], ' z components \\n')\n",
    "\n",
    "print(tau_1_vis.m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Here, start straining to regress neutrinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_1 = nu_1.p/tau_1_vis.p\n",
    "alpha_2 = nu_2.p/tau_2_vis.p \n",
    "#p is the magnitude of the momenta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5. 5. 5. ... 5. 5. 5.], shape=(968192,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def one_d(val):\n",
    "    return tf.constant(val, shape = df4[\"pi_px_1\"].shape, dtype = np.float32)\n",
    "\n",
    "def Mom4_to_tf(Mom4_1D):\n",
    "    return tf.convert_to_tensor(Mom4_1D, dtype = 'float32')\n",
    "\n",
    "print (one_d (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here start checking the colinearity method\n",
    "\n",
    "#first verify the spread of phi_nu compared to phi_vis\n",
    "hist = np.array(nu_1.phi - tau_1_vis.phi)\n",
    "hist1 = np.array(nu_2.phi - tau_2_vis.phi)\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "plt.hist(hist, bins = 1000, alpha=0.5, label = 'Difefrence between nu_phi_1 and vis_phi_1\\nMean:%.2f, Std:%.2f'%(hist.mean(), hist.std()))\n",
    "plt.hist(hist1, bins = 1000, alpha=0.5, label = 'Difefrence between nu_phi_2 and vis_phi_2\\nMean:%.2f, Std:%.2f'%(hist1.mean(), hist1.std()))\n",
    "plt.legend()\n",
    "plt.xlim(-1,1)\n",
    "plt.grid()\n",
    "plt.ylabel('Occurencies')\n",
    "plt.title('Check for phi')\n",
    "plt.xlabel('nu_phi - vis_phi')\n",
    "plt.savefig('Diff_nu_vis_phi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lab_frame (InputLayer)       [(None, 968192, 13)]      0         \n",
      "_________________________________________________________________\n",
      "learning (Dense)             (None, 968192, 64)        896       \n",
      "_________________________________________________________________\n",
      "learning2 (Dense)            (None, 968192, 64)        4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 968192, 64)        0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 968192, 1)         65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1349/1356 [============================>.] - ETA: 0s - loss: 6.1450 - mae: 1.6632WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 6.1269 - mae: 1.6615 - val_loss: 2.0465 - val_mae: 1.1538\n",
      "Epoch 2/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.2569 - mae: 1.3031 - val_loss: 1.8363 - val_mae: 1.0923\n",
      "Epoch 3/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.1544 - mae: 1.2807 - val_loss: 1.7916 - val_mae: 1.0777\n",
      "Epoch 4/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.1267 - mae: 1.2740 - val_loss: 1.7782 - val_mae: 1.0725\n",
      "Epoch 5/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.1130 - mae: 1.2700 - val_loss: 1.7704 - val_mae: 1.0702\n",
      "Epoch 6/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.1069 - mae: 1.2681 - val_loss: 1.7694 - val_mae: 1.0703\n",
      "Epoch 7/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.1022 - mae: 1.2668 - val_loss: 1.7671 - val_mae: 1.0691\n",
      "Epoch 8/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0982 - mae: 1.2656 - val_loss: 1.7675 - val_mae: 1.0701\n",
      "Epoch 9/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0952 - mae: 1.2647 - val_loss: 1.7618 - val_mae: 1.0665\n",
      "Epoch 10/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0918 - mae: 1.2636 - val_loss: 1.7643 - val_mae: 1.0693\n",
      "Epoch 11/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0876 - mae: 1.2626 - val_loss: 1.7567 - val_mae: 1.0662\n",
      "Epoch 12/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0849 - mae: 1.2620 - val_loss: 1.7541 - val_mae: 1.0654\n",
      "Epoch 13/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0829 - mae: 1.2615 - val_loss: 1.7504 - val_mae: 1.0641\n",
      "Epoch 14/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0821 - mae: 1.2612 - val_loss: 1.7516 - val_mae: 1.0647\n",
      "Epoch 15/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0815 - mae: 1.2612 - val_loss: 1.7507 - val_mae: 1.0651\n",
      "Epoch 16/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0810 - mae: 1.2610 - val_loss: 1.7497 - val_mae: 1.0645\n",
      "Epoch 17/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0804 - mae: 1.2609 - val_loss: 1.7508 - val_mae: 1.0656\n",
      "Epoch 18/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0802 - mae: 1.2608 - val_loss: 1.7554 - val_mae: 1.0675\n",
      "Epoch 19/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0799 - mae: 1.2608 - val_loss: 1.7508 - val_mae: 1.0660\n",
      "Epoch 20/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0792 - mae: 1.2607 - val_loss: 1.7517 - val_mae: 1.0657\n",
      "Epoch 21/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0796 - mae: 1.2607 - val_loss: 1.7479 - val_mae: 1.0640\n",
      "Epoch 22/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0790 - mae: 1.2606 - val_loss: 1.7497 - val_mae: 1.0650\n",
      "Epoch 23/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0789 - mae: 1.2605 - val_loss: 1.7479 - val_mae: 1.0638\n",
      "Epoch 24/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0786 - mae: 1.2605 - val_loss: 1.7469 - val_mae: 1.0641\n",
      "Epoch 25/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0783 - mae: 1.2604 - val_loss: 1.7466 - val_mae: 1.0642\n",
      "Epoch 26/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0785 - mae: 1.2604 - val_loss: 1.7474 - val_mae: 1.0645\n",
      "Epoch 27/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0781 - mae: 1.2603 - val_loss: 1.7474 - val_mae: 1.0641\n",
      "Epoch 28/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0784 - mae: 1.2604 - val_loss: 1.7508 - val_mae: 1.0652\n",
      "Epoch 29/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0777 - mae: 1.2602 - val_loss: 1.7483 - val_mae: 1.0645\n",
      "Epoch 30/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0779 - mae: 1.2603 - val_loss: 1.7471 - val_mae: 1.0643\n",
      "Epoch 31/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0780 - mae: 1.2603 - val_loss: 1.7473 - val_mae: 1.0642\n",
      "Epoch 32/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0778 - mae: 1.2603 - val_loss: 1.7487 - val_mae: 1.0644\n",
      "Epoch 33/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0777 - mae: 1.2602 - val_loss: 1.7463 - val_mae: 1.0638\n",
      "Epoch 34/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0777 - mae: 1.2602 - val_loss: 1.7460 - val_mae: 1.0640\n",
      "Epoch 35/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0774 - mae: 1.2602 - val_loss: 1.7469 - val_mae: 1.0638\n",
      "Epoch 36/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0777 - mae: 1.2602 - val_loss: 1.7494 - val_mae: 1.0650\n",
      "Epoch 37/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0778 - mae: 1.2603 - val_loss: 1.7470 - val_mae: 1.0638\n",
      "Epoch 38/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0776 - mae: 1.2602 - val_loss: 1.7503 - val_mae: 1.0660\n",
      "Epoch 39/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0774 - mae: 1.2601 - val_loss: 1.7486 - val_mae: 1.0653\n",
      "Epoch 40/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0771 - mae: 1.2601 - val_loss: 1.7466 - val_mae: 1.0639\n",
      "Epoch 41/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0772 - mae: 1.2601 - val_loss: 1.7511 - val_mae: 1.0658\n",
      "Epoch 42/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0774 - mae: 1.2602 - val_loss: 1.7463 - val_mae: 1.0642\n",
      "Epoch 43/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0769 - mae: 1.2600 - val_loss: 1.7467 - val_mae: 1.0646\n",
      "Epoch 44/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0772 - mae: 1.2601 - val_loss: 1.7519 - val_mae: 1.0662\n",
      "Epoch 45/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0770 - mae: 1.2600 - val_loss: 1.7465 - val_mae: 1.0643\n",
      "Epoch 46/50\n",
      "1356/1356 [==============================] - 3s 2ms/step - loss: 2.0768 - mae: 1.2600 - val_loss: 1.7467 - val_mae: 1.0640\n",
      "Epoch 47/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0768 - mae: 1.2600 - val_loss: 1.7486 - val_mae: 1.0658\n",
      "Epoch 48/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0767 - mae: 1.2600 - val_loss: 1.7510 - val_mae: 1.0661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0766 - mae: 1.2600 - val_loss: 1.7466 - val_mae: 1.0644\n",
      "Epoch 50/50\n",
      "1356/1356 [==============================] - 2s 2ms/step - loss: 2.0769 - mae: 1.2600 - val_loss: 1.7470 - val_mae: 1.0643\n"
     ]
    }
   ],
   "source": [
    "#Then Try simple NN to regress only the phi component of the two neutrinos simultaneously\n",
    "\n",
    "x = [Mom4_to_tf(tau_1_vis.e),\n",
    "     Mom4_to_tf(tau_1_vis.p_x),\n",
    "     Mom4_to_tf(tau_1_vis.p_y),\n",
    "     Mom4_to_tf(tau_1_vis.p_z),\n",
    "     Mom4_to_tf(tau_1_vis.phi),\n",
    "     Mom4_to_tf(tau_2_vis.e),\n",
    "     Mom4_to_tf(tau_2_vis.p_x),\n",
    "     Mom4_to_tf(tau_2_vis.p_y),\n",
    "     Mom4_to_tf(tau_2_vis.p_z),\n",
    "     Mom4_to_tf(tau_2_vis.phi),\n",
    "     df4['met'],\n",
    "     df4['metx'],\n",
    "     df4['mety']\n",
    "    ]\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = [Mom4_to_tf(nu_1.phi), Mom4_to_tf(nu_2.phi)]\n",
    "\n",
    "y = tf.transpose(y)\n",
    "\n",
    "\n",
    "# def loss_fn_phi(y_true, y_pred):\n",
    "#     return (tf.math.cos(y_true)-tf.math.cos(y_pred))**2\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(64, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(64, activation = 'relu', name=\"learning2\")(x2)\n",
    "x4 = tf.keras.layers.Dropout(0.1, name=\"dropout\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(300, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "# t = tau_1_vis_loss\n",
    "# y_ = y.T\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss_fn_phi = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(loss = loss_fn_phi, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "fig = plt.figure(figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-300-o,\\n Only regress phi Batch: 500')\n",
    "#ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, 0])\n",
    "hist2 = np.array(y[:, 0])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 1000, alpha = 0.5,label = \"True-Regressed nu_phi_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "# hist1 = np.array(model({\"lab_frame\": x})[:, 1])\n",
    "# hist2 = np.array(y[:, 1])\n",
    "# hist_b = hist2-hist1\n",
    "# plt.hist(hist_b, bins = 1000, alpha = 0.5,label = \"True-Regressed nu_phi_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-2,2)\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlabel('Difference in phi')\n",
    "\n",
    "plt.savefig('Regress_only_phi_C4.png')\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find all the tranings with the custom loss function and the full 4 vector of the neutrinos, here for book keeping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(968192,), dtype=float32)\n",
      "tf.Tensor([0.08989737 0.0308425  0.00908413 ... 0.04957712 0.01530869 0.05492   ], shape=(968192,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Here copying the result from the KIT paper\n",
    "\n",
    "smear_px, smear_py = one_d(0), one_d(0)   #the smearing of the detector, we don't know yet what it is\n",
    "\n",
    "ref = [#smear_px,py                      #0\n",
    "       one_d(1.776),                      #1\n",
    "       #df4[\"metx\"],                   #2\n",
    "       #df4[\"mety\"],                   #3\n",
    "       Mom4_to_tf(tau_1_vis.e),       #4\n",
    "       Mom4_to_tf(tau_1_vis.p_x),     #5\n",
    "       Mom4_to_tf(tau_1_vis.p_y),     #6\n",
    "       Mom4_to_tf(tau_1_vis.p_z),     #7\n",
    "       Mom4_to_tf(tau_2_vis.e),       #8\n",
    "       Mom4_to_tf(tau_2_vis.p_x),     #9 \n",
    "       Mom4_to_tf(tau_2_vis.p_y),     #10\n",
    "       Mom4_to_tf(tau_2_vis.p_z),     #11\n",
    "       one_d(125),                    #12\n",
    "       #Mom4_to_tf(nu_1.e),            #13       corresponding to          #0\n",
    "       Mom4_to_tf(nu_1.p_x),          #14                                 #1\n",
    "       Mom4_to_tf(nu_1.p_y),          #15                                 #2\n",
    "       Mom4_to_tf(nu_1.p_z),          #16                                 #3\n",
    "       #Mom4_to_tf(nu_2.e),            #17                                 #4\n",
    "       Mom4_to_tf(nu_2.p_x),          #18                                 #5\n",
    "       Mom4_to_tf(nu_2.p_y),          #19                                 #6\n",
    "       Mom4_to_tf(nu_2.p_z),          #20                                 #7\n",
    "]\n",
    "\n",
    "\n",
    "#i_smear_px = 0\n",
    "i_tau_mass = 0\n",
    "#i_smeared_met_px = 1\n",
    "#i_smeared_met_py = 2\n",
    "i_tau1_e = 1\n",
    "i_tau1_px = 2\n",
    "i_tau1_py = 3\n",
    "i_tau1_pz = 4\n",
    "i_tau2_e = 5\n",
    "i_tau2_px = 6\n",
    "i_tau2_py = 7\n",
    "i_tau2_pz = 8\n",
    "i_gen_mass = 9\n",
    "#i_nu1_e = 12\n",
    "i_nu1_px = 10\n",
    "i_nu1_py = 11\n",
    "i_nu1_pz = 12\n",
    "#i_nu2_e = 16\n",
    "i_nu2_px = 13\n",
    "i_nu2_py = 14\n",
    "i_nu2_pz = 15\n",
    "\n",
    "\n",
    "m_tau_squared = tf.transpose(one_d(1.776)**2)\n",
    "\n",
    "ref = tf.transpose(ref)\n",
    "\n",
    "def loss_D_p (y_true, y_pred):\n",
    "    #calculting the difference between the the components, need to add the smearing of detector eventually\n",
    "    target_components = [i_nu1_px, i_nu1_py, i_nu1_pz, i_nu2_px, i_nu2_py, i_nu2_pz]\n",
    "    target_components_diff_list = []\n",
    "    for i in target_components: target_components_diff_list.append((y_true[:,i]-y_pred[:,i])**2)\n",
    "    dxyz = 0\n",
    "    for d in target_components_diff_list: dxyz+=d\n",
    "    return dxyz\n",
    "\n",
    "def energy_nu (y_true, y_pred, number):\n",
    "    if number == 1:\n",
    "        return tf.sqrt(y_pred[:, i_nu1_px]**2 + y_pred[:, i_nu1_py]**2 + y_pred[:, i_nu1_pz]**2)\n",
    "    if number == 2:\n",
    "        return tf.sqrt(y_pred[:, i_nu2_px]**2 + y_pred[:, i_nu2_py]**2 + y_pred[:, i_nu2_pz]**2)\n",
    "\n",
    "def loss_mass_tau(y_true, y_pred):\n",
    "    #now we try only to use the y_pred for neutrino info, this is I guess their way of \n",
    "    #only training for neutrino info whilst keeping nice structure\n",
    "    #we are always assuming m=0\n",
    "    # note, we are taking y_tau as exact, we could choose not to...\n",
    "    E1 = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) \n",
    "    P1_squared = (y_true[:, i_tau1_px] + y_pred[:, i_nu1_px])**2 + (y_true[:, i_tau1_py] + y_pred[:, i_nu1_py])**2 + (y_true[:, i_tau1_pz] + y_pred[:, i_nu1_pz])**2\n",
    "    R1 = (E1**2 - P1_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    \n",
    "    E2 = y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2) \n",
    "    P2_squared = (y_true[:, i_tau2_px] + y_pred[:, i_nu2_px])**2 + (y_true[:, i_tau2_py] + y_pred[:, i_nu2_py])**2 + (y_true[:, i_tau2_pz] + y_pred[:, i_nu2_pz])**2\n",
    "    R2 = (E2**2 - P2_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    return tf.math.abs(R1) + tf.abs(R2)\n",
    "\n",
    "\n",
    "def loss_mass_Higgs(y_true, y_pred):\n",
    "    EH = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) + y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2)\n",
    "    px_H = y_true[:, i_tau1_px] + y_true[:, i_tau2_px] + y_pred[:, i_nu1_px] + y_pred[:, i_nu2_px]\n",
    "    py_H = y_true[:, i_tau1_py] + y_true[:, i_tau2_py] + y_pred[:, i_nu1_py] + y_pred[:, i_nu2_py]\n",
    "    pz_H = y_true[:, i_tau1_pz] + y_true[:, i_tau2_pz] + y_pred[:, i_nu1_pz] + y_pred[:, i_nu2_pz]\n",
    "    \n",
    "    return tf.abs((EH**2-px_H**2-py_H**2-pz_H**2 - y_true[:, i_gen_mass]**2)/(y_true[:,i_gen_mass])**2)\n",
    "\n",
    "\n",
    "def loss_phi(y_true, y_pred):\n",
    "    phi_diff_1 = (tf.math.atan2(y_pred[:, i_nu1_py],y_pred[:, i_nu1_px])-tf.math.atan2(y_true[:, i_nu1_py],y_true[:, i_nu1_px]))**2\n",
    "    phi_diff_2 = (tf.math.atan2(y_pred[:, i_nu2_py],y_pred[:, i_nu2_px])-tf.math.atan2(y_true[:, i_nu2_py],y_true[:, i_nu2_px]))**2\n",
    "\n",
    "    \n",
    "    return tf.convert_to_tensor(phi_diff_1+phi_diff_2)\n",
    "    \n",
    "\n",
    "print(loss_phi(ref,ref))\n",
    "print(loss_mass_Higgs(ref,ref))\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "\n",
    "    dxyz = loss_D_p(y_true, y_pred)\n",
    "    #dmet = loss_dmet(y_true, y_pred)\n",
    "    #dPTtaus = loss_dPTtaus(y_true, y_pred)\n",
    "    dphi = loss_phi(y_true,y_pred)\n",
    "    dtau = loss_mass_tau(y_true, y_pred)\n",
    "    dHiggs = loss_mass_Higgs(y_true, y_pred)\n",
    "    #dM = loss_dM_had(y_true, y_pred)\n",
    "\n",
    "    return dxyz + dtau + dHiggs + dphi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(968192,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2cff96f962f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#y = np.array([alpha_1[:n], alpha_2[:n]]).T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ref' is not defined"
     ]
    }
   ],
   "source": [
    "# Start training here: would work better with small dataset !\n",
    "nu_1_training = np.array(nu_1)[0]\n",
    "\n",
    "print(nu_1_training.shape)\n",
    "\n",
    "n = -1\n",
    "\n",
    "#these inputs aren't sufficient for the upgraded loss function...\n",
    "# x = np.array([tau_1_vis[0][:n], tau_1_vis[1][:n], tau_1_vis[2][:n], \n",
    "#                tau_1_vis[3][:n], df4[\"metx\"][:n], df4[\"mety\"][:n], df4[\"met\"][:n]]).T\n",
    "\n",
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              df4[\"metx\"],\n",
    "              df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              df4[\"met\"]\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "#y = np.array([alpha_1[:n], alpha_2[:n]]).T\n",
    "\n",
    "#tau_1_vis_loss = tf.convert_to_tensor([tau_1_vis[0][:10000], tau_1_vis[1][:10000], tau_1_vis[2][:10000], tau_1_vis[3][:10000]], dtype = 'float32')\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(1000, activation = 'relu', kernel_regularizer=tf.keras.regularizers.L2(0.2), name=\"learning\")(input_1)\n",
    "#x4 = tf.keras.layers.Dropout(0.5, name=\"dropout\")(x2)\n",
    "x3 = tf.keras.layers.Dense(1000, activation = 'elu', kernel_regularizer=tf.keras.regularizers.L2(0.2), name=\"learning2\")(x2)\n",
    "#x4 = tf.keras.layers.Dropout(0.1, name=\"dropout\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(300, activation = 'elu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(16, name=\"output\")(x3)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "# t = tau_1_vis_loss\n",
    "# y_ = y.T\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-f7e40b82fd17>:5: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(2,1,1)\n",
      "<ipython-input-52-f7e40b82fd17>:27: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(2,1,2)\n"
     ]
    }
   ],
   "source": [
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-300-o,\\ninputs exclude mets, output exclude met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('4_vect_qual_500_500_B500_R_4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here verify the phi component\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "hist1 = tf.math.atan2(model({\"lab_frame\": x})[:, i_nu1_py], model({\"lab_frame\": x})[:, i_nu1_px])\n",
    "hist2 = Mom4_to_tf(nu_1.phi)\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_phi_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "hist1 = tf.math.atan2(model({\"lab_frame\": x})[:, i_nu2_py], model({\"lab_frame\": x})[:, i_nu2_px])\n",
    "hist2 = Mom4_to_tf(nu_2.phi)\n",
    "hist_b = np.array(hist2-hist1)\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_phi_2\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-2,2)\n",
    "plt.xlabel('Difference of phi component')\n",
    "\n",
    "plt.savefig('Phi_qual_500_500_B500_R_4.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2\n",
    "\n",
    "         \n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "1356/1356 [==============================] - ETA: 0s - loss: 1917.6036 - mae: 39.9942WARNING:tensorflow:Model was constructed with shape (None, 968192, 10) for input Tensor(\"lab_frame_4:0\", shape=(None, 968192, 10), dtype=float32), but it was called on an input with incompatible shape (None, 10).\n",
      "1356/1356 [==============================] - 15s 11ms/step - loss: 1917.6036 - mae: 39.9942 - val_loss: 2610.1282 - val_mae: 43.4733\n",
      "Epoch 2/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1699.7272 - mae: 39.6700 - val_loss: 2597.8945 - val_mae: 43.7552\n",
      "Epoch 3/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1677.6960 - mae: 39.7335 - val_loss: 2549.5977 - val_mae: 43.7872\n",
      "Epoch 4/50\n",
      "1356/1356 [==============================] - 14s 11ms/step - loss: 1663.7881 - mae: 39.7429 - val_loss: 2485.2976 - val_mae: 43.4555\n",
      "Epoch 5/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1651.1306 - mae: 39.8377 - val_loss: 2494.3550 - val_mae: 43.6106\n",
      "Epoch 6/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1643.9099 - mae: 39.8467 - val_loss: 2494.1179 - val_mae: 43.6432\n",
      "Epoch 7/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1634.3356 - mae: 39.8797 - val_loss: 2473.9553 - val_mae: 43.6530\n",
      "Epoch 8/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1630.4716 - mae: 39.8815 - val_loss: 2447.2979 - val_mae: 43.7359\n",
      "Epoch 9/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1622.0109 - mae: 39.8931 - val_loss: 2495.3511 - val_mae: 43.7103\n",
      "Epoch 10/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1621.7166 - mae: 39.9172 - val_loss: 2460.1260 - val_mae: 43.6831\n",
      "Epoch 11/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1612.5052 - mae: 39.9166 - val_loss: 2489.4316 - val_mae: 43.7947\n",
      "Epoch 12/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1608.5955 - mae: 39.9081 - val_loss: 2459.4163 - val_mae: 43.7122\n",
      "Epoch 13/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1608.4413 - mae: 39.9142 - val_loss: 2539.3528 - val_mae: 43.5760\n",
      "Epoch 14/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1605.4712 - mae: 39.9686 - val_loss: 2489.7297 - val_mae: 43.7217\n",
      "Epoch 15/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1597.7333 - mae: 39.9655 - val_loss: 2440.1936 - val_mae: 43.7354\n",
      "Epoch 16/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1597.1824 - mae: 39.9943 - val_loss: 2457.0183 - val_mae: 43.6589\n",
      "Epoch 17/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1594.3080 - mae: 40.0058 - val_loss: 2459.9612 - val_mae: 43.7222\n",
      "Epoch 18/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1594.0132 - mae: 40.0000 - val_loss: 2500.2651 - val_mae: 43.9251\n",
      "Epoch 19/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1583.8054 - mae: 40.0577 - val_loss: 2491.0281 - val_mae: 43.9807\n",
      "Epoch 20/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1583.2651 - mae: 40.0868 - val_loss: 2514.5571 - val_mae: 43.5606\n",
      "Epoch 21/50\n",
      "1356/1356 [==============================] - 17s 12ms/step - loss: 1577.4895 - mae: 40.0679 - val_loss: 2529.6833 - val_mae: 44.0536\n",
      "Epoch 22/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1576.4879 - mae: 40.1033 - val_loss: 2492.8384 - val_mae: 43.8942\n",
      "Epoch 23/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1571.1874 - mae: 40.1227 - val_loss: 2472.2207 - val_mae: 44.0561\n",
      "Epoch 24/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1571.9403 - mae: 40.1778 - val_loss: 2503.5684 - val_mae: 43.8279\n",
      "Epoch 25/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1569.0905 - mae: 40.1779 - val_loss: 2468.4170 - val_mae: 43.9947\n",
      "Epoch 26/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1565.1615 - mae: 40.2022 - val_loss: 2486.9824 - val_mae: 43.9508\n",
      "Epoch 27/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1558.7700 - mae: 40.2188 - val_loss: 2475.9065 - val_mae: 43.9619\n",
      "Epoch 28/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1558.1907 - mae: 40.2382 - val_loss: 2498.5112 - val_mae: 44.0257\n",
      "Epoch 29/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1552.7136 - mae: 40.2349 - val_loss: 2520.7000 - val_mae: 44.1866\n",
      "Epoch 30/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1550.0940 - mae: 40.2576 - val_loss: 2521.6653 - val_mae: 44.0729\n",
      "Epoch 31/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1544.7440 - mae: 40.2847 - val_loss: 2521.4771 - val_mae: 44.0893\n",
      "Epoch 32/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1541.3024 - mae: 40.3077 - val_loss: 2508.6287 - val_mae: 44.3325\n",
      "Epoch 33/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1540.9872 - mae: 40.3195 - val_loss: 2512.3147 - val_mae: 44.1629\n",
      "Epoch 34/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1533.9865 - mae: 40.3493 - val_loss: 2603.9446 - val_mae: 44.5622\n",
      "Epoch 35/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1531.9338 - mae: 40.3968 - val_loss: 2521.9021 - val_mae: 44.1406\n",
      "Epoch 36/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1533.7352 - mae: 40.3765 - val_loss: 2516.8896 - val_mae: 44.1622\n",
      "Epoch 37/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1524.0305 - mae: 40.4048 - val_loss: 2547.2910 - val_mae: 44.3408\n",
      "Epoch 38/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1518.9465 - mae: 40.4270 - val_loss: 2515.4958 - val_mae: 44.2166\n",
      "Epoch 39/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1518.4982 - mae: 40.4616 - val_loss: 2521.7163 - val_mae: 44.2120\n",
      "Epoch 40/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1514.5626 - mae: 40.4721 - val_loss: 2532.1399 - val_mae: 44.0862\n",
      "Epoch 41/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1511.2383 - mae: 40.4589 - val_loss: 2528.5022 - val_mae: 44.4758\n",
      "Epoch 42/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1508.4866 - mae: 40.5074 - val_loss: 2568.1223 - val_mae: 44.6798\n",
      "Epoch 43/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1507.3695 - mae: 40.5364 - val_loss: 2515.4834 - val_mae: 44.3125\n",
      "Epoch 44/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1500.6742 - mae: 40.5202 - val_loss: 2521.2170 - val_mae: 44.2737\n",
      "Epoch 45/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1492.9366 - mae: 40.5404 - val_loss: 2560.5671 - val_mae: 44.5046\n",
      "Epoch 46/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1492.0536 - mae: 40.5596 - val_loss: 2558.2803 - val_mae: 44.4379\n",
      "Epoch 47/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1490.9824 - mae: 40.6010 - val_loss: 2512.8352 - val_mae: 44.4544\n",
      "Epoch 48/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1488.2098 - mae: 40.6221 - val_loss: 2557.1802 - val_mae: 44.5234\n",
      "Epoch 49/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1477.4401 - mae: 40.6374 - val_loss: 2577.4529 - val_mae: 44.5623\n",
      "Epoch 50/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 1483.1232 - mae: 40.6474 - val_loss: 2570.5508 - val_mae: 44.5914\n"
     ]
    }
   ],
   "source": [
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              #df4[\"metx\"],\n",
    "              #df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              #df4[\"met\"]\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(300, activation = 'elu', name=\"learning2\")(x2)\n",
    "x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(16, name=\"output\")(x4)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "#tf.keras.losses.MeanSquaredError() #common to the 4 iterations\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 500-300-500-o,\\ninputs exclude mets, output exclude met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('Quality_500_300_500_1_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Traning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.6240624e-04 8.4828280e-05 5.4249998e-05 ... 2.8547027e-05 9.8437504e-06\n",
      " 1.7312501e-05], shape=(968192,), dtype=float32)\n",
      "tf.Tensor([0.08989737 0.0308425  0.00908413 ... 0.04957712 0.01530869 0.05492   ], shape=(968192,), dtype=float32)\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame_6:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame_6:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1354/1356 [============================>.] - ETA: 0s - loss: 1543.7555 - mae: 38.5546WARNING:tensorflow:Model was constructed with shape (None, 968192, 13) for input Tensor(\"lab_frame_6:0\", shape=(None, 968192, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1543.2177 - mae: 38.5539 - val_loss: 1841.2581 - val_mae: 42.0406\n",
      "Epoch 2/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 1214.5883 - mae: 37.8601 - val_loss: 1733.1243 - val_mae: 41.7674\n",
      "Epoch 3/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1147.7802 - mae: 37.7313 - val_loss: 1692.9059 - val_mae: 41.6186\n",
      "Epoch 4/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 1111.9667 - mae: 37.6784 - val_loss: 1676.6332 - val_mae: 41.6002\n",
      "Epoch 5/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1091.7142 - mae: 37.6597 - val_loss: 1594.8550 - val_mae: 41.5146\n",
      "Epoch 6/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1071.4916 - mae: 37.6533 - val_loss: 1624.4427 - val_mae: 41.6492\n",
      "Epoch 7/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 1060.8175 - mae: 37.6789 - val_loss: 1565.7533 - val_mae: 41.6062\n",
      "Epoch 8/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1049.4861 - mae: 37.6720 - val_loss: 1548.3080 - val_mae: 41.6144\n",
      "Epoch 9/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1035.5745 - mae: 37.6883 - val_loss: 1574.4279 - val_mae: 41.6558\n",
      "Epoch 10/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1025.9240 - mae: 37.7154 - val_loss: 1532.8997 - val_mae: 41.6905\n",
      "Epoch 11/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1017.7857 - mae: 37.7298 - val_loss: 1542.9863 - val_mae: 41.5890\n",
      "Epoch 12/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 1013.5701 - mae: 37.7499 - val_loss: 1534.5054 - val_mae: 41.6585\n",
      "Epoch 13/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 1004.5600 - mae: 37.7595 - val_loss: 1521.7397 - val_mae: 41.7052\n",
      "Epoch 14/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 996.6945 - mae: 37.7616 - val_loss: 1512.1178 - val_mae: 41.6455\n",
      "Epoch 15/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 991.7304 - mae: 37.7807 - val_loss: 1545.9613 - val_mae: 41.7425\n",
      "Epoch 16/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 986.1164 - mae: 37.7876 - val_loss: 1545.2850 - val_mae: 41.7869\n",
      "Epoch 17/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 980.5477 - mae: 37.8080 - val_loss: 1539.1650 - val_mae: 41.9055\n",
      "Epoch 18/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 974.1382 - mae: 37.7916 - val_loss: 1548.4375 - val_mae: 41.7828\n",
      "Epoch 19/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 969.2938 - mae: 37.8233 - val_loss: 1544.8883 - val_mae: 41.9162\n",
      "Epoch 20/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 963.5911 - mae: 37.8358 - val_loss: 1510.0526 - val_mae: 41.8160\n",
      "Epoch 21/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 959.3618 - mae: 37.8543 - val_loss: 1520.6190 - val_mae: 41.7609\n",
      "Epoch 22/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 957.3499 - mae: 37.8484 - val_loss: 1521.5811 - val_mae: 41.8323\n",
      "Epoch 23/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 947.8010 - mae: 37.8469 - val_loss: 1513.6431 - val_mae: 41.9332\n",
      "Epoch 24/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 948.0798 - mae: 37.8761 - val_loss: 1515.9496 - val_mae: 41.9018\n",
      "Epoch 25/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 938.1719 - mae: 37.8894 - val_loss: 1508.4991 - val_mae: 41.7926\n",
      "Epoch 26/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 937.1107 - mae: 37.9071 - val_loss: 1566.4767 - val_mae: 41.9987\n",
      "Epoch 27/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 939.6241 - mae: 37.9263 - val_loss: 1500.1003 - val_mae: 41.9180\n",
      "Epoch 28/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 925.2584 - mae: 37.9196 - val_loss: 1511.9523 - val_mae: 41.9032\n",
      "Epoch 29/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 927.2836 - mae: 37.9401 - val_loss: 1511.0701 - val_mae: 42.1022\n",
      "Epoch 30/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 920.6776 - mae: 37.9480 - val_loss: 1544.0540 - val_mae: 42.0636\n",
      "Epoch 31/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 916.6577 - mae: 37.9651 - val_loss: 1507.0012 - val_mae: 41.9874\n",
      "Epoch 32/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 911.2335 - mae: 37.9713 - val_loss: 1541.2793 - val_mae: 41.8976\n",
      "Epoch 33/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 910.1839 - mae: 37.9544 - val_loss: 1503.1342 - val_mae: 42.0761\n",
      "Epoch 34/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 904.4823 - mae: 37.9951 - val_loss: 1494.4731 - val_mae: 42.0667\n",
      "Epoch 35/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 902.5694 - mae: 38.0226 - val_loss: 1534.6825 - val_mae: 42.0785\n",
      "Epoch 36/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 900.7153 - mae: 38.0402 - val_loss: 1524.1418 - val_mae: 42.1261\n",
      "Epoch 37/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 896.0626 - mae: 38.0637 - val_loss: 1512.2312 - val_mae: 42.0815\n",
      "Epoch 38/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 890.6423 - mae: 38.0869 - val_loss: 1520.8789 - val_mae: 42.1243\n",
      "Epoch 39/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 887.1625 - mae: 38.0842 - val_loss: 1506.0841 - val_mae: 42.1827\n",
      "Epoch 40/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 888.6796 - mae: 38.0946 - val_loss: 1531.1791 - val_mae: 42.2469\n",
      "Epoch 41/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 884.8829 - mae: 38.0983 - val_loss: 1526.8054 - val_mae: 42.2663\n",
      "Epoch 42/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 879.3757 - mae: 38.0917 - val_loss: 1534.4884 - val_mae: 42.1807\n",
      "Epoch 43/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 878.3714 - mae: 38.1250 - val_loss: 1533.6588 - val_mae: 42.2208\n",
      "Epoch 44/50\n",
      "1356/1356 [==============================] - 14s 10ms/step - loss: 874.6194 - mae: 38.1180 - val_loss: 1573.0096 - val_mae: 42.4353\n",
      "Epoch 45/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 875.4766 - mae: 38.1182 - val_loss: 1532.5330 - val_mae: 42.1181\n",
      "Epoch 46/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 868.9577 - mae: 38.1175 - val_loss: 1533.0015 - val_mae: 42.3348\n",
      "Epoch 47/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 865.9069 - mae: 38.1334 - val_loss: 1535.6298 - val_mae: 42.3280\n",
      "Epoch 48/50\n",
      "1356/1356 [==============================] - 13s 9ms/step - loss: 862.1082 - mae: 38.1332 - val_loss: 1526.9556 - val_mae: 42.2272\n",
      "Epoch 49/50\n",
      "1356/1356 [==============================] - 13s 10ms/step - loss: 860.8137 - mae: 38.1473 - val_loss: 1558.7573 - val_mae: 42.3685\n",
      "Epoch 50/50\n",
      "1356/1356 [==============================] - 12s 9ms/step - loss: 861.0936 - mae: 38.1611 - val_loss: 1559.0499 - val_mae: 42.3285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-65dac8553fdb>:163: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(2,1,1)\n",
      "<ipython-input-17-65dac8553fdb>:185: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(2,1,2)\n"
     ]
    }
   ],
   "source": [
    "# Here copying the result from the KIT paper\n",
    "\n",
    "smear_px, smear_py = one_d(0), one_d(0)   #the smearing of the detector, we don't know yet what it is\n",
    "\n",
    "ref = [#smear_px,py                      #0\n",
    "       one_d(1.776),                      #1\n",
    "       df4[\"metx\"],                   #2\n",
    "       df4[\"mety\"],                   #3\n",
    "       Mom4_to_tf(tau_1_vis.e),       #4\n",
    "       Mom4_to_tf(tau_1_vis.p_x),     #5\n",
    "       Mom4_to_tf(tau_1_vis.p_y),     #6\n",
    "       Mom4_to_tf(tau_1_vis.p_z),     #7\n",
    "       Mom4_to_tf(tau_2_vis.e),       #8\n",
    "       Mom4_to_tf(tau_2_vis.p_x),     #9 \n",
    "       Mom4_to_tf(tau_2_vis.p_y),     #10\n",
    "       Mom4_to_tf(tau_2_vis.p_z),     #11\n",
    "       one_d(125),                    #12\n",
    "       #Mom4_to_tf(nu_1.e),            #13       corresponding to          #0\n",
    "       Mom4_to_tf(nu_1.p_x),          #14                                 #1\n",
    "       Mom4_to_tf(nu_1.p_y),          #15                                 #2\n",
    "       Mom4_to_tf(nu_1.p_z),          #16                                 #3\n",
    "       #Mom4_to_tf(nu_2.e),            #17                                 #4\n",
    "       Mom4_to_tf(nu_2.p_x),          #18                                 #5\n",
    "       Mom4_to_tf(nu_2.p_y),          #19                                 #6\n",
    "       Mom4_to_tf(nu_2.p_z),          #20                                 #7\n",
    "]\n",
    "\n",
    "\n",
    "#i_smear_px = 0\n",
    "i_tau_mass = 0\n",
    "i_smeared_met_px = 1\n",
    "i_smeared_met_py = 2\n",
    "i_tau1_e = 3\n",
    "i_tau1_px = 4\n",
    "i_tau1_py = 5\n",
    "i_tau1_pz = 6\n",
    "i_tau2_e = 7\n",
    "i_tau2_px = 8\n",
    "i_tau2_py = 9\n",
    "i_tau2_pz = 10\n",
    "i_gen_mass = 11\n",
    "#i_nu1_e = 12\n",
    "i_nu1_px = 12\n",
    "i_nu1_py = 13\n",
    "i_nu1_pz = 14\n",
    "#i_nu2_e = 16\n",
    "i_nu2_px = 15\n",
    "i_nu2_py = 16\n",
    "i_nu2_pz = 17\n",
    "\n",
    "\n",
    "m_tau_squared = tf.transpose(one_d(1.776)**2)\n",
    "\n",
    "ref = tf.transpose(ref)\n",
    "\n",
    "def loss_D_p (y_true, y_pred):\n",
    "    #calculting the difference between the the components, need to add the smearing of detector eventually\n",
    "    target_components = [i_nu1_px, i_nu1_py, i_nu1_pz, i_nu2_px, i_nu2_py, i_nu2_pz]\n",
    "    target_components_diff_list = []\n",
    "    for i in target_components: target_components_diff_list.append((y_true[:,i]-y_pred[:,i])**2)\n",
    "    dxyz = 0\n",
    "    for d in target_components_diff_list: dxyz+=d\n",
    "    return dxyz\n",
    "\n",
    "def energy_nu (y_true, y_pred, number):\n",
    "    if number == 1:\n",
    "        return tf.sqrt(y_pred[:, i_nu1_px]**2 + y_pred[:, i_nu1_py]**2 + y_pred[:, i_nu1_pz]**2)\n",
    "    if number == 2:\n",
    "        return tf.sqrt(y_pred[:, i_nu2_px]**2 + y_pred[:, i_nu2_py]**2 + y_pred[:, i_nu2_pz]**2)\n",
    "\n",
    "def loss_mass_tau(y_true, y_pred):\n",
    "    #now we try only to use the y_pred for neutrino info, this is I guess their way of \n",
    "    #only training for neutrino info whilst keeping nice structure\n",
    "    #we are always assuming m=0\n",
    "    # note, we are taking y_tau as exact, we could choose not to...\n",
    "    E1 = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) \n",
    "    P1_squared = (y_true[:, i_tau1_px] + y_pred[:, i_nu1_px])**2 + (y_true[:, i_tau1_py] + y_pred[:, i_nu1_py])**2 + (y_true[:, i_tau1_pz] + y_pred[:, i_nu1_pz])**2\n",
    "    R1 = (E1**2 - P1_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    \n",
    "    E2 = y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2) \n",
    "    P2_squared = (y_true[:, i_tau2_px] + y_pred[:, i_nu2_px])**2 + (y_true[:, i_tau2_py] + y_pred[:, i_nu2_py])**2 + (y_true[:, i_tau2_pz] + y_pred[:, i_nu2_pz])**2\n",
    "    R2 = (E2**2 - P2_squared - y_true[:, i_tau_mass]**2)/(y_true[:, i_gen_mass])**2\n",
    "    return tf.math.abs(R1) + tf.abs(R2)\n",
    "\n",
    "\n",
    "def loss_mass_Higgs(y_true, y_pred):\n",
    "    EH = y_true[:, i_tau1_e] + energy_nu(y_true, y_pred, 1) + y_true[:, i_tau2_e] + energy_nu(y_true, y_pred, 2)\n",
    "    px_H = y_true[:, i_tau1_px] + y_true[:, i_tau2_px] + y_pred[:, i_nu1_px] + y_pred[:, i_nu2_px]\n",
    "    py_H = y_true[:, i_tau1_py] + y_true[:, i_tau2_py] + y_pred[:, i_nu1_py] + y_pred[:, i_nu2_py]\n",
    "    pz_H = y_true[:, i_tau1_pz] + y_true[:, i_tau2_pz] + y_pred[:, i_nu1_pz] + y_pred[:, i_nu2_pz]\n",
    "    \n",
    "    return tf.abs((EH**2-px_H**2-py_H**2-pz_H**2 - y_true[:, i_gen_mass]**2)/(y_true[:,i_gen_mass])**2)\n",
    "    \n",
    "\n",
    "print(loss_mass_tau(ref,ref))\n",
    "print(loss_mass_Higgs(ref,ref))\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "\n",
    "    dxyz = loss_D_p(y_true, y_pred)\n",
    "    #dmet = loss_dmet(y_true, y_pred)\n",
    "    #dPTtaus = loss_dPTtaus(y_true, y_pred)\n",
    "    dtau = loss_mass_tau(y_true, y_pred)\n",
    "    dHiggs = loss_mass_Higgs(y_true, y_pred)\n",
    "    #dM = loss_dM_had(y_true, y_pred)\n",
    "\n",
    "    return dxyz + dtau + dHiggs\n",
    "\n",
    "\n",
    "x = np.array([one_d(125), \n",
    "              one_d(1.776),\n",
    "              #smear_px,\n",
    "              #smear_py,\n",
    "              df4[\"metx\"],\n",
    "              df4[\"mety\"], \n",
    "              Mom4_to_tf(tau_1_vis.e),\n",
    "              Mom4_to_tf(tau_1_vis.p_x),\n",
    "              Mom4_to_tf(tau_1_vis.p_y),\n",
    "              Mom4_to_tf(tau_1_vis.p_z),\n",
    "              Mom4_to_tf(tau_2_vis.e),\n",
    "              Mom4_to_tf(tau_2_vis.p_x),\n",
    "              Mom4_to_tf(tau_2_vis.p_y),\n",
    "              Mom4_to_tf(tau_2_vis.p_z),\n",
    "              df4[\"met\"]\n",
    "             ])\n",
    "x = tf.transpose(x)\n",
    "\n",
    "y = ref\n",
    "\n",
    "\n",
    "input_1 = tf.keras.Input(shape = x.shape, name=\"lab_frame\")\n",
    "\n",
    "x2 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning\")(input_1)\n",
    "x3 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning3\")(x2)\n",
    "#x4 = tf.keras.layers.Dense(100, activation = 'elu', name=\"learning3\")(x3)\n",
    "#x4 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning3\")(x3)\n",
    "#x5 = tf.keras.layers.Dense(500, activation = 'elu', name=\"learning4\")(x4)\n",
    "#x6 = tf.keras.layers.Dense(500, activation = 'relu', name=\"learning5\")(x5)\n",
    "\n",
    "output = tf.keras.layers.Dense(18, name=\"output\")(x3)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_1],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "model.summary\n",
    "\n",
    "#tf.keras.losses.MeanSquaredError() #common to the 4 iterations\n",
    "model.compile(loss = loss_fn, optimizer = 'adam', metrics = ['mae'])#, loss_mass_Higgs, loss_mass_tau, loss_D_p])\n",
    "\n",
    "history = model.fit(x, y, validation_split = 0.3,\n",
    "    epochs=50,\n",
    "    batch_size = 500)\n",
    "\n",
    "\n",
    "\n",
    "#This is the 'quality check' section\n",
    "fig = plt.figure('2_fig', figsize=(10,10), frameon = True)\n",
    "\n",
    "plt.title('Quality check: 300-300-o,\\ninputs include mets, output include met. Batch: 500')\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -6])\n",
    "hist2 = np.array(y[:, -6])\n",
    "hist_a = hist2-hist1\n",
    "plt.hist(hist_a, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_1\\nMean: %.2f, std: %.2f\"%(hist_a.mean(), hist_a.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -5])\n",
    "hist2 = np.array(y[:, -5])\n",
    "hist_b = hist2-hist1\n",
    "plt.hist(hist_b, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_1\\nMean: %.2f, std: %.2f\"%(hist_b.mean(), hist_b.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -4])\n",
    "hist2 = np.array(y[:, -4])\n",
    "hist_c = hist2-hist1\n",
    "plt.hist(hist_c, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_1\\nMean: %.2f, std: %.2f\"%(hist_c.mean(), hist_c.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(-50,50)\n",
    "plt.ylabel('Occurences')\n",
    "         \n",
    "         \n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -3])\n",
    "hist2 = np.array(y[:, -3])\n",
    "hist_d = hist2-hist1\n",
    "plt.hist(hist_d, bins = 100, alpha = 0.5,label = \"True-Regressed nu_px_2\\nMean: %.2f, std: %.2f\"%(hist_d.mean(), hist_d.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "hist2 = np.array(y[:, -2])\n",
    "hist_e = hist2-hist1\n",
    "plt.hist(hist_e, bins = 100, alpha = 0.5,label = \"True-Regressed nu_py_2\\nMean: %.2f, std: %.2f\"%(hist_e.mean(), hist_e.std()))\n",
    "\n",
    "hist1 = np.array(model({\"lab_frame\": x})[:, -1])\n",
    "hist2 = np.array(y[:, -1])\n",
    "hist_f = hist2-hist1\n",
    "plt.hist(hist_f, bins = 100, alpha = 0.5,label = \"True-Regressed nu_pz_2\\nMean: %.2f, std: %.2f\"%(hist_f.mean(), hist_f.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Difference of Momenta component')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xlim(-50,50)\n",
    "\n",
    "plt.savefig('Quality_300_300_include_met_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0.1561270904944474 0\n"
     ]
    }
   ],
   "source": [
    "hist1 = np.array(model({\"lab_frame\": x})[:, -2])\n",
    "\n",
    "print(len(model({\"lab_frame\": x})[1]))\n",
    "hist2 = np.array(y[:, -2])\n",
    "need = \"nu_1_E\"\n",
    "dd = 0\n",
    "\n",
    "def frac(d = -2):\n",
    "    difference = y[:, 0]-model({\"lab_frame\": x})[:, 0]\n",
    "    difference = np.reshape(difference, [-1])\n",
    "    l = np.where(abs(difference)<=10**(d),1,0)\n",
    "    print(float(float(np.sum(l))/len(l)), d)\n",
    "    return float(float(np.sum(l))/len(l))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(hist1, bins = 100, alpha = 0.5,label = \"NN %s component : fraction($\\Delta$<$10^{%.1f}$)=%.3f \"%(need, dd, frac(dd)))\n",
    "plt.hist(hist2, bins = 100, alpha = 0.5,label = 'True %s - Features: phi_CP_1 (fixed)'%need)      \n",
    "plt.ylabel(\"Frequency\", fontsize = 'x-large')\n",
    "plt.xlabel(\"%s\"%(need), fontsize = 'x-large')\n",
    "plt.grid()\n",
    "#plt.xlim(0,400)\n",
    "plt.legend()#prop = {'size', 10})\n",
    "plt.savefig('neutrino_next_2_300_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculation:\n",
    "    \"\"\"\n",
    "    Class for calculating the aco_angle variables\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        #this is the function doing all the calculations manually just takes as input the dataframe\n",
    "        #The different *initial* 4 vectors, (E,px,py,pz)\n",
    "        self.pi_1 = np.array([df[\"pi_E_1\"],df[\"pi_px_1\"],df[\"pi_py_1\"],df[\"pi_pz_1\"]])\n",
    "        self.pi_2 = np.array([df[\"pi_E_2\"],df[\"pi_px_2\"],df[\"pi_py_2\"],df[\"pi_pz_2\"]])\n",
    "\n",
    "        self.pi0_1 = np.array([df[\"pi0_E_1\"],df[\"pi0_px_1\"],df[\"pi0_py_1\"],df[\"pi0_pz_1\"]])\n",
    "        self.pi0_2 = np.array([df[\"pi0_E_2\"],df[\"pi0_px_2\"],df[\"pi0_py_2\"],df[\"pi0_pz_2\"]])\n",
    "\n",
    "        #Charged and neutral pion momenta\n",
    "        self.pi_1_4Mom = Momentum4(df[\"pi_E_1\"],df[\"pi_px_1\"],df[\"pi_py_1\"],df[\"pi_pz_1\"])\n",
    "        self.pi_2_4Mom = Momentum4(df[\"pi_E_2\"],df[\"pi_px_2\"],df[\"pi_py_2\"],df[\"pi_pz_2\"])\n",
    "\n",
    "        #Same for the pi0\n",
    "        self.pi0_1_4Mom = Momentum4(df[\"pi0_E_1\"],df[\"pi0_px_1\"],df[\"pi0_py_1\"],df[\"pi0_pz_1\"])\n",
    "        self.pi0_2_4Mom = Momentum4(df[\"pi0_E_2\"],df[\"pi0_px_2\"],df[\"pi0_py_2\"],df[\"pi0_pz_2\"])\n",
    "\n",
    "        self.impact_param_1 = Momentum4(np.zeros(len(df[\"ip_x_1\"])),df[\"ip_x_1\"],df[\"ip_y_1\"],df[\"ip_z_1\"])\n",
    "        self.impact_param_2 = Momentum4(np.zeros(len(df[\"ip_x_2\"])),df[\"ip_x_2\"],df[\"ip_y_2\"],df[\"ip_z_2\"])\n",
    "\n",
    "        #comment or uncomment depending on which aco_angle you want \n",
    "        #self.pi0_1_4Mom = self.impact_param_1\n",
    "        #self.pi0_2_4Mom = self.impact_param_2\n",
    "\n",
    "        #This is the COM frame of the two charged pions w.r.t. which we'll boost\n",
    "        self.ref_COM_4Mom = Momentum4(self.pi_1_4Mom+self.pi_2_4Mom)\n",
    "        boost = Momentum4(self.ref_COM_4Mom[0], -self.ref_COM_4Mom[1], -self.ref_COM_4Mom[2], -self.ref_COM_4Mom[3])\n",
    "        \n",
    "        \n",
    "        boost = -self.ref_COM_4Mom\n",
    "        #energies=[df4[\"pi_E_1\"],df4[\"pi_E_2\"],df4[\"pi0_E_1\"],df4[\"pi0_E_2\"]]\n",
    "\n",
    "        #Lorentz boost everything in the ZMF of the two charged pions\n",
    "        self.pi0_1_4Mom_star = self.pi0_1_4Mom.boost_particle(boost)\n",
    "        self.pi0_2_4Mom_star = self.pi0_2_4Mom.boost_particle(boost)\n",
    "\n",
    "        #Lorentz boost everything in the ZMF of the two neutral pions\n",
    "        self.pi_1_4Mom_star = self.pi_1_4Mom.boost_particle(boost)\n",
    "        self.pi_2_4Mom_star = self.pi_2_4Mom.boost_particle(boost)\n",
    "\n",
    "\n",
    "        #calculating the perpependicular component\n",
    "        pi0_1_3Mom_star_perp=cross_product(self.pi0_1_4Mom_star[1:], self.pi_1_4Mom_star[1:])\n",
    "        pi0_2_3Mom_star_perp=cross_product(self.pi0_2_4Mom_star[1:], self.pi_2_4Mom_star[1:])\n",
    "\n",
    "        #Now normalise:\n",
    "        pi0_1_3Mom_star_perp=pi0_1_3Mom_star_perp/norm(pi0_1_3Mom_star_perp)\n",
    "        pi0_2_3Mom_star_perp=pi0_2_3Mom_star_perp/norm(pi0_2_3Mom_star_perp)\n",
    "\n",
    "        self.pi0_1_4Mom_star_perp = [self.pi0_1_4Mom_star[0], pi0_1_3Mom_star_perp[0], \n",
    "                                     pi0_1_3Mom_star_perp[1], pi0_1_3Mom_star_perp[2]]\n",
    "\n",
    "        self.pi0_2_4Mom_star_perp = [self.pi0_1_4Mom_star[0], pi0_2_3Mom_star_perp[0], \n",
    "                                     pi0_2_3Mom_star_perp[1], pi0_2_3Mom_star_perp[2]]\n",
    "\n",
    "        #Calculating phi_star\n",
    "        self.phi_CP_unshifted = np.arccos(dot_product(pi0_1_3Mom_star_perp,pi0_2_3Mom_star_perp))\n",
    "        \n",
    "        print(self.phi_CP_unshifted[:10],'This is phi_CP')\n",
    "\n",
    "        self.phi_CP = self.phi_CP_unshifted\n",
    "        \n",
    "        print(pi0_1_3Mom_star_perp[:,23], 'this is pi0_1_3mom')\n",
    "\n",
    "        #The energy ratios\n",
    "        self.y_T = np.array(df['y_1_1']*df['y_1_2'])\n",
    "\n",
    "        #The O variable\n",
    "        cross = np.array(np.cross(pi0_1_3Mom_star_perp.transpose(),pi0_2_3Mom_star_perp.transpose()).transpose())\n",
    "        self.bigO = dot_product(self.pi_2_4Mom_star[1:],cross)\n",
    "        \n",
    "        print(self.bigO[:10], '\\n this is big0')\n",
    "\n",
    "        #perform the shift w.r.t. O* sign\n",
    "        \n",
    "        \n",
    "        #phi_CP=np.where(self.bigO>=0, 2*np.pi-self.phi_CP_unshifted, self.phi_CP_unshifted)\n",
    "        self.phi_CP_1 = np.where(self.bigO>=0, 2*np.pi-self.phi_CP_unshifted, self.phi_CP_unshifted)\n",
    "        \n",
    "        print(self.phi_CP_1[:10], '\\n this is after first shft')\n",
    "\n",
    "       # self.phi_CP_2 = np.where(self.y_T<=0, self.phi_CP+np.pi, self.phi_CP-np.pi)\n",
    "\n",
    "        #additionnal shift that needs to be done do see differences between odd and even scenarios, with y=Energy ratios\n",
    "        self.phi_CP = np.where(self.y_T>=0, np.where(self.phi_CP_1<np.pi, self.phi_CP_1+np.pi, self.phi_CP_1-np.pi), self.phi_CP_1)\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        print(self.phi_CP[:10], 'this is full')\n",
    "        \n",
    "        self.y = df[\"aco_angle_1\"]\n",
    "    \n",
    "    def checks(self):\n",
    "\n",
    "        target = [self.df[\"aco_angle_1\"]]#self.df[\"aco_angle_7\"]]\n",
    "        y = tf.transpose(tf.convert_to_tensor(target, dtype=np.float32))\n",
    "\n",
    "        inputs = [self.pi0_1_4Mom, self.pi_1_4Mom, self.pi0_2_4Mom, self.pi_2_4Mom]\n",
    "        x = tf.convert_to_tensor(inputs, dtype=np.float32)\n",
    "        x = tf.transpose(x, [2, 0, 1])\n",
    "        \n",
    "        k = tf.convert_to_tensor([\n",
    "                          self.impact_param_1[0], self.impact_param_1[1], self.impact_param_1[2], self.impact_param_1[3],\n",
    "                          #self.pi0_1_4Mom[0], self.pi0_1_4Mom[1], self.pi0_1_4Mom[2], self.pi0_1_4Mom[3],\n",
    "                          self.pi_1_4Mom[0], self.pi_1_4Mom[1], self.pi_1_4Mom[2], self.pi_1_4Mom[3],\n",
    "                          #self.pi0_2_4Mom[0], self.pi0_2_4Mom[1], self.pi0_2_4Mom[2], self.pi0_2_4Mom[3],\n",
    "                          self.impact_param_2[0], self.impact_param_2[1], self.impact_param_2[2], self.impact_param_2[3],\n",
    "                          self.pi_2_4Mom[0], self.pi_2_4Mom[1], self.pi_2_4Mom[2], self.pi_2_4Mom[3]],\n",
    "                         dtype=np.float32)\n",
    "\n",
    "# the extra info we are giving\n",
    "        l = tf.convert_to_tensor([self.y_T], dtype=np.float32)\n",
    "\n",
    "        return x,y,k,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_1 = nu_1 + tau_1_vis\n",
    "tau_2 = nu_2 + tau_2_vis\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1.m,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (np.array(tau_1.m).mean(), np.array(tau_1.m).std()))\n",
    "plt.hist(np.array(tau_2.m),  bins = 1000, alpha = 0.5, label = 'dot product |tau_2_vis| and |nu_2|\\nmean=%.2f, std=%.2f'% (np.array(tau_2.m).mean(), np.array(tau_2.m).std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0,6)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Mass of sum of rho decay products and tau neutrino', fontsize = 'large')\n",
    "plt.title('Checking that tau neutrino and visible decay products\\nsum up to tau (RM9999)', fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.show()\n",
    "plt.savefig('sum_to_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-71f6fbb4e571>:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "Higgs = tau_1 + tau_2 \n",
    "plt.figure()\n",
    "plt.hist(Higgs.m,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (np.array(Higgs.m).mean(), np.array(Higgs.m).std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xlim(0,6)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Mass of the sum of the two taus', fontsize = 'large')\n",
    "plt.title('Checking that the two taus\\nsum up to Higgs (RM9999)', fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.show()\n",
    "plt.savefig('sum_to_Higgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99919824 0.99922621 0.99976824 ... 0.99905886 0.99952941 0.99982143]\n",
      "[0.99770581 0.99983855 0.99924261 0.98171425 0.9989211  0.99978589\n",
      " 0.99934793 0.99860659 0.99942672 0.99745414]\n"
     ]
    }
   ],
   "source": [
    "#check of the colinearity approximation\n",
    "\n",
    "#Next up: colinarity, tau mass (Kingsley checked) and then Higgs. \n",
    "\n",
    "dot_prod_1 = np.einsum('ia,ia->a',tau_1_vis[1:, ...], nu_1[1:, ...])\n",
    "dot_prod_1 = dot_prod_1/(norm(tau_1_vis[1:, ...])* norm(nu_1[1:, ...]))\n",
    "\n",
    "dot_prod_2 = np.einsum('ia,ia->a',tau_2_vis[1:, ...], nu_2[1:, ...])\n",
    "dot_prod_2 = dot_prod_2/(norm(tau_2_vis[1:, ...])* norm(nu_2[1:, ...]))\n",
    "\n",
    "print(dot_prod_1)\n",
    "print(dot_prod_2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(dot_prod_1,  bins = 1000, alpha = 0.5, label = 'dot product |tau_1_vis| and |nu_1|\\nmean=%.2f, std=%.2f'% (dot_prod_1.mean(), dot_prod_1.std()))\n",
    "plt.hist(dot_prod_2,  bins = 1000, alpha = 0.5, label = 'dot product |tau_2_vis| and |nu_2|\\nmean=%.2f, std=%.2f'% (dot_prod_2.mean(), dot_prod_2.std()))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0.95,1.05)\n",
    "plt.ylabel('Occurencies', fontsize = 'x-large')\n",
    "plt.xlabel('Dot product between rho decay products and tau neutrino', fontsize = 'large')\n",
    "plt.title('Checking the colinarity approximation between\\ntau neutrino and visible decay products', fontsize = 'xx-large', weight = 'bold')\n",
    "\n",
    "plt.savefig('Colinearity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the colinear approximation ? can calculate the dot product between the visible decay products and the nus\n",
    "check_x_1 = np.array(nu_1.p_x-tau_1_vis.p_x)#dot_product(tau_1_vis[1:]/norm(tau_1_vis[1:]), nu_1[1:]/norm(nu_1[1:])))\n",
    "check_x_2 = np.array(nu_2.p_x-tau_2_vis.p_x)#dot_product(tau_2_vis[1:]/norm(tau_2_vis[1:]), nu_2[1:]/norm(nu_2[1:])))\n",
    "check_y_1 = np.array(nu_1.p_y-tau_1_vis.p_y)\n",
    "check_y_2 = np.array(nu_2.p_y-tau_2_vis.p_y)\n",
    "\n",
    "\n",
    "check_z_1 = []\n",
    "check_z_2 = []\n",
    "\n",
    "for i in range (len(nu_1.p_z)):\n",
    "    if nu_1.p_z[i] != 9999:\n",
    "        check_z_1.append(nu_1.p_z[i]-tau_1_vis.p_z[i])\n",
    "    if nu_2.p_z[i] != 9999:\n",
    "        check_z_2.append(nu_2.p_z[i]-tau_2_vis.p_z[i])\n",
    "\n",
    "check_z_1 = np.array(check_z_1)\n",
    "check_z_2 = np.array(check_z_2)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(tau_1_vis.p_x - nu_1.p_x, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_px and nu_1_px\\nmean=%.2f, std=%.2f'% (check_x_1.mean(),check_x_1.std()))\n",
    "plt.hist(tau_2_vis.p_x - nu_2.p_x, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_px and nu_2_px\\nmean=%.2f, std=%.2f'% (check_x_2.mean(),check_x_2.std()))\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_x')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(tau_1_vis.p_y - nu_1.p_y, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_py and nu_1_py\\nmean=%.2f, std=%.2f'% (check_y_1.mean(),check_y_1.std()))\n",
    "plt.hist(tau_2_vis.p_y - nu_2.p_y, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_py and nu_2_py\\nmean=%.2f, std=%.2f'% (check_y_2.mean(),check_y_2.std()))\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_y')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(check_z_1, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_pz and nu_1_pz\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_z_1.mean(),check_z_1.std()))\n",
    "plt.hist(check_z_2, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_pz and nu_2_pz\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_z_2.mean(),check_z_2.std()))\n",
    "\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino momenta components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between momenta components', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-250,250)\n",
    "plt.legend()\n",
    "plt.savefig('Check_angles_remove_z')\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_E_1 = []#dot_product(tau_1_vis[1:]/norm(tau_1_vis[1:]), nu_1[1:]/norm(nu_1[1:])))\n",
    "check_E_2 = []#dot_product(tau_2_vis[1:]/norm(tau_2_vis[1:]), nu_2[1:]/norm(nu_2[1:])))\n",
    "\n",
    "for i in range (len(nu_1.e)):\n",
    "    if nu_1.e[i]!=9999:\n",
    "        check_E_1.append(nu_1.e[i]-tau_1_vis.e[i])\n",
    "    if nu_2.e[i]!=9999:\n",
    "        check_E_2.append(nu_2.e[i]-tau_2_vis.e[i])\n",
    "\n",
    "check_E_1 = np.array(check_E_1)\n",
    "check_E_2 = np.array(check_E_2)\n",
    "plt.figure()\n",
    "plt.hist(check_E_1, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_1_vis_E and nu_1_E\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_E_1.mean(),check_E_1.std()))\n",
    "plt.hist(check_E_2, bins = 1000, alpha = 0.5, \n",
    "         label = 'Difference between tau_2_vis_E and nu_2_E\\nREMOVE 9999 mean=%.2f, std=%.2f'% (check_E_2.mean(),check_E_2.std()))\n",
    "\n",
    "\n",
    "plt.title('Sanity check - difference between \\nsum(pions) and neutrino energy components', fontsize = 'xx-large', weight = 'bold')\n",
    "plt.xlabel('Difference between energies', fontsize = 'x-large')\n",
    "plt.ylabel('Frequency', fontsize = 'x-large')\n",
    "plt.grid()\n",
    "plt.xlim(-500,500)\n",
    "plt.legend()\n",
    "plt.savefig('Check_energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-75e86c0dc241>:29: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig('Compare_nu_E_to_met')\n"
     ]
    }
   ],
   "source": [
    "sum_nu = nu_1 + nu_2\n",
    "\n",
    "met = np.array(df4[\"met\"])\n",
    "sum_energies=[]\n",
    "met_ref = []\n",
    "x = [0,500]\n",
    "\n",
    "for i in range (len(nu_1.e)):\n",
    "    if nu_1.e[i]!=9999 and nu_2.e[i]!=9999:\n",
    "        sum_energies.append(sum_nu.e[i])\n",
    "        met_ref.append(met[i])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(met_ref, sum_energies, 'gx')\n",
    "plt.plot(x,x, 'k--', label = 'y=x')\n",
    "plt.xlabel(\"Missing transverse energy\", fontsize = 'x-large')\n",
    "plt.ylabel(\"Energy of the sum of the neutrinos\", fontsize = 'x-large')\n",
    "plt.title(\"Sanity check, removing 9999 \\n gen level nu energies and met\", fontsize = 'xx-large', weight = 'bold')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0,500)\n",
    "plt.savefig('Compare_nu_E_to_met')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-d78cf37700c1>:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    }
   ],
   "source": [
    "x_ref = np.array([0, 6])\n",
    "x_ref2 = [6,0]\n",
    "\n",
    "gen_phitt = np.array(df4[\"gen_phitt\"][:2000])*2*np.pi/180+np.pi\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_ref, x_ref, '--', label = 'x=y')\n",
    "plt.plot(x_ref, x_ref2, '--', label = 'y=-x')\n",
    "\n",
    "plt.hist2d(df4[\"aco_angle_1\"], df4[\"gen_phitt\"], bins=50)\n",
    "#plt.plot(df4[\"aco_angle_1\"][:2000], gen_phitt, 'bx', label='aco_angle_1')\n",
    "#plt.plot(df4[\"aco_angle_5\"][:100], gen_phitt, 'gx', label='aco_angle_5')\n",
    "#plt.plot(df4[\"aco_angle_6\"][:100], gen_phitt, 'rx', label='aco_angle_6')\n",
    "#plt.plot(df4[\"aco_angle_7\"][:100], gen_phitt, 'kx', label='aco_angle_7')\n",
    "#plt.xlabel(\"aco_angles\", fontsize = 'x-large')\n",
    "#plt.ylabel(\"gen_phitt, (rad & shifted)\", fontsize = 'x-large')\n",
    "plt.title(\"Sanity check,\\n gen_phitt against aco angles\", fontsize = 'xx-large', weight = 'bold')\n",
    "#plt.grid()\n",
    "#plt.legend()\n",
    "plt.savefig('Gen_phitt-aco_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c5219455b4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtau_mass_dist_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c5219455b4fa>\u001b[0m in \u001b[0;36mtau_mass_dist_1\u001b[0;34m(y, y_pred)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msum_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msum_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_1_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m   \"\"\"\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6164\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6166\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6168\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ICMasters/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul]"
     ]
    }
   ],
   "source": [
    "#quadratic distance to target - regress both at the same time, will be required for making sense out of the met\n",
    "\n",
    "#try to first work with aplha, only 1 variable - easier\n",
    "# y_1 is the first regressed value, I want them as [[],[]]\n",
    "#D_target = (apha_1 - y_1)**2 + (apha_2 - y_2)**2 \n",
    "\n",
    "def tau_mass_dist_1(y, y_pred):\n",
    "    global tau_1_vis  \n",
    "    \n",
    "    y = tf.transpose(y)\n",
    "    y_pred = tf.transpose(y_pred)\n",
    "    \n",
    "    \n",
    "    sum_energy = tf.transpose(tf.convert_to_tensor(tau_1_vis.e, dtype = 'float32'))+ tf.transpose(y_pred)[0] * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    sum_p = (1+ tf.transpose(y_pred)[0]) * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    \n",
    "    sum_energy_1 = tf.transpose(tf.convert_to_tensor(tau_1_vis.e, dtype = 'float32'))+ y[0] * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    sum_p_1 = (1+ tf.transpose(y[0])) * tf.transpose(tf.convert_to_tensor(tau_1_vis.p, dtype = 'float32'))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    m_tau =  tf.transpose(tf.convert_to_tensor(np.ones(sum_p.shape) * 1.77, dtype = 'float32')) \n",
    "    \n",
    "    return tf.math.abs(sum_energy_1 + sum_energy)#tf.math.abs(sum_energy**2 - sum_p**2 - m_tau**2)  #tf.math.mod(sum_energy**2 - sum_p**2 - m_tau**2)\n",
    "\n",
    "\n",
    "def loss_fn(y, y_pred):\n",
    "#     y = tf.transpose(y)\n",
    "#     y_pred = tf.transpose(y_pred)\n",
    "    return tf.convert_to_tensor((y[0] - y_pred[0])**2 + (y[1] - y_pred[1])**2 + tau_mass_dist_1(y, y_pred))#, dtype = np.float32)\n",
    "\n",
    "\n",
    "#sum_energy**2 - sum_p**2 -\n",
    "\n",
    "# + y[1] * tf.convert_to_tensor(tau_2_vis.p)\n",
    "    \n",
    "    \n",
    "\n",
    "print (tau_mass_dist_1([alpha_1, alpha_2],[alpha_1, alpha_2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
